[{"categories":["kaggle"],"content":"\r1 比赛介绍在这次竞赛中，将识别学生写作中的元素。更具体地说，将自动分割文本，并对6-12年级学生所写的文章中的论证和修辞元素进行分类。帮助学生提高写作水平的一个方法是通过自动反馈工具，评估学生的写作并提供个性化的反馈。 竞赛类型：本次竞赛属于深度学习/自然语言处理，所以推荐使用的模型或者库：Roberta/Deberta/Longformer 赛题数据：官方提供的训练集大约有15000篇文章，测试集大约有10000篇文章。然后将分割的每个元素分类为以下内容之一：引子/立场/主张/反诉/反驳/证据/结论性声明，值得注意的是，文章的某些部分将是未加注释的（即它们不适合上述的分类）。 评估标准：标签和预测的单词之间的重合度。通过计算每个类别的TP/FP/FN，然后取所有类别的 macro F1 score 分数得出。详见：Feedback Prize - Evaluating Student Writing | Kaggle。 推荐阅读 Kaggle 内的一篇 EDA（探索数据分析）来获取一些预备知识：NLP on Student Writing: EDA | Kaggle。 ","date":"2022-03-24","objectID":"/feedback-prize/:1:0","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#1-比赛介绍"},{"categories":["kaggle"],"content":"\r1.1 数据说明官方提供的训练集大约有15000篇文章，测试集大约有10000篇文章。然后将分割的每个元素分类为以下内容之一：引子/立场/主张/反诉/反驳/证据/结论性声明，值得注意的是，文章的某些部分将是未加注释的（即它们不适合上述的分类）。 官方数据页面：Feedback Prize - Evaluating Student Writing | Kaggle。 将分割的每个元素分类为以下内容之一： 引子–以统计数字、引文、描述或其他一些手段开始的介绍，以吸引读者的注意力并指向论题。 立场–对主要问题的看法或结论。 主张–支持该立场的主张。 反诉–反驳另一个诉求的诉求，或提出与立场相反的理由。 反驳–驳斥反诉的主张。 证据–支持主张、反主张或反驳的观点或例子。 结论性声明–重申主张的结论性声明。 值得注意的是，文章的某些部分将是未加注释的（即它们不适合上述的分类）。 train.csv - 一个包含训练集中所有论文注释版本的.csv文件： id - 作文的ID。 discourse_id - 话语元素的ID。 discourse_start - 话语元素在文章中开始的字符位置。 discourse_end - 话语元素在文章中结束的位置。 discourse_text - 话语元素的文本。 discourse_type - 话语元素的分类。 discourse_type_num - 话语元素的列举式分类标签（带序号）。 predictionstring - 训练样本的词索引，为预测所需。 ","date":"2022-03-24","objectID":"/feedback-prize/:1:1","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#11-数据说明"},{"categories":["kaggle"],"content":"\r1.2 评价标准评估依据是：标签 和 预测 的单词之间的重合度。 对于每个样本，所有的标签和对某一特定类别的预测都要进行比较。 如果标签和预测之间的重合度\u003e=0.5，而预测和标签之间的重合度\u003e=0.5，则预测是一个匹配，被认为是一个真阳性。 如果存在多个匹配，则取重叠度最高的一对匹配。 任何没有匹配的标签是假阴性，任何没有匹配的预测是假阳性。 1.2.1 举例标签： 预测： 第一个预测与任何一个标签都没有\u003e=0.5的重叠，是一个假阳性。 第二个预测将与第二个标签完全重叠，是一个真阳性。 第三个标签将是不匹配的，是一个假阴性。 最后的分数是通过计算每个类别的TP/FP/FN，然后取所有类别的 macro F1 score 分数得出的。 词的索引是通过使用Python的.split()函数计算的，并在得到的列表中获取索引。 两个重叠部分的计算方法是：取标签/预测对中的每个指数列表的set()，并计算两个集合的交点除以每个集合的长度。 F1-score： $$ F_1 = \\frac{2}{recall^{-1} + precision^{-1}} = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall} = \\frac{tp}{tp + \\frac{1}{2}(fp + fn)} $$ macor-F1： 适用环境：多分类问题，不受数据不平衡影响，容易受到识别性高（高recall、高precision）的类别影响。 计算每个类别的：$F_1-score_i=2\\frac{Recall_i \\cdot Precision_i}{Recall_i + Precision_i}$ 计算 $macro-F_1=\\frac{F_1-score_1+F_1-socre_2+F_1-score_3}{3}$ ","date":"2022-03-24","objectID":"/feedback-prize/:1:2","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#12-评价标准"},{"categories":["kaggle"],"content":"\r1.2 评价标准评估依据是：标签 和 预测 的单词之间的重合度。 对于每个样本，所有的标签和对某一特定类别的预测都要进行比较。 如果标签和预测之间的重合度\u003e=0.5，而预测和标签之间的重合度\u003e=0.5，则预测是一个匹配，被认为是一个真阳性。 如果存在多个匹配，则取重叠度最高的一对匹配。 任何没有匹配的标签是假阴性，任何没有匹配的预测是假阳性。 1.2.1 举例标签： 预测： 第一个预测与任何一个标签都没有\u003e=0.5的重叠，是一个假阳性。 第二个预测将与第二个标签完全重叠，是一个真阳性。 第三个标签将是不匹配的，是一个假阴性。 最后的分数是通过计算每个类别的TP/FP/FN，然后取所有类别的 macro F1 score 分数得出的。 词的索引是通过使用Python的.split()函数计算的，并在得到的列表中获取索引。 两个重叠部分的计算方法是：取标签/预测对中的每个指数列表的set()，并计算两个集合的交点除以每个集合的长度。 F1-score： $$ F_1 = \\frac{2}{recall^{-1} + precision^{-1}} = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall} = \\frac{tp}{tp + \\frac{1}{2}(fp + fn)} $$ macor-F1： 适用环境：多分类问题，不受数据不平衡影响，容易受到识别性高（高recall、高precision）的类别影响。 计算每个类别的：$F_1-score_i=2\\frac{Recall_i \\cdot Precision_i}{Recall_i + Precision_i}$ 计算 $macro-F_1=\\frac{F_1-score_1+F_1-socre_2+F_1-score_3}{3}$ ","date":"2022-03-24","objectID":"/feedback-prize/:1:2","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#121-举例"},{"categories":["kaggle"],"content":"\r1.3 推荐BaselineTensorFlow - LongFormer - Baseline: TensorFlow - LongFormer - NER - [CV 0.633] | Kaggle PyTorch - BigBird - Baseline: PyTorch - BigBird - NER - [CV 0.615] | Kaggle HuggingFace baseline：Feedback Prize HuggingFace Baseline: Training | Kaggle ","date":"2022-03-24","objectID":"/feedback-prize/:1:3","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#13-推荐baseline"},{"categories":["kaggle"],"content":"\r2 NER 命名实体识别","date":"2022-03-24","objectID":"/feedback-prize/:2:0","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#2-ner-命名实体识别"},{"categories":["kaggle"],"content":"\r2.1 什么是 NER命名实体识别？命名实体识别（Named Entity Recognition，简称NER），又称作“专名识别”，是指识别文 本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。简单的讲，就是 识别自然文本中的实体指称的边界和类别（本次比赛的类型就是 NER 命名实体识别）。 NER是NLP中一项基础性关键任务。从自然语言处理的流程来看，NER可以看作词法分析中 未登录词识别的一种，是未登录词中数量最多、识别难度最大、对分词效果影响最大问题。 同时NER也是关系抽取、事件抽取、知识图谱、机器翻译、问答系统等诸多NLP任务的基础。 ","date":"2022-03-24","objectID":"/feedback-prize/:2:1","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#21-什么是-ner命名实体识别"},{"categories":["kaggle"],"content":"\r2.2 NER通常的难点实体命名识别语料较小，容易产生过拟合；命名实体识别更侧重高召回率，但在信息检索 领域，高准确率更重要；通用的识别多种类型的命名实体的系统性能很差。 ","date":"2022-03-24","objectID":"/feedback-prize/:2:2","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#22-ner通常的难点"},{"categories":["kaggle"],"content":"\r2.3 NER模型演化路径","date":"2022-03-24","objectID":"/feedback-prize/:2:3","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#23-ner模型演化路径"},{"categories":["kaggle"],"content":"\r2.3.1 传统机器学习⽅法：CRF条件随机场（Conditional Random Field，CRF）是NER⽬前的主流模型。它的⽬标函数不仅考虑输⼊的状态特征函数，⽽且还包含了标签转移特征函数。在已知模型时，给输⼊序列求预测输出序列即求使⽬标函数最⼤化的最优序列，是⼀个动态规划问题，可以使⽤Viterbi算法解码来得到最优标签序列。 其优点在于其为⼀个位置进⾏标注的过程中可以利⽤丰富的内部及上下⽂特征信息。 ","date":"2022-03-24","objectID":"/feedback-prize/:2:4","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#231-传统机器学习法crf"},{"categories":["kaggle"],"content":"\r2.3.2 LSTM + CRF随着深度学习的发展，学术界提出了Deep Learning + CRF模型做序列标注。在神经⽹络的输出层接⼊CRF层（重点是利⽤标签转移概率）来做句⼦级别的标签预测，使得标注过程不再是对各个token独⽴分类。 ","date":"2022-03-24","objectID":"/feedback-prize/:2:5","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#232-lstm--crf"},{"categories":["kaggle"],"content":"\r2.3.3 BERT + (LSTM) + CRFBERT中蕴含了大量的通用知识，利用预训练好的BERT模型，再用少量的标注数据进行FINETUNE是一种快速的获得效果不错的NER的方法。 ","date":"2022-03-24","objectID":"/feedback-prize/:2:6","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#233-bert--lstm--crf"},{"categories":["kaggle"],"content":"\r3 NLP数据增强","date":"2022-03-24","objectID":"/feedback-prize/:3:0","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#3-nlp数据增强"},{"categories":["kaggle"],"content":"\r3.1 为什么数据增强很重要？数据增强时通过形成新的和不同的样本来训练数据集，数据增强对于提高机器学习模型的性能和结果是有用的。因为如果机器学习模型中的数据集是丰富和充分的，那么该模型的表现会更好，更准确。 对于机器学习模型来说，收集和标记数据可能是一个耗费精力和成本的过程。通过使用数据增强技术对数据集进行扩增，对于公司/组织可以使减少这些运营成本。 ","date":"2022-03-24","objectID":"/feedback-prize/:3:1","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#31-为什么数据增强很重要"},{"categories":["kaggle"],"content":"\r3.2 Sentence Shuffling在 Sentence Shuffling 中，我们将随机洗牌文本中的句子。 Original Text： Modern humans today are always on their phone. They are always on their phone more than 5 hours a day no stop. All they do is text back and forward and just have group Chats on social media. They even do it while driving. Augmented Text： They even do it while driving. They are always on their phone more than 5 hours a day no stop. All they do is text back and forward and just have group Chats on social media. Modern humans today are always on their phone ","date":"2022-03-24","objectID":"/feedback-prize/:3:2","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#32-sentence-shuffling"},{"categories":["kaggle"],"content":"\r3.3 Remove Duplicate Sentences在Remove Duplicates中，我们将删除文本中的重复句子。 为了证明这一点，我们把一个句子与它本身连接起来，输出的应该只是原始文本。 Original Text： Modern humans today are always on their phone. Modern humans today are always on their phone. Augmented Text： Modern humans today are always on their phone. ","date":"2022-03-24","objectID":"/feedback-prize/:3:3","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#33-remove-duplicate-sentences"},{"categories":["kaggle"],"content":"\r3.4 Remove Numbers在Remove Numbers中，我们将从文本中移除任何数字。我们可以简单地使用正则表达式实现这一点。 Original Text： There are 15594 samples of training data. Augmented Text： There are samples of training data. ","date":"2022-03-24","objectID":"/feedback-prize/:3:4","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#34-remove-numbers"},{"categories":["kaggle"],"content":"\r3.5 Remove Hashtags在Remove Hashtags中，我们将删除文本中的任何hashtag。 Original Text： Kaggle Competitions are fun. #MachineLearning Augmented Text： Kaggle Competitions are fun. ","date":"2022-03-24","objectID":"/feedback-prize/:3:5","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#35-remove-hashtags"},{"categories":["kaggle"],"content":"\r3.6 Remove Mentions在这个转换中，我们删除了文本中的任何提及（以’@‘开头的词）。 Original Text： @AnthonyGoldbloom is the founder of Kaggle. Augmented Text： is the founder of Kaggle. ","date":"2022-03-24","objectID":"/feedback-prize/:3:6","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#36-remove-mentions"},{"categories":["kaggle"],"content":"\r3.7 Remove URLs在这种转换中，我们从文本中删除任何URL。 Original Text： https://www.kaggle.com hosts the world’s best Machine Learning Hackathons. Augmented Text： hosts the world’s best Machine Learning Hackathons ","date":"2022-03-24","objectID":"/feedback-prize/:3:7","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#37-remove-urls"},{"categories":["kaggle"],"content":"\r3.8 Cut Out Words在这种转换中，我们从文本中删除一些词。 Original Text： Competition objective is to analyze argumentative writing elements from students grade 6-12. Augmented Text： Competition objective is to analyze argumentative elements from grade. ","date":"2022-03-24","objectID":"/feedback-prize/:3:8","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#38-cut-out-words"},{"categories":["kaggle"],"content":"\r3.9 KeyboardAugKeyboardAug，它使用键盘上的键的相邻性来模拟打错。 Original Text： Dear Senator, I favor keeping the Electoral College in the arguement. One of the reasons I feel that way is that it is harder for someone who is runnig for president to win. To win they would need to win over the votes of most of the small states. Or win over the votes over some of the small states and some of the big states. So it would need someone who is smart or at least somewhat smart. Keyboard augmentation： Dear Senator, I favor keeping the Electoral College in the arguement. One of the reasons I Vee? that way is that it is harder for soJdoJe who is runnig for president to win. To win they would need to win over the votes of most of the s,apl states. Or win over the votes over some of the small states and some of the big states. So it would need someone who is smart or at least somewhat smart. ","date":"2022-03-24","objectID":"/feedback-prize/:3:9","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#39-keyboardaug"},{"categories":["kaggle"],"content":"\r3.10 SpellingAugKeyboardAug创造的错别字太不自然，接着尝试SpellingAug，它使用一个常见的拼写错误数据库。 Original Text： Dear Senator, I favor keeping the Electoral College in the arguement. One of the reasons I feel that way is that it is harder for someone who is runnig for president to win. To win they would need to win over the votes of most of the small states. Or win over the votes over some of the small states and some of the big states. So it would need someone who is smart or at least somewhat smart. SpellingAug augmentation： Dear Senator, I favor keeping the Electoral College in the arguement. One of the reasons I feel that way is that it is harder for someone who is runnig for president to win. To win they would need to win overt the votes of most of the smol states. Or win over the votes over some of the small states and some of the big states. To it would need someone who is smart onr at least somewhat smart. ","date":"2022-03-24","objectID":"/feedback-prize/:3:10","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#310-spellingaug"},{"categories":["kaggle"],"content":"\r3.11 SynonymAugSynonymAug，它用同义词替换了一些单词。 Original Text： Dear Senator, I favor keeping the Electoral College in the arguement. One of the reasons I feel that way is that it is harder for someone who is runnig for president to win. To win they would need to win over the votes of most of the small states. Or win over the votes over some of the small states and some of the big states. So it would need someone who is smart or at least somewhat smart. SynonymAug augmentation： Dear Senator, I favor keeping the Electoral College in the arguement. One of the reasons I feel that way is that it is harder for someone who is runnig for president to win. To win they would need to win over the votes of most of the small states. Or win over the ballot over some of the small states and some of the big res publica. So it would need someone who is smart or at least somewhat smart. ","date":"2022-03-24","objectID":"/feedback-prize/:3:11","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#311-synonymaug"},{"categories":["kaggle"],"content":"\r3.12 WordEmbsAug使用单词嵌入来寻找类似的单词进行扩增，这里使用的是GloVe模型的本地副本。 Original Text： Dear Senator, I favor keeping the Electoral College in the arguement. One of the reasons I feel that way is that it is harder for someone who is runnig for president to win. To win they would need to win over the votes of most of the small states. Or win over the votes over some of the small states and some of the big states. So it would need someone who is smart or at least somewhat smart. WordEmbsAug augmentation： Dear Senator, I favor keeping the Electoral College in the arguement. One of the reasons I feel that way is that it is harder for someone who is runnig for mubarak to win. To win they would need to win over the votes of most of the small states. Or win over the votes over some of the small states and some of the bigger states. So it would need someone who is smart or at least somewhat smart. ","date":"2022-03-24","objectID":"/feedback-prize/:3:12","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#312-wordembsaug"},{"categories":["kaggle"],"content":"\r3.13 ContextualWordEmbsAug与WordEmbsAug相似，但使用更强大的上下文词嵌入。这里与BERT一起使用。 Original Text： Dear Senator, I favor keeping the Electoral College in the arguement. One of the reasons I feel that way is that it is harder for someone who is runnig for president to win. To win they would need to win over the votes of most of the small states. Or win over the votes over some of the small states and some of the big states. So it would need someone who is smart or at least somewhat smart. ContextualWordEmbsAug augmentation： Dear Senator, I favor keeping the Electoral College in the arguement. Part of the reasons I feel that way is that it is harder for someone who is runnig for president so win. To win they would need to win over the votes of most of the small counties. Or win over the votes over some among the small states and some of the big states. So it would need loser who is smart or at least somewhat smart. ","date":"2022-03-24","objectID":"/feedback-prize/:3:13","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#313-contextualwordembsaug"},{"categories":["kaggle"],"content":"\r3.14 结论SpellingAug 和 Contextual WordEmbsAug 看起来产生了很好的结果，可以在不 调整训练时给出的话语注释的情况下使用。 ","date":"2022-03-24","objectID":"/feedback-prize/:3:14","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#314-结论"},{"categories":["kaggle"],"content":"\r4 模型选择","date":"2022-03-24","objectID":"/feedback-prize/:4:0","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#4-模型选择"},{"categories":["kaggle"],"content":"\r4.1 TransformerTransformer是2017年提出的一种模型架构（《Attention is All You Need》），其优点除了效果好之外，由于encoder端是并行计算的，训练的时间也被大大缩短了。其开创性的思想，颠覆了以往序列建模和RNN划等号的思路，被广泛应用于NLP的各个领域。目前在NLP各业务全面开花的语言模型如GPT, BERT等，都是基于Transformer模型。 Transformer 模型使用了 Self-Attention 机制， Self-Attention 也是 Transformer 最核心的思想，不采用RNN顺序结构，使得模型可以并行化训练，而且能够拥有全局信息。 其中，attention的计算方式有多种，加性attention、点积attention，还有带参数的计算方式。具体可以去看相关文章。 对self-attention来说，它跟每一个input vector都做attention，所以没有考虑到input sequence的顺序。 ","date":"2022-03-24","objectID":"/feedback-prize/:4:1","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#41-transformer"},{"categories":["kaggle"],"content":"\r4.2 BertBERT是基于transformer的双向编码表示，它是一个预训练模型，模型训练时的两个任务是预测句子中被掩盖的词以及判断输入的两个句子是不是上下句。在预训练好的BERT模型后面根据特定任务加上相应的网络，可以完成NLP的下游任务，比如文本分类、机器翻译等。 虽然BERT是基于transformer的，但是它只使用了transformer的encoder部分，它的整体框架是由多层transformer的encoder堆叠而成的。 每一层的encoder则是由一层muti-head-attention和一层feed-forword组成，大的模型有24层，每层16个attention，小的模型12层，每层12个attention。每个attention的主要作用是通过目标词与句子中的所有词汇的相关度，对目标词重新编码。所以每个attention的计算包括三个步骤：计算词之间的相关度，对相关度归一化，通过相关度和所有词的编码进行加权求和获取目标词的编码。 在BERT中，输入的向量是由三种不同的embedding求和而成，分别是： wordpiece embedding：单词本身的向量表示。WordPiece是指将单词划分成一组有限的公共子词单元，能在单词的有效性和字符的灵活性之间取得一个折中的平衡。 position embedding：将单词的位置信息编码成特征向量。因为我们的网络结构没有RNN 或者LSTM，因此我们无法得到序列的位置信息，所以需要构建一个position embedding。构建position embedding有两种方法：BERT是初始化一个position embedding，然后通过训练将其学出来；而Transformer是通过制定规则来构建一个position embedding。 segment embedding：BERT 能够处理对输入句子对的分类任务。这类任务就像判断两个文本是否是语义相似的。句子对中的两个句子被简单的拼接在一起后送入到模型中。那BERT如何去区分一个句子对中的两个句子呢？答案就是segment embeddings。 BERT的优点是只有BERT表征会基于所有层中的左右两侧语境。BERT能做到这一点得益于Transformer中Attention机制将任意位置的两个单词的距离转换成了1。 ","date":"2022-03-24","objectID":"/feedback-prize/:4:2","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#42-bert"},{"categories":["kaggle"],"content":"\r4.3 LongformerLongformer 是一种可高效处理长文本的模型，出自 AllenAI 2020年。目前已开源，而且可以通过 huggingface 快速使用。 传统Transformer-based模型在处理长文本时存在一些问题，因为它们均采用“我全都要看”型的attention机制，即每一个token都要与其他所有token进行交互，无论是空间还是时间复杂度都高达O(n^2) 。为了解决这个问题，之前有些工作是将长文本切分为若干个较短的Text Segment，然后逐个处理，例如Transformer-XL。但这会导致不同的Text Segment之间无法进行交互，因而必然存在大量的information loss（信息丢失）。 本文提出的 Longformer，改进了Transformer传统的self-attention机制。具体来说，每一个token只对固定窗口大小附近的token进行 local attention（局部注意力）。并且 Longformer 针对具体任务，在原有 local attention 的基础上增加了一种 global attention（全局注意力）。 Longformer 在两个字符级语言建模任务上都取得了SOTA的效果。并且作者用 Longformer 的attention方法继续预训练 RoBERTa，训练得到的语言模型在多个长文档任务上进行fine-tune后，性能全面超越 RoBERTa 作者共提出了三种新的attention机制，这三种方法都很好的降低了传统self-attention的复杂度，它们分别是滑窗机制、空洞滑窗机制、融合全局信息的滑窗机制。 ","date":"2022-03-24","objectID":"/feedback-prize/:4:3","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#43-longformer"},{"categories":["kaggle"],"content":"\r4.3.1 滑窗机制 (SLIDING WINDOW ATTENTION)对于每一个token，只对其附近的w个token计算attention，复杂度为O(n×w) ，其中n为文本的长度。作者认为，根据应用任务的不同，可以对Transformer每一层施以不同的窗口大小w。 作者在具体实现的时候，设置的窗口大小=512，与BERT的input限制完全一样，所以大家不要存有“ Longformer 比 BERT 更轻量”的错觉。 ","date":"2022-03-24","objectID":"/feedback-prize/:4:4","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#431-滑窗机制-sliding-window-attention"},{"categories":["kaggle"],"content":"\r4.3.2 空洞滑窗机制 (DILATED SLIDING WINDOW)对每一个token进行编码时，普通的滑窗机制只能考虑到长度为w的上下文。作者进一步提出空洞滑窗机制（实际上空洞滑窗是CV领域中很早就有的一项技术），在不增加计算负荷的前提下，拓宽视野范围。在滑动窗口中，被attented到的两个相邻token之间会存在大小为d的间隙，因此每个token的视野范围可达到d×w。实验表明，由于考虑了更加全面的上下文信息，空洞滑窗机制比普通的滑窗机制表现更佳。 ","date":"2022-03-24","objectID":"/feedback-prize/:4:5","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#432-空洞滑窗机制-dilated-sliding-window"},{"categories":["kaggle"],"content":"\r4.3.3 融合全局信息的滑窗机制 (GLOBAL+SLIDING WINDOW)我们知道BERT类的语言模型在fine-tune时，实现方式略有不同。比如，对于文本分类任务，我们会在整个输入的前面加上[CLS]这个token；而对于QA任务，我们则会将问题与文本进行拼接后进行输入。在Longformer中，作者也希望能够根据具体任务的不同，在原本local attention的基础上添加少量的global attention。例如，在分类任务中会在[CLS]初添加一个global attention（对应下图第一行第一列全绿）；而在QA任务上会对question中的所有token添加global attention。如下图所示，对于添加了global attention的token，我们对其编码时要对整个序列做attention，并且编码其它token时，也都要attend到它。 ","date":"2022-03-24","objectID":"/feedback-prize/:4:6","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#433-融合全局信息的滑窗机制-globalsliding-window"},{"categories":["kaggle"],"content":"\r4.3.4 结论作者在text8和enwiki8两个字符级任务上对 Longformer 进行了实验。实验中每一层采用了不同的窗口大小，具体来说：底层使用较小的滑窗，以构建局部信息；越上层滑窗越大，以扩大感受野。 训练时，理想状况当时是希望使用GPU所能承受最大的w和sequence length，但为了加快训练速度，作者采用的是多阶段训练法：从较短的序列长度和窗口大小开始，后续每个阶段将窗口大小和训练长度增加一倍，并将学习率减半。 作者一共训练了5个阶段，第一个阶段sequence length是2048，最后一个阶段是23040。 实验结果，Longformer在这两个数据集上皆达到了SOTA。 ","date":"2022-03-24","objectID":"/feedback-prize/:4:7","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#434-结论"},{"categories":["kaggle"],"content":"\r5 HuggingFaceHuggingface Transformer 能够帮我们跟踪流⾏的新模型，并且提供统⼀的代码⻛格来使⽤BERT、XLNet和GPT等等各种不同的模型。⽽且它有⼀个模型仓库，所有常⻅的预训练模型和不同任务上fine-tuning的模型都可以在这⾥⽅便的下载（解决了各种Pretraining的Transformer模型实现不同、对比麻烦的问题）。 设计原则： 简洁，只有configuration，models和tokenizer三个主要类。 所有的模型都可以通过统一的from_pretrained()函数来实现加载，transformers会处理下载、缓存和其它所有加载模型相关的细节。而所有这些模型都统一在Hugging Face Models管理。 基于上面的三个类，提供更上层的pipeline和Trainer/TFTrainer，从而用更少的代码实现模型的预测和微调。 它不是一个基础的神经网络库来一步一步构造Transformer，而是把常见的Transformer模型封装成一个building block，我们可以方便的在PyTorch或者TensorFlow里使用它。 主要概念： Model类（如 BertModel）：包括30+的PyTorch模型(torch.nn.Module)和对应的TensorFlow模型(tf.keras.Model)。 Congif类（如 BertConfig）：它保存了模型的相关(超)参数。我们通常不需要自己来构造它。如果我们不需要进行模型的修改，那么创建模型时会自动使用对于的配置。 Tokenizer类（如 BertTokenizer）：它保存了词典等信息并且实现了把字符串变成ID序列的功能。 ","date":"2022-03-24","objectID":"/feedback-prize/:5:0","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#5-huggingface"},{"categories":["kaggle"],"content":"\r5.1.1 PipelinePipeline（使用预训练模型的函数），支持如下任务： 情感分析(Sentiment analysis)：一段文本是正面还是负面的情感倾向 文本生成(Text generation)：给定一段文本，让模型补充后面的内容 命名实体识别(Name entity recognition)：识别文字中出现的人名地名的命名实体 问答(Question answering)：给定一段文本以及针对它的一个问题，从文本中抽取答案 填词(Filling masked text)：把一段文字的某些部分mask住，然后让模型填空 摘要(Summarization)：根据一段长文本中生成简短的摘要 翻译(Translation)：把一种语言的文字翻译成另一种语言 特征提取(Feature extraction)：把一段文字用一个向量来表示 情感分析的例子： from transformers import pipeline classifier = pipeline('sentiment-analysis’) results = classifier([\"We are very happy to show you the Transformers library.\", \"We hope you don't hate it.\"]) for result in results: print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\") # 运行结果 label: POSITIVE, with score: 0.9998 label: NEGATIVE, with score: 0.5309 如果觉得模型不合适，寻找合适的模型： classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\") ","date":"2022-03-24","objectID":"/feedback-prize/:5:1","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#511-pipeline"},{"categories":["kaggle"],"content":"\r5.1.2 TokenizerTokenizer的作用大致就是分词，然后把词变成的整数ID，当然有些模型会使用subword。但是不管怎么样，最终的目的是把一段文本变成ID的序列。当然它也必须能够反过来把ID序列变成文本。 inputs = tokenizer(\"We are very happy to show you the Transformers library.\") Tokenizer对象是callable，因此可以直接传入一个字符串，返回一个dict。最主要的是ID的list，同时也会返回attention mask： print(inputs) {'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} 我们也可以一次传入一个batch的字符串，这样便于批量处理。这时我们需要指定padding 为True并且设置最大的长度： pt_batch = tokenizer( [\"We are very happy to show you the Transformers library.\", \"We hope you don't hate it.\"], padding=True, truncation=True, max_length=512, return_tensors=\"pt\" ) truncation为True会把过长的输入切掉，从而保证所有的句子都是相同长度的。 return_tensors=”pt” 表示返回的是 PyTorch的Tensor，如果使用TensorFlow则需要设置。 return_tensors=”tf”。 分词结果： \u003e\u003e\u003e for key, value in pt_batch.items(): ... print(f\"{key}: {value.numpy().tolist()}\") input_ids: [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0]] attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]] pt_batch仍然是一个dict，input_ids是一个batch的ID序列，我们可以看到第二个字符串较短，所以它被padding成和第一个一样长。如果某个句子的长度超过max_length，也会被 切掉多余的部分。 ","date":"2022-03-24","objectID":"/feedback-prize/:5:2","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#512-tokenizer"},{"categories":["kaggle"],"content":"\r5.1.3 Model from transformers import AutoTokenizer, AutoModelForSequenceClassification model_name = \"distilbert-base-uncased-finetuned-sst-2-english\" pt_model = AutoModelForSequenceClassification.from_pretrained(model_name) tokenizer = AutoTokenizer.from_pretrained(model_name) Tokenizer的处理结果可以输入给模型，对于PyTorch则需要使用**来展开参数： # PyTorch pt_outputs = pt_model(**pt_batch) Transformers的所有输出都是tuple，即使只有一个结果也会是长度为1的tuple： \u003e\u003e\u003e print(pt_outputs) (tensor([[-4.0833, 4.3364], [ 0.0818, -0.0418]], grad_fn=\u003cAddmmBackward\u003e),) ","date":"2022-03-24","objectID":"/feedback-prize/:5:3","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#513-model"},{"categories":["kaggle"],"content":"\r5.1.4 Config如果你想自定义模型(这里指的是调整模型的超参数，比如网络的层数，每层的attention head个数等等，如果你要实现一个全新的模型，那就不能用这里的方法了)，那么你需要构造配置类。 每个模型都有对应的配置类，比如DistilBertConfig。你可以通过它来指定隐单元的个数，dropout等等。如果你修改了核心的超参数(比如隐单元的个数)，那么就不能使用 from_pretrained加载预训练的模型了，这时你必须从头开始训练模型。当然Tokenizer一般还是可以复用的。 下面的代码修改了核心的超参数，构造了Tokenizer和模型对象： from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512) tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased') model = DistilBertForSequenceClassification(config) ","date":"2022-03-24","objectID":"/feedback-prize/:5:4","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#514-config"},{"categories":["kaggle"],"content":"\r5.1.5 NER 命名实体识别本次比赛是NER类型，我们来HuggingFace在NER上的用法： from transformers import pipeline nlp = pipeline(\"ner\") sequence = [\"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very\", \"close to the Manhattan Bridge which is visible from the window.\"] \u003e\u003e\u003e print(nlp(sequence)) [ {'word': 'Hu', 'score': 0.9995632767677307, 'entity': 'I-ORG'}, {'word': '##gging', 'score': 0.9915938973426819, 'entity': 'I-ORG'}, {'word': 'Face', 'score': 0.9982671737670898, 'entity': 'I-ORG'}, {'word': 'Inc', 'score': 0.9994403719902039, 'entity': 'I-ORG'}, {'word': 'New', 'score': 0.9994346499443054, 'entity': 'I-LOC'}, {'word': 'York', 'score': 0.9993270635604858, 'entity': 'I-LOC'}, {'word': 'City', 'score': 0.9993864893913269, 'entity': 'I-LOC'}, {'word': 'D', 'score': 0.9825621843338013, 'entity': 'I-LOC'}, {'word': '##UM', 'score': 0.936983048915863, 'entity': 'I-LOC'}, {'word': '##BO', 'score': 0.8987102508544922, 'entity': 'I-LOC'}, {'word': 'Manhattan', 'score': 0.9758241176605225, 'entity': 'I-LOC'}, {'word': 'Bridge', 'score': 0.990249514579773, 'entity': 'I-LOC'} ] ","date":"2022-03-24","objectID":"/feedback-prize/:5:5","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#515-ner-命名实体识别"},{"categories":["kaggle"],"content":"\r5.1.6 训练Huggingface Transformers提供了Trainer用作训练： from transformers import BertForSequenceClassification, Trainer, TrainingArguments model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\") training_args = TrainingArguments( output_dir='./results', # output directory num_train_epochs=3, # total # of training epochs per_device_train_batch_size=16, # batch size per device during training per_device_eval_batch_size=64, # batch size for evaluation warmup_steps=500, # number of warmup steps for learning rate scheduler weight_decay=0.01, # strength of weight decay logging_dir='./logs', # directory for storing logs ) trainer = Trainer( model=model, # the instantiated Transformers model to be trained args=training_args, # training arguments, defined above train_dataset=train_dataset, # training dataset eval_dataset=test_dataset # evaluation dataset ) TrainingArguments参数指定了训练的设置：输出目录、总的epochs、训练的batch_size、预测的batch_size、warmup的step数、weight_decay和log目录。然后使用trainer.train()和trainer.evaluate()函数就可以进行训练和验证。 我们也可以自己实现模型，但是要求它的forward返回的第一个参数是loss。 ","date":"2022-03-24","objectID":"/feedback-prize/:5:6","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#516-训练"},{"categories":["kaggle"],"content":"\r6 模型融合","date":"2022-03-24","objectID":"/feedback-prize/:6:0","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#6-模型融合"},{"categories":["kaggle"],"content":"\r6.1 融合方法介绍\r6.1.1 VotingVoting可以说是一种最为简单的模型融合方式。假如对于一个二分类模型，有3个基础模型，那么就采取投票的方式，投票多者为最终的分类。 6.1.2 BaggingBagging的思想是利用抽样生成不同的训练集，进而训练不同的模型，将这些模型的输出结果综合（投票或平均的方式）得到最终的结果。 其本质是利用了模型的多样性，改善算法整体的效果。Bagging的重点在于不同训练集的生成，这里使用了一种名为Bootstrap的方法，即有放回的重复随机抽样，从而生成不同的数据集。 6.1.3 BoostingBoosting是一种提升算法，其思想是在算法迭代过程中，每次迭代构建新的分类器，重点关注被之前分类器分类错误的样本，如此迭代，最终加权平均所有分类器的结果，从而提升分类精度。 与Bagging相比来说最大的区别就是Boosting是串行的，而Bagging中所有的分类器是可以同时生成的（分类器之间无关系），而Boosting中则必须先生成第一个分类器，然后依次往后进行。核心思想是通过改变训练集进行有针对性的学习，通过每次更新迭代，增加错误样本的权重，减小正确样本的权重。知错就改，逐渐变好。典型应用为：Adaboost、GBDT和Xgboost。 6.1.4 Blending类似概率voting，用不相交的数据训练不同的Base Model，将它们的输出（概率值）取加权平均（两列数据偏差越小，且方差越大，则blend越有效果）。 ","date":"2022-03-24","objectID":"/feedback-prize/:6:1","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#61-融合方法介绍"},{"categories":["kaggle"],"content":"\r6.1 融合方法介绍\r6.1.1 VotingVoting可以说是一种最为简单的模型融合方式。假如对于一个二分类模型，有3个基础模型，那么就采取投票的方式，投票多者为最终的分类。 6.1.2 BaggingBagging的思想是利用抽样生成不同的训练集，进而训练不同的模型，将这些模型的输出结果综合（投票或平均的方式）得到最终的结果。 其本质是利用了模型的多样性，改善算法整体的效果。Bagging的重点在于不同训练集的生成，这里使用了一种名为Bootstrap的方法，即有放回的重复随机抽样，从而生成不同的数据集。 6.1.3 BoostingBoosting是一种提升算法，其思想是在算法迭代过程中，每次迭代构建新的分类器，重点关注被之前分类器分类错误的样本，如此迭代，最终加权平均所有分类器的结果，从而提升分类精度。 与Bagging相比来说最大的区别就是Boosting是串行的，而Bagging中所有的分类器是可以同时生成的（分类器之间无关系），而Boosting中则必须先生成第一个分类器，然后依次往后进行。核心思想是通过改变训练集进行有针对性的学习，通过每次更新迭代，增加错误样本的权重，减小正确样本的权重。知错就改，逐渐变好。典型应用为：Adaboost、GBDT和Xgboost。 6.1.4 Blending类似概率voting，用不相交的数据训练不同的Base Model，将它们的输出（概率值）取加权平均（两列数据偏差越小，且方差越大，则blend越有效果）。 ","date":"2022-03-24","objectID":"/feedback-prize/:6:1","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#611-voting"},{"categories":["kaggle"],"content":"\r6.1 融合方法介绍\r6.1.1 VotingVoting可以说是一种最为简单的模型融合方式。假如对于一个二分类模型，有3个基础模型，那么就采取投票的方式，投票多者为最终的分类。 6.1.2 BaggingBagging的思想是利用抽样生成不同的训练集，进而训练不同的模型，将这些模型的输出结果综合（投票或平均的方式）得到最终的结果。 其本质是利用了模型的多样性，改善算法整体的效果。Bagging的重点在于不同训练集的生成，这里使用了一种名为Bootstrap的方法，即有放回的重复随机抽样，从而生成不同的数据集。 6.1.3 BoostingBoosting是一种提升算法，其思想是在算法迭代过程中，每次迭代构建新的分类器，重点关注被之前分类器分类错误的样本，如此迭代，最终加权平均所有分类器的结果，从而提升分类精度。 与Bagging相比来说最大的区别就是Boosting是串行的，而Bagging中所有的分类器是可以同时生成的（分类器之间无关系），而Boosting中则必须先生成第一个分类器，然后依次往后进行。核心思想是通过改变训练集进行有针对性的学习，通过每次更新迭代，增加错误样本的权重，减小正确样本的权重。知错就改，逐渐变好。典型应用为：Adaboost、GBDT和Xgboost。 6.1.4 Blending类似概率voting，用不相交的数据训练不同的Base Model，将它们的输出（概率值）取加权平均（两列数据偏差越小，且方差越大，则blend越有效果）。 ","date":"2022-03-24","objectID":"/feedback-prize/:6:1","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#612-bagging"},{"categories":["kaggle"],"content":"\r6.1 融合方法介绍\r6.1.1 VotingVoting可以说是一种最为简单的模型融合方式。假如对于一个二分类模型，有3个基础模型，那么就采取投票的方式，投票多者为最终的分类。 6.1.2 BaggingBagging的思想是利用抽样生成不同的训练集，进而训练不同的模型，将这些模型的输出结果综合（投票或平均的方式）得到最终的结果。 其本质是利用了模型的多样性，改善算法整体的效果。Bagging的重点在于不同训练集的生成，这里使用了一种名为Bootstrap的方法，即有放回的重复随机抽样，从而生成不同的数据集。 6.1.3 BoostingBoosting是一种提升算法，其思想是在算法迭代过程中，每次迭代构建新的分类器，重点关注被之前分类器分类错误的样本，如此迭代，最终加权平均所有分类器的结果，从而提升分类精度。 与Bagging相比来说最大的区别就是Boosting是串行的，而Bagging中所有的分类器是可以同时生成的（分类器之间无关系），而Boosting中则必须先生成第一个分类器，然后依次往后进行。核心思想是通过改变训练集进行有针对性的学习，通过每次更新迭代，增加错误样本的权重，减小正确样本的权重。知错就改，逐渐变好。典型应用为：Adaboost、GBDT和Xgboost。 6.1.4 Blending类似概率voting，用不相交的数据训练不同的Base Model，将它们的输出（概率值）取加权平均（两列数据偏差越小，且方差越大，则blend越有效果）。 ","date":"2022-03-24","objectID":"/feedback-prize/:6:1","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#613-boosting"},{"categories":["kaggle"],"content":"\r6.1 融合方法介绍\r6.1.1 VotingVoting可以说是一种最为简单的模型融合方式。假如对于一个二分类模型，有3个基础模型，那么就采取投票的方式，投票多者为最终的分类。 6.1.2 BaggingBagging的思想是利用抽样生成不同的训练集，进而训练不同的模型，将这些模型的输出结果综合（投票或平均的方式）得到最终的结果。 其本质是利用了模型的多样性，改善算法整体的效果。Bagging的重点在于不同训练集的生成，这里使用了一种名为Bootstrap的方法，即有放回的重复随机抽样，从而生成不同的数据集。 6.1.3 BoostingBoosting是一种提升算法，其思想是在算法迭代过程中，每次迭代构建新的分类器，重点关注被之前分类器分类错误的样本，如此迭代，最终加权平均所有分类器的结果，从而提升分类精度。 与Bagging相比来说最大的区别就是Boosting是串行的，而Bagging中所有的分类器是可以同时生成的（分类器之间无关系），而Boosting中则必须先生成第一个分类器，然后依次往后进行。核心思想是通过改变训练集进行有针对性的学习，通过每次更新迭代，增加错误样本的权重，减小正确样本的权重。知错就改，逐渐变好。典型应用为：Adaboost、GBDT和Xgboost。 6.1.4 Blending类似概率voting，用不相交的数据训练不同的Base Model，将它们的输出（概率值）取加权平均（两列数据偏差越小，且方差越大，则blend越有效果）。 ","date":"2022-03-24","objectID":"/feedback-prize/:6:1","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#614-blending"},{"categories":["kaggle"],"content":"\r6.2 模型融合 Stacking交叉验证部分：首先将训练数据分为 5 份，接下来一共 5 个迭代，每次迭代时，将 4 份数据作为 Training Set 对每个 Base Model 进行训练，然后在剩下一份 Hold-out Set 上进行预测。同时也要将其在测试数据上的预测保存下来，对测试数据的全部做出预测。 5 个迭代都完成以后我们就获得了一个 训练数据行数 * Base Model数量 的矩阵，这个矩阵接下来作为第二层的 Model 的训练数据，训练完以后，对test data做预测。 这时我们得到了两个预测矩阵，平均后就得到最后的输出。 ","date":"2022-03-24","objectID":"/feedback-prize/:6:2","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#62-模型融合-stacking"},{"categories":["kaggle"],"content":"\r7 总的解决方案思路采用了 longformer+ deberta 的双模型融合，由于官方数据的有一些不干净的原标签，所以我们使用经过修复的corrected_train.csv。 在文本数据的处理上，我们将max_len设置在了1024（在推理是扩大至longformer=4096/deberta=2048）。之后我们对数据做了10Fold的标准切分。 模型上我们选择了 allenai/longformer-base-4096和microsoft/deberta-large 版本，在之后接了一个Dropout层和Linear层。 在模型预测出结果后，我们使用了后处理的方式来进一步筛选预测的实体，主要是对每种实体的最小长度和最小置信度做出限制，如果小于阈值则被后处理筛掉。 ","date":"2022-03-24","objectID":"/feedback-prize/:7:0","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#7-总的解决方案思路"},{"categories":["kaggle"],"content":"\r模型代码 class FeedbackModel(nn.Module): def __init__(self): super(FeedbackModel, self).__init__() # 载入 backbone if Config.model_savename == 'longformer': model_config = LongformerConfig.from_pretrained(Config.model_name) self.backbone = LongformerModel.from_pretrained(Config.model_name, config=model_config) else: model_config = AutoConfig.from_pretrained(Config.model_name) self.backbone = AutoModel.from_pretrained(Config.model_name, config=model_config) self.model_config = model_config self.dropout1 = nn.Dropout(0.1) self.dropout2 = nn.Dropout(0.2) self.dropout3 = nn.Dropout(0.3) self.dropout4 = nn.Dropout(0.4) self.dropout5 = nn.Dropout(0.5) self.head = nn.Linear(model_config.hidden_size, Config.num_labels) # 分类头 def forward(self, input_ids, mask): x = self.backbone(input_ids, mask) # 五个不同的dropout结果 logits1 = self.head(self.dropout1(x[0])) logits2 = self.head(self.dropout2(x[0])) logits3 = self.head(self.dropout3(x[0])) logits4 = self.head(self.dropout4(x[0])) logits5 = self.head(self.dropout5(x[0])) logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5 # 五层取平均 return logits ","date":"2022-03-24","objectID":"/feedback-prize/:7:1","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#模型代码"},{"categories":["kaggle"],"content":"\r后处理参数 # 每种实体的的最小长度阈值，小于阈值不识别 MIN_THRESH = { \"I-Lead\": 11, \"I-Position\": 7, \"I-Evidence\": 12, \"I-Claim\": 1, \"I-Concluding Statement\": 11, \"I-Counterclaim\": 6, \"I-Rebuttal\": 4, } # 每种实体的的最小置信度，小于阈值不识别 PROB_THRESH = { \"I-Lead\": 0.687, \"I-Position\": 0.537, \"I-Evidence\": 0.637, \"I-Claim\": 0.537, \"I-Concluding Statement\": 0.687, \"I-Counterclaim\": 0.37, \"I-Rebuttal\": 0.537, } 比赛上分历程： longformer Baseline 5Fold，Public LB : 0.678； 将推理阶段的max_len设置为4096，Public LB : 0.688； 加入后处理，Public LB : 0.694； 尝试了deberta-base 但分数太低，我们没有尝试将其加入融合； deberta-large 5Fold 加入后处理，Public LB : 0.705； 将两个模型融合，Public LB：0.709； 对学习率，epoch等进行调参，Public LB：0.712； 使用修复标签后的corrected_train.csv，Public LB：0.714； 尝试将5fold换成10fold，Public LB：0.716； 对后处理进行调参，Public LB：0.718； ","date":"2022-03-24","objectID":"/feedback-prize/:7:2","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#后处理参数"},{"categories":["kaggle"],"content":"\r总结竞赛是由乔治亚州立大学举办的，对学生写作中的论证和修辞元素进行识别。本次竞赛在数据上我们修复了官方数据的不干净的原标签部分，整体的方案上采用了 longformer + deberta 的双模型融合，为了防止过拟合我们尝试了在模型头部位置加入Dropout层。我们训练出了longformer-base-4096和deberta-large两个模型，再通过后处理对每种实体的最小长度和最小置信度做出限制，筛掉小于阈值的预测值，最后进行CV-10Fold和简单的加权融合。此外，我们还尝试了deberta-base等，但没有起效果。最终我们获得了Private LB: 0.718 (Top2%) 的成绩。 ","date":"2022-03-24","objectID":"/feedback-prize/:8:0","series":null,"tags":["nlp","python"],"title":"Feedback Prize - Evaluating Student Writing | Kaggle","uri":"/feedback-prize/#总结"},{"categories":null,"content":"\r视频\r","date":"2024-01-03","objectID":"/learning/should-i-have-children/:1:0","series":null,"tags":null,"title":"Should I have children? - CrowdScience, BBC World Service","uri":"/learning/should-i-have-children/#视频"},{"categories":["database"],"content":"\rInnoDB 起因： MySQL中数据存储是在物理磁盘上，而真正的数据处理是在内存中执行。（考虑到磁盘读写速度非常慢，频繁的操作性能差）。 解决方案： InooDB将一个表的数据划分为若干页（pages），以页作为磁盘与内存交互的基本单位，这些页通过 B-Tree索引联系起来，每页的默认大小为16kb（配置为 innodb_page_size）。 这样保证每次至少读取1页数据到内存或写入磁盘，减少了内存与磁盘的交互次数，提升性能。 遵循缓存设计思想： 时间维度：如果一条数据正在在被使用，那么在接下来一段时间内大概率还会再被使用。 可以认为热点数据缓存都属于这种思路的实现。 空间维度：如果一条数据正在在被使用，那么存储在它附近的数据大概率也会很快被使用。 InnoDB的数据页和操作系统的页缓存则是这种思路的体现。 其中，B-Tree索引就是聚簇索引（Clustered Index），这个索引的节点包含了所有列数据（也就是page）。二级索引的节点只有指向主键的指针。 ","date":"2023-11-12","objectID":"/innodb-page/:1:0","series":["db - mysql"],"tags":["mysql","innodb","database"],"title":"Innodb Page","uri":"/innodb-page/#innodb"},{"categories":["database"],"content":"\rPage Structure\rFile Header：存储页的通用信息。 Page Heade：存储数据页专有的信息。 Infimum + Supremum：页面中最小和最大记录。 User Records：用户存储的记录内容。 Free Space：页面中空闲空间。 Page Directory：存储页中记录的相对位置。 File Trailer：校验页是否完整。 ","date":"2023-11-12","objectID":"/innodb-page/:2:0","series":["db - mysql"],"tags":["mysql","innodb","database"],"title":"Innodb Page","uri":"/innodb-page/#page-structure"},{"categories":["database"],"content":"\rInfimum + Supremum 分别是最小记录和最大记录，属于MySQL为每个页添加的虚拟记录。 目的：防止插入新记录时需要移动已经存在的记录的位置。 最小记录永远位于页的最前面，最大记录永远位于页的最后面，这样可以避免记录移动，提高性能。 最小记录的记录头中 heap_no = 0，最大记录的记录头中 heap_no = 1，正式记录中的 heap_no从2开始。 最小记录的 record_type = 2，最大记录的 record_type = 3。 最小记录是页中单链表的头节点，最大记录是页中单链表的尾节点。 它们的存在有助于确保B+树的有序性。 ","date":"2023-11-12","objectID":"/innodb-page/:2:1","series":["db - mysql"],"tags":["mysql","innodb","database"],"title":"Innodb Page","uri":"/innodb-page/#infimum--supremum"},{"categories":["database"],"content":"\r行格式的选择 不同行格式适用于不同的使用情景： Compact：适用于常规的OLTP(联机事务处理)应用。 Dynamic \u0026 Compressed：适用与特定情况，比如包含大量变长字段或需要数据压缩的情况。 ","date":"2023-11-12","objectID":"/innodb-page/:3:0","series":["db - mysql"],"tags":["mysql","innodb","database"],"title":"Innodb Page","uri":"/innodb-page/#行格式的选择"},{"categories":["database"],"content":"\rCompact 行格式存储InnoDB中的一种行格式，用于存储整个记录的数据。 对于每个记录，Compact行格式将数据存储在本页中。 对于较大的字段或行溢出数据，会使用溢出页来存储。 比如 Text类型。如果页存在于聚簇索引上，那么这个节点数据就会过大，会一下读出很多页，也会降低读取效率（假如我们本身没有想获取 Text 列的情况下）。 因此，InnoDB对于变长字段，一般倾向于把它们存储到其他地方。如何存储，就和 **InnoDB行格式（Row Format）**有关。 行格式有四种：compact、redundant、dynamic \u0026 compressed。 可以通过创建或修改表的语句来指定 行格式： CREATE TABLE table_name( ... )ROW_FORMAT=行格式; ALTER TABLE table_name ROW_FORMAT=行格式; ","date":"2023-11-12","objectID":"/innodb-page/:4:0","series":["db - mysql"],"tags":["mysql","innodb","database"],"title":"Innodb Page","uri":"/innodb-page/#compact-行格式存储"},{"categories":["database"],"content":"\rCompact行格式结构\r","date":"2023-11-12","objectID":"/innodb-page/:4:1","series":["db - mysql"],"tags":["mysql","innodb","database"],"title":"Innodb Page","uri":"/innodb-page/#compact行格式结构"},{"categories":["database"],"content":"\rCompact行格式存储 - 变长字段长度列表 变长字段长度列表是compact行格式中的开头列表。 包含数据不为null，且类型为长度不定的，比如varchar、varbinary、text、json等。 不需要考虑它们到底占多少字节，省去列数据之间的边界定义。 存储格式：16进制。 排列顺序：16进制后的逆序排列。 这种排列方式有助于在查找长度信息时提高效率。 ","date":"2023-11-12","objectID":"/innodb-page/:4:2","series":["db - mysql"],"tags":["mysql","innodb","database"],"title":"Innodb Page","uri":"/innodb-page/#compact行格式存储---变长字段长度列表"},{"categories":["database"],"content":"\rCompact行格式存储 - NULL值列表 NULL值列表仅包含可以为NULL的字段，如果某个字段是 not null，这个字段就不会进入 NULL值列表中。 ","date":"2023-11-12","objectID":"/innodb-page/:4:3","series":["db - mysql"],"tags":["mysql","innodb","database"],"title":"Innodb Page","uri":"/innodb-page/#compact行格式存储---null值列表"},{"categories":["database"],"content":"\rCompact行格式存储 - 记录头信息 记录头固定为 5字节大小。 名称 大小(bits) 描述 无用位/预留位 2 目前没用到 deleted_flag 1 标记记录是否被删除 min_rec_flag 1 b+树种非叶子节点最小记录标记 n_owned 4 当前记录拥有记录数 heap_no 13 该记录在堆中的位置信息（堆中的序号） record_type 3 记录类型 next_record 16 页中下一条记录的相对位置 其中： delete_flag： 类型：0表示未删除；1表示删除。 未删除的记录不会立即从磁盘移除，而是先打上删除标记，所有被删除的记录会组成一个垃圾链表。 之后新插入的记录可能会重用到垃圾链表占用的空间，因此这个空间也称为可重用空间。 heap_no：表示当前记录在本页的位置 比如：heap_no=2，就表示这条记录在本页中的位置事2. 实际上，InnoDB会自动地为每页加上两条虚拟记录，即最小记录0和最大记录1。两条记录被单独地放在 Infimum 和 Supremum的部分，但还是会占用页中位置0和1。 record_type有4种类型： 普通记录：0（10进制）或 000（2进制）。 B+树非叶子节点记录：1（10进制）或 001（2进制）。 最小记录（伪记录的首记录）：2（10进制）或 010（2进制）。 最大记录（伪记录的尾记录）：3（10进制）或 011（2进制）。 next_record： 表示从当前记录的真实数据到下一条记录的真实数据的地址偏移量。 简单理解为单项链表，最小记录的下一个是第一条真实记录，最后一条真实记录的下一个是最大记录。 从heap_no角度来看，即，0 -\u003e 2 -\u003e … -\u003e n -\u003e 1，因为0是最小记录，1是最大记录，其中，n是最后一条真实记录。 注： 我们用箭头取代实际的偏移量，方便理解。 但在页中记录之间的实际顺序是通过 DB_ROW_ID 这个隐藏列来决定的。 ","date":"2023-11-12","objectID":"/innodb-page/:4:4","series":["db - mysql"],"tags":["mysql","innodb","database"],"title":"Innodb Page","uri":"/innodb-page/#compact行格式存储---记录头信息"},{"categories":["database"],"content":"\rCompact行格式存储 - 隐藏字段/列（属于记录的真实数据） 列名 大小（字节） 描述 DB_ROW_ID 6 用作主键ID（非表中主键） DB_TRX_ID 6 当前记录项的事务id DB_ROLL_PTR 7 undo log指针 DB_ROW_ID： 用作记录的物理标识符，不一定是主键。 这个字段用户无法直接查看： 因此你无法知道它里面存的数据究竟是什么格式。 可能是 ID值，可能是物理化的ID值，或者其他，都不为认知，我们直知道它和ID相关。 这个字段是可排序的（每个页中的记录依据它来排序）。 每页中记录排序顺序是根据该字段/列决定的。 它的生成规则： 优先使用用户自定义主键作为主键。 如果用户没定义主键，会选取一个unique键作为主键。 如果表中没有定义unique键，则会为表默认添加一个生成 DB_ROW_ID的隐藏列最为主键。 注：它不一定是全局唯一，只是在页内唯一。 DB_TRX_ID 用于记录当前记录项的事务ID，每个事务开始时，系统版本号（Transaction ID）会递增，因此它用于标识记录所属的事务。 每开始一个新事务，系统版本号会自动递增，而事务开始时刻的系统版本号回作为事务id，事务commit的话，就会更新这里。 DB_ROLL_PTR 指向当前记录项的 undo log，用于撤销和回滚事务所作的更改。 如果发生事务回滚，系统会从此字段来查找之前版本的数据，以将记录恢复到事务开始之前的状态。 ","date":"2023-11-12","objectID":"/innodb-page/:4:5","series":["db - mysql"],"tags":["mysql","innodb","database"],"title":"Innodb Page","uri":"/innodb-page/#compact行格式存储---隐藏字段列属于记录的真实数据"},{"categories":["database"],"content":"\rCompact行格式存储 - 行溢出数据（记录的真实数据）varchar(m)最多能存储的数据： MySQL对一条记录占用的最大存储空间是有限制的，除BLONB或者TEXT类型的列外，其他所有列（不包括隐藏列和记录头信息）占用的字节长度加起来不应该超过 65535个字节。 不严谨的认为，MySQL一行记录占用的存储空间不能超过65535个字节。 实际上，65535个字节包括： 真实数据。 真实数据占用的字节长度。 NULL值标识（如果是 not null就无此空间）。 而MySQL中磁盘与内存交互的基本单位是页，一般为16kb，16384个字节。而一行记录最大可以是65535个字节。 造成了一页存不下一行数据的情况。 在 Compact 和 Redundant 行格式中，对于占用存储空间大的列，在记录真实数据处只会存储该列的一部分数据。把剩余的数据分散存储在几个其他页中，然后在记录的真实数据处用20个字节存储指向这些页的地址，从未可以找到剩余数据的所在页。 注：本记录只会存储该列的前768个字节的数据 和 一个指向其他页的地址。剩下的数据会存放的其他页。 这种情况就叫做 行溢出，存储超过768字节数据的那些页面叫做页内溢出页（Overflow Pages）,这些页内溢出页中存储了行溢出数据的一部分。 注意：上述都是Compact 和 Redundant行格式中处理行溢出数据 InnoDB存储引擎中，默认的行格式是Dynamic。并且Dynamic与Compact很像，只是在处理行溢出数据上有差异。 其中，Dynamic 不会在记录的真实数据处存放 前786个字节，而是将所有字节都存储在其他页面，这些页面称为溢出页。 另外，Compressed 行格式 会采用 压缩算法对页面进行压缩，以节省空间。它通常不会涉及行溢出数据的问题，因为，它的设计目的时在页内压缩数据，而不是将数据存储在其他页中。 查询InnoDB默认行格式： show variables like 'innodb_default_row_format'; ","date":"2023-11-12","objectID":"/innodb-page/:4:6","series":["db - mysql"],"tags":["mysql","innodb","database"],"title":"Innodb Page","uri":"/innodb-page/#compact行格式存储---行溢出数据记录的真实数据"},{"categories":["database"],"content":"\rPage Directory (页目录)（这段和ChatGPT描述的有出入） 它是InnoDB存储引擎内部的一个机制，用于帮助管理和加速数据页的查找，而不是直接管理记录。 ChatGPT解释：Page Directory时用来定位数据页的工具，其原理不是通过分组实现。（我看网上教程写的是分组）？？？？？？？？ 功能：保证数据量较大时的查找数据的速度。 解决的问题：在页中，记录是按照主键大小正序串联成一个单链表。如果在数据量较大的时候根据主键进行遍历查询，效率太差。 大致原理： 将所有正常的记录（包括最大、最小记录，不包括标记为已删除的记录）划分为几个组。（具体怎么划分还不确定） 每个组的最后一条记录（也就是最大的记录）的头信息中的 n_owned属性表示该组内共有几条数据。 将每组的最后一条记录的地址偏移量单独提取出来按顺序存储到靠近页尾部的地方，这个位置就是 Page Directory。 规定： 对于最小记录所在的分组只能有1条记录，最大记录所在的分组拥有的记录条数只能在 1-8条之间，剩下的分组中记录的条数范围只能在4-8条之间。 如图中，记录共有18条，InnoDB会把它们分成5组，第一组中只有一个最小记录： 如何通过page directory查找指定主键值的记录： 通过二分法确定该记录所在的槽，并找到该槽所在分组中主键值最小的记录。 通过记录的next_record属性遍历该槽所在组中的各个记录。 ","date":"2023-11-12","objectID":"/innodb-page/:5:0","series":["db - mysql"],"tags":["mysql","innodb","database"],"title":"Innodb Page","uri":"/innodb-page/#page-directory-页目录"},{"categories":["database"],"content":"\rPage Header 专门用来存储数据页相关的各种状态信息。 PAGE_N_DIR_SLOTS：表示 Page Directory 中的槽数量。 PAGE_HEAP_TOP：Free Space 的起始地址。Free Space 是数据页中尚未被记录使用的空间。 PAGE_N_HEAP：当前数据页中的记录数量，但不包括已经标记为删除的记录。 PAGE_FREE \u0026 PAGE_GARBAGE 共同表示了已删除记录的管理，已删除的记录通过链表结构组织起来，可以重新被利用。 PAGE_FREE：第一个已标记为删除的记录的地址（各个已删除的地址通过 next_record也会组成一个单链表）。 PAGE_GARBAGE：已删除记录占用的字节数。 PAGE_LAST_INSERT：最后插入记录的位置。 PAGE_DIRECTION：最后一条记录插入方向。 PAGE_N_DIRECTION：一个方向连续插入的记录数（如果最后一条记录的插入方向改变，这个状态值会被清零重新统计）。 PAGE_N_RECS：数据页中的记录数量，不包括最小和最大记录以及标记为删除的记录。 PAGE_MAX_TRX_ID：用于标识对当前数据页进行修改的最大事务ID。 PAGE_LEVEL：用于标识数据页在 B+ 树索引中的层级位置。 PAGE_INDEX_ID：标识了数据页所属的索引ID。 PAGE_BTR_SEG_LEAF \u0026 PAGE_BTR_SEG_TOP：用于标识 B+ 树叶子段和非叶子段的头部信息。仅在B+树的Root页定义。 ","date":"2023-11-12","objectID":"/innodb-page/:6:0","series":["db - mysql"],"tags":["mysql","innodb","database"],"title":"Innodb Page","uri":"/innodb-page/#page-header"},{"categories":["database"],"content":"\rFile Header用来描述各种页都适用的通用信息： FIL_PAGE_SPACE_OR_CHKSUM 页的校验和（checksum值）。 校验和是通过对页的内容进行计算得到的值，用于验证页面内容的完整性。有助于检测数据文件在存储或传输过程中是否发生了损坏。 FIL_PAGE_OFFSET：为每一个页都有一个唯一的页号 这个页号标识了数据文件中的页的位置，允许 InnoDB 存储引擎精确定位到特定页。 FIL_PAGE_PREV 上一个页的页号。 FIL_PAGE_NEXT 下一个页的页号。 这两个属性用于构建双向链表，将数据文件中的页连接在一起。这对于页的管理和遍历非常有用。 FIL_PAGE_LSN 页面被最后修改时对应的日志序列位置（英文名是：Log Sequence Number）。 FIL_PAGE_TYPE：当前页的类型。 InnoDB 将不同类型的页用于不同的目的，例如数据页、索引页等。这个属性标识了当前页的用途。 FIL_PAGE_FILE_FLUSH_LSN 仅在系统表空间的一个页中定义，代表文件至少被刷新到了对应的LSN值。 FIL_PAGE_ARCH_LOG_NO_OR_SPACE_ID 页属于哪个表空间。 ","date":"2023-11-12","objectID":"/innodb-page/:7:0","series":["db - mysql"],"tags":["mysql","innodb","database"],"title":"Innodb Page","uri":"/innodb-page/#file-header"},{"categories":["database"],"content":"\rFile Trailer MySQL中内存和磁盘的基本交互单位是页。如果内存中页被修改了，那么某个时刻一定会将内存页同步到磁盘中。如果在同步的过程中，系统出现问题，就可能导致磁盘中的页数据没能完全同步，也就是发生了脏页的情况。为了避免发生这种问题，mysql在每个页的尾部加上了File Trailer来校验页的完整性。 File Trailer由8个字节组成： 前4个字节代表页的校验和：这个部分是和File Header中的校验和相对应的。 File Header和File Trailer都有校验和，如果两者一致则表示数据页是完整的。否则，则表示数据页是脏页。 后4个字节代表页面被最后修改时对应的日志序列位置（LSN）。 校验和（Checksum）：前4个字节表示页的校验和。校验和是通过对页的内容应用某种算法而得到的值，它用于验证页面内容的完整性。File Header 和 File Trailer 中的校验和应该是一致的。如果它们不一致，这可能表示页面在同步到磁盘时发生了错误或中断，即发生了脏页的情况。校验和的匹配是确保页面完整性的关键。 日志序列位置（LSN）：后4个字节表示页面被最后修改时对应的日志序列位置（LSN）。这个值记录了页面最后一次修改的日志位置，用于恢复和崩溃恢复。通过比较 File Header 和 File Trailer 中的 LSN，可以确定页面是否已经完全同步到磁盘。如果 LSN 不一致，表示页面的修改可能没有完全刷入磁盘。 ","date":"2023-11-12","objectID":"/innodb-page/:8:0","series":["db - mysql"],"tags":["mysql","innodb","database"],"title":"Innodb Page","uri":"/innodb-page/#file-trailer"},{"categories":["java"],"content":"\rStatic Class 静态类可以在一个类里面定义静态类，比如，内部类 (nested class)。把nested class封闭起来的叫外部类。只有内部类才可以static。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:0","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#static-class-静态类"},{"categories":["java"],"content":"\r静态内部类和非静态内部类的不同 内部静态类不需要有指向外部类的引用，但非静态内部类需要持有对外部类的引用。 非静态内部类能够访问外部类的静态和非静态成员，静态类不能够访问外部类的非静态成员，智能访问外部类的静态成员。 非静态内部类不能够脱离外部类实体被创建，它可以访问外部类的数据和方法，因为它就在外部类里面 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#静态内部类和非静态内部类的不同"},{"categories":["java"],"content":"\rThe difference between Comparable and ComparatorComparable是一个对像本身就已经支持自比较所需要实现的接口（如 String、Integer自己就可以完成比较大小操作，已经实现了该接口）。 Comparable接口只提供了 int compareTo(T o)方法，也就是说假如我定义了一个Person类，这个类实现了 Comparable接口，那么当我实例化Person类的person1后，我想比较person1和一个现有的Person对象person2的大小时，我就可以这样来调用：person1.comparTo(person2),通过返回值就可以判断了；而此时如果你定义了一个 PersonComparator（实现了Comparator接口）的话，那你就可以这样：PersonComparator comparator= new PersonComparator(); comparator.compare(person1,person2)；。 Comparator是一个专用的比较器，当前对象不支持自比较或者自比较函数不能够满足要求时，可以写一个比较器来完成对象之间的大小比较。 Comparator定义了俩个方法，分别是 int compare 和 boolean equals，用于比较两个Comparator是否相等。 注意：有时在实现Comparator接口时，并没有实现equals方法，可程序并没有报错，原因是实现该接口的类也是Object类的子类，而Object类已经实现了equals方法. ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:2","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#the-difference-between-comparable-and-comparator"},{"categories":["java"],"content":"\rIterator 迭代器迭代器是一种设计模式，是一个对象，遍历选择序列中的元素。 .next() 向下移动指针，返回下一个元素，如果没有元素，就报异常。（当调用.next()，会自动调用.remove()，也就是说迭代器遍历一次，该位置的元素会被清除） .hasNext() 返回True表示有下个元素，false表示没有元素了，此时只判断下一个元素的有无，并不移动指针。 .remove() 删除的是指针指向的元素，当前指针没有元素，那么会抛异常。 外部迭代 public static void traditionalIterator(ArrayList\u003cArtist\u003e artList) { int count = 0; Iterator\u003cArtist\u003e iterator = artList.iterator(); System.out.println(iterator.hasNext()); while (iterator.hasNext()) { iterator.next(); count++; } System.out.println(\"外部迭代器：\"+count); } 使用迭代器的方法调用流程：4次 hasNext() 查询 +1 hasNext() 返回查询值 +1 next() 移动指针 +1 next() 返回当前指针的元素值 +1 内部迭代 public static void streamIterator(ArrayList\u003cArtist\u003e artList) { long count = artList.stream(). filter(artist -\u003e 0 != artist.getAge()) .count(); System.out.println(\"内部迭代器：\"+count); } 使用stream类库后的方法调用流程：2次 构建操作 返回结果 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:3","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#iterator-迭代器"},{"categories":["java"],"content":"\rIterator 迭代器迭代器是一种设计模式，是一个对象，遍历选择序列中的元素。 .next() 向下移动指针，返回下一个元素，如果没有元素，就报异常。（当调用.next()，会自动调用.remove()，也就是说迭代器遍历一次，该位置的元素会被清除） .hasNext() 返回True表示有下个元素，false表示没有元素了，此时只判断下一个元素的有无，并不移动指针。 .remove() 删除的是指针指向的元素，当前指针没有元素，那么会抛异常。 外部迭代 public static void traditionalIterator(ArrayList artList) { int count = 0; Iterator iterator = artList.iterator(); System.out.println(iterator.hasNext()); while (iterator.hasNext()) { iterator.next(); count++; } System.out.println(\"外部迭代器：\"+count); } 使用迭代器的方法调用流程：4次 hasNext() 查询 +1 hasNext() 返回查询值 +1 next() 移动指针 +1 next() 返回当前指针的元素值 +1 内部迭代 public static void streamIterator(ArrayList artList) { long count = artList.stream(). filter(artist -\u003e 0 != artist.getAge()) .count(); System.out.println(\"内部迭代器：\"+count); } 使用stream类库后的方法调用流程：2次 构建操作 返回结果 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:3","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#外部迭代"},{"categories":["java"],"content":"\rIterator 迭代器迭代器是一种设计模式，是一个对象，遍历选择序列中的元素。 .next() 向下移动指针，返回下一个元素，如果没有元素，就报异常。（当调用.next()，会自动调用.remove()，也就是说迭代器遍历一次，该位置的元素会被清除） .hasNext() 返回True表示有下个元素，false表示没有元素了，此时只判断下一个元素的有无，并不移动指针。 .remove() 删除的是指针指向的元素，当前指针没有元素，那么会抛异常。 外部迭代 public static void traditionalIterator(ArrayList artList) { int count = 0; Iterator iterator = artList.iterator(); System.out.println(iterator.hasNext()); while (iterator.hasNext()) { iterator.next(); count++; } System.out.println(\"外部迭代器：\"+count); } 使用迭代器的方法调用流程：4次 hasNext() 查询 +1 hasNext() 返回查询值 +1 next() 移动指针 +1 next() 返回当前指针的元素值 +1 内部迭代 public static void streamIterator(ArrayList artList) { long count = artList.stream(). filter(artist -\u003e 0 != artist.getAge()) .count(); System.out.println(\"内部迭代器：\"+count); } 使用stream类库后的方法调用流程：2次 构建操作 返回结果 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:3","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#内部迭代"},{"categories":["java"],"content":"\r函数式编程 Stream 流 artList.stream().filter(artist -\u003e 0 != artist.getAge()); long count = artList.stream(). filter(artist -\u003e 0 != artist.getAge()) .count(); 只过滤不记数像 filter 这样只描述Stream，最终不产生新集合的方法叫做惰性求值方法 而像 count 这样最终会从 Stream 产生值的方法叫做 及早求值方法。 由于使用了惰性求值，没有输出艺术家名字 // 它本身有返回值，是Stream类型，因为不需要进一步操作返回值，所以就写成这样 artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }); 输出艺术家名字 long getName = artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }).count(); 输出结果：将集合中所有的元素都打印出来，和return返回什么无关，只是打印getName。 阿凡达 奇异博士 前面两个区别，前者是惰性求值（个人理解是没有返回），因此不会打印。而同样的语句在后者，因为加入了一个终止操作的流，就比如记数操作，名字就被会打印。（不懂其原理） 解答判断一个操作是惰性求值还是及早求值的方法，关注其返回值 返回值是Stream就是惰性操作。（不会展示打印内容） 返回值是其它类型或者空，就是及早操作。（会展示打印内容） Stream\u003cArtist\u003e getNameStream = artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }); long getName = artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }).count(); 使用这些操作的理想方式是形成一个惰性求值的链，最后用一个及早求值的操作返回想要的结果，这正是它的合理之处。 整个过程和建造者模式有共同之处。建造者模式使用一系列操作设置属性和配置，最后调用一个build方法，这时，对象才被真正创建。 为什么要区分惰性求值和及早求值只有在对需要什么样的结果和操作有了了解后，才能更有效率地进行计算。 比如，如果要找到大于10的第一个数，并不需要和所与元素进行不交，只需要找到第一个匹配的元素即可。这意味着可以在集合类上级联多种操作，但迭代只需要一次。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:4","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#函数式编程-stream-流"},{"categories":["java"],"content":"\r函数式编程 Stream 流 artList.stream().filter(artist -\u003e 0 != artist.getAge()); long count = artList.stream(). filter(artist -\u003e 0 != artist.getAge()) .count(); 只过滤不记数像 filter 这样只描述Stream，最终不产生新集合的方法叫做惰性求值方法 而像 count 这样最终会从 Stream 产生值的方法叫做 及早求值方法。 由于使用了惰性求值，没有输出艺术家名字 // 它本身有返回值，是Stream类型，因为不需要进一步操作返回值，所以就写成这样 artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }); 输出艺术家名字 long getName = artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }).count(); 输出结果：将集合中所有的元素都打印出来，和return返回什么无关，只是打印getName。 阿凡达 奇异博士 前面两个区别，前者是惰性求值（个人理解是没有返回），因此不会打印。而同样的语句在后者，因为加入了一个终止操作的流，就比如记数操作，名字就被会打印。（不懂其原理） 解答判断一个操作是惰性求值还是及早求值的方法，关注其返回值 返回值是Stream就是惰性操作。（不会展示打印内容） 返回值是其它类型或者空，就是及早操作。（会展示打印内容） Stream getNameStream = artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }); long getName = artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }).count(); 使用这些操作的理想方式是形成一个惰性求值的链，最后用一个及早求值的操作返回想要的结果，这正是它的合理之处。 整个过程和建造者模式有共同之处。建造者模式使用一系列操作设置属性和配置，最后调用一个build方法，这时，对象才被真正创建。 为什么要区分惰性求值和及早求值只有在对需要什么样的结果和操作有了了解后，才能更有效率地进行计算。 比如，如果要找到大于10的第一个数，并不需要和所与元素进行不交，只需要找到第一个匹配的元素即可。这意味着可以在集合类上级联多种操作，但迭代只需要一次。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:4","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#只过滤不记数"},{"categories":["java"],"content":"\r函数式编程 Stream 流 artList.stream().filter(artist -\u003e 0 != artist.getAge()); long count = artList.stream(). filter(artist -\u003e 0 != artist.getAge()) .count(); 只过滤不记数像 filter 这样只描述Stream，最终不产生新集合的方法叫做惰性求值方法 而像 count 这样最终会从 Stream 产生值的方法叫做 及早求值方法。 由于使用了惰性求值，没有输出艺术家名字 // 它本身有返回值，是Stream类型，因为不需要进一步操作返回值，所以就写成这样 artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }); 输出艺术家名字 long getName = artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }).count(); 输出结果：将集合中所有的元素都打印出来，和return返回什么无关，只是打印getName。 阿凡达 奇异博士 前面两个区别，前者是惰性求值（个人理解是没有返回），因此不会打印。而同样的语句在后者，因为加入了一个终止操作的流，就比如记数操作，名字就被会打印。（不懂其原理） 解答判断一个操作是惰性求值还是及早求值的方法，关注其返回值 返回值是Stream就是惰性操作。（不会展示打印内容） 返回值是其它类型或者空，就是及早操作。（会展示打印内容） Stream getNameStream = artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }); long getName = artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }).count(); 使用这些操作的理想方式是形成一个惰性求值的链，最后用一个及早求值的操作返回想要的结果，这正是它的合理之处。 整个过程和建造者模式有共同之处。建造者模式使用一系列操作设置属性和配置，最后调用一个build方法，这时，对象才被真正创建。 为什么要区分惰性求值和及早求值只有在对需要什么样的结果和操作有了了解后，才能更有效率地进行计算。 比如，如果要找到大于10的第一个数，并不需要和所与元素进行不交，只需要找到第一个匹配的元素即可。这意味着可以在集合类上级联多种操作，但迭代只需要一次。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:4","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#由于使用了惰性求值没有输出艺术家名字"},{"categories":["java"],"content":"\r函数式编程 Stream 流 artList.stream().filter(artist -\u003e 0 != artist.getAge()); long count = artList.stream(). filter(artist -\u003e 0 != artist.getAge()) .count(); 只过滤不记数像 filter 这样只描述Stream，最终不产生新集合的方法叫做惰性求值方法 而像 count 这样最终会从 Stream 产生值的方法叫做 及早求值方法。 由于使用了惰性求值，没有输出艺术家名字 // 它本身有返回值，是Stream类型，因为不需要进一步操作返回值，所以就写成这样 artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }); 输出艺术家名字 long getName = artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }).count(); 输出结果：将集合中所有的元素都打印出来，和return返回什么无关，只是打印getName。 阿凡达 奇异博士 前面两个区别，前者是惰性求值（个人理解是没有返回），因此不会打印。而同样的语句在后者，因为加入了一个终止操作的流，就比如记数操作，名字就被会打印。（不懂其原理） 解答判断一个操作是惰性求值还是及早求值的方法，关注其返回值 返回值是Stream就是惰性操作。（不会展示打印内容） 返回值是其它类型或者空，就是及早操作。（会展示打印内容） Stream getNameStream = artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }); long getName = artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }).count(); 使用这些操作的理想方式是形成一个惰性求值的链，最后用一个及早求值的操作返回想要的结果，这正是它的合理之处。 整个过程和建造者模式有共同之处。建造者模式使用一系列操作设置属性和配置，最后调用一个build方法，这时，对象才被真正创建。 为什么要区分惰性求值和及早求值只有在对需要什么样的结果和操作有了了解后，才能更有效率地进行计算。 比如，如果要找到大于10的第一个数，并不需要和所与元素进行不交，只需要找到第一个匹配的元素即可。这意味着可以在集合类上级联多种操作，但迭代只需要一次。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:4","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#输出艺术家名字"},{"categories":["java"],"content":"\r函数式编程 Stream 流 artList.stream().filter(artist -\u003e 0 != artist.getAge()); long count = artList.stream(). filter(artist -\u003e 0 != artist.getAge()) .count(); 只过滤不记数像 filter 这样只描述Stream，最终不产生新集合的方法叫做惰性求值方法 而像 count 这样最终会从 Stream 产生值的方法叫做 及早求值方法。 由于使用了惰性求值，没有输出艺术家名字 // 它本身有返回值，是Stream类型，因为不需要进一步操作返回值，所以就写成这样 artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }); 输出艺术家名字 long getName = artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }).count(); 输出结果：将集合中所有的元素都打印出来，和return返回什么无关，只是打印getName。 阿凡达 奇异博士 前面两个区别，前者是惰性求值（个人理解是没有返回），因此不会打印。而同样的语句在后者，因为加入了一个终止操作的流，就比如记数操作，名字就被会打印。（不懂其原理） 解答判断一个操作是惰性求值还是及早求值的方法，关注其返回值 返回值是Stream就是惰性操作。（不会展示打印内容） 返回值是其它类型或者空，就是及早操作。（会展示打印内容） Stream getNameStream = artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }); long getName = artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }).count(); 使用这些操作的理想方式是形成一个惰性求值的链，最后用一个及早求值的操作返回想要的结果，这正是它的合理之处。 整个过程和建造者模式有共同之处。建造者模式使用一系列操作设置属性和配置，最后调用一个build方法，这时，对象才被真正创建。 为什么要区分惰性求值和及早求值只有在对需要什么样的结果和操作有了了解后，才能更有效率地进行计算。 比如，如果要找到大于10的第一个数，并不需要和所与元素进行不交，只需要找到第一个匹配的元素即可。这意味着可以在集合类上级联多种操作，但迭代只需要一次。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:4","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#解答"},{"categories":["java"],"content":"\r函数式编程 Stream 流 artList.stream().filter(artist -\u003e 0 != artist.getAge()); long count = artList.stream(). filter(artist -\u003e 0 != artist.getAge()) .count(); 只过滤不记数像 filter 这样只描述Stream，最终不产生新集合的方法叫做惰性求值方法 而像 count 这样最终会从 Stream 产生值的方法叫做 及早求值方法。 由于使用了惰性求值，没有输出艺术家名字 // 它本身有返回值，是Stream类型，因为不需要进一步操作返回值，所以就写成这样 artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }); 输出艺术家名字 long getName = artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }).count(); 输出结果：将集合中所有的元素都打印出来，和return返回什么无关，只是打印getName。 阿凡达 奇异博士 前面两个区别，前者是惰性求值（个人理解是没有返回），因此不会打印。而同样的语句在后者，因为加入了一个终止操作的流，就比如记数操作，名字就被会打印。（不懂其原理） 解答判断一个操作是惰性求值还是及早求值的方法，关注其返回值 返回值是Stream就是惰性操作。（不会展示打印内容） 返回值是其它类型或者空，就是及早操作。（会展示打印内容） Stream getNameStream = artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }); long getName = artList.stream().filter(artist -\u003e { System.out.println(artist.getName()); return artist.getName().equals(\"阿凡达\"); }).count(); 使用这些操作的理想方式是形成一个惰性求值的链，最后用一个及早求值的操作返回想要的结果，这正是它的合理之处。 整个过程和建造者模式有共同之处。建造者模式使用一系列操作设置属性和配置，最后调用一个build方法，这时，对象才被真正创建。 为什么要区分惰性求值和及早求值只有在对需要什么样的结果和操作有了了解后，才能更有效率地进行计算。 比如，如果要找到大于10的第一个数，并不需要和所与元素进行不交，只需要找到第一个匹配的元素即可。这意味着可以在集合类上级联多种操作，但迭代只需要一次。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:4","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#为什么要区分惰性求值和及早求值"},{"categories":["java"],"content":"\r常用的流操作\rcollect(toList())collect(toList()) 方法由Stream里的值生成要给列表，是一个及早求值操作。 Stream的of方法使用一组初始值生成新的Stream。 // 创建List List\u003cString\u003e list = Stream.of(\"a\", \"b\", \"c\", \"d\").collect(Collectors.toList()); // 方式2 该方法应该是上面的进化版，更简洁 List\u003cString\u003e list = Stream.of(\"a\", \"b\", \"c\", \"d\").toList(); 首先由列表生成一个Stream，然后进行一个Stream上的操作，继而是collect操作，由Stream声称列表。 map如果有一个函数可以将一种类型的值转换成另外一种类型，map操作就可以使用该函数，讲一个流中的值转成一个新的流。 比如，将一组字符串转换成大写形式，传统方式是在循环中对每个字符串调用 toUppercase方法，然后将得到的结果加入到一个新的列表。 传统小写变大写的方法 ArrayList\u003cString\u003e collected = new ArrayList\u003c\u003e(); for (String str : asList(\"a\",\"b\",\"hello\")) { String s = str.toUpperCase(); collected.add(s); } ··· Stream小写变大写 ```java List\u003cString\u003e collect = Stream.of(\"a\", \"b\", \"hello\") .map(str -\u003e str.toUpperCase()) .collect(Collectors.toList()); // 简洁版 List\u003cString\u003e collect = Stream.of(\"a\", \"b\", \"hello\") .map(String::toUpperCase).toList(); 传给map的Lambda表达式只接受一个String类型的参数，返回一个新的String。参数和返回值不必属于同一种类型，但是Lambda表达式必须是Function接口的一个实例，Function接口是只包含一个参数的普通函数接口。 filter遍历数据并检查其中的元素时，可以尝试使用filter。主要作用就是筛选出有用的信息。 例：找出一组字符串中以数字开头的字符串， 传统方法： ArrayList\u003cString\u003e list = new ArrayList\u003c\u003e(); for (String str : asList(\"a\", \"1a\", \"a1\", \"2b\")) { if (Character.isDigit(str.charAt(0))) { list.add(str); } } Stream流方式： List\u003cString\u003e list = Stream.of(\"a\", \"1a\", \"a1\", \"2b\") .filter(s -\u003e Character.isDigit(s.charAt(0))) .toList(); 和map很像，filter接受要给函数作为参数，该函数用Lambda表达式表示。如果for循环中的if条件芋圆可以被filter代替。他们的返回值都是true或false，来过滤Stream中符合条件的。 flatMapflatMap可以将一个类中不同属性但是同类型的元素装载到一个Stream中，但是map做不到 list.stream() .flatMap(artist -\u003e Stream.of(art.getName(), art.getNationality())) .toList(); list.stream() .map(artist -\u003e artist.getName()) .toList(); 这样返回的集合就是说我们想要的艺术家的年龄的集合。 flatMap 和 mapmap可用一个新的值代替Stream中的值。而flatMap是生成新的Stream对象取代之前，或者是之前都n个流对象，flatMap将他们整合后变成一个。 List\u003cInteger\u003e together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(numbers -\u003e numbers.stream()) .toList(); // Idea推荐写法 List\u003cInteger\u003e together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(Collection::stream) .toList(); max \u0026 min求最值 // 书中给出的写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(artist -\u003e artist.getAge())) .get(); // Idea推荐写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(Artist::getAge)) .get(); Artist maxArtistAge = list.stream() .max(Comparator.comparing(Artist::getAge)) .get(); Optional\u003cArtist\u003e max = list.stream() .max(Comparator.comparing(Artist::getAge)); 排序的时候需要用到 Comparator 比较器对象。Java8中提供了新的静态方法 comparing()，使用它可以方便地实现一个比较器。之前需要比较两个对象的某个属性值。现在一个存取方法就够了。 需要深入研究comparing方法，该方法接受一个函数并返回另一个函数，该方法本该早已加入Java标准库，但是由于匿名内部类可读性差且书写冗长，一直未实现。现在在Lambda表达式，可以简单的完成。 调用空Stream的max/min方法，返回的是Optional对象，它代表一个可能存在也可能不存在的值。它里面的实际的值就是我们要得到的对象是在 .max().get() / .min()/get() 得到的。 通用模式max和min方法都属于更通用的一种编程模式 使用for循环重写上面的代码： List\u003cArtist\u003e list = List.of(new Artist(11, \"China\", \"Sam\"), new Artist(9, \"UK\", \"Die\"), new Artist(22, \"US\", \"Daming\")); Artist artist = list.get(0); for(Artist obj : list){ if(obj.getAge() \u003c artist.getAge()){ artist = obj; } } reduce 聚合方法加减乘除都行。把Stream的所有的元素按照聚合函数聚合成一个结果 传统reduce模式： List\u003cInteger\u003e numbers = List.of(1, 2, 3, 4); Integer accumulator = 0; for(Integer element : numbers){ accumulator = element + accumulator; } Stream中reduce模式： Integer count = Stream.of(1, 2, 3, 4) .reduce(0, (acc, element) -\u003e acc + element); // idea推荐写法，0表示和列表中进行计算的数字，Integer表示类型，sum表示计算方式 Integer count = Stream.of(1, 2, 3, 4) .reduce(0, Integer::sum); 展开reduce操作： ```java // 在这定义两个元素是相加的 BinaryOperator\u003cInteger\u003e accumulator = (acc, element) -\u003e acc + element; // 或者 BinaryOperator\u003cInteger\u003e accumulator1 = Integer::sum; // 这里给accumulator添加元素，apply里就是两个元素 // 这两个元素会按照accumulator中的运算法则进行运算 int count = accumulator1.apply( accumulator1.apply( accumulator1.apply( accumulator1.apply(0, 1), 2), 3), 4); Reduce参数： Performs a reduction on the elements of this stream, using the provided identity value and an associative accumulation function, and returns the reduced value. This is equivalent to: T result = identity; for (T element : this stream) result = accumulator.apply(result, element) return result; ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:5","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#常用的流操作"},{"categories":["java"],"content":"\r常用的流操作\rcollect(toList())collect(toList()) 方法由Stream里的值生成要给列表，是一个及早求值操作。 Stream的of方法使用一组初始值生成新的Stream。 // 创建List List list = Stream.of(\"a\", \"b\", \"c\", \"d\").collect(Collectors.toList()); // 方式2 该方法应该是上面的进化版，更简洁 List list = Stream.of(\"a\", \"b\", \"c\", \"d\").toList(); 首先由列表生成一个Stream，然后进行一个Stream上的操作，继而是collect操作，由Stream声称列表。 map如果有一个函数可以将一种类型的值转换成另外一种类型，map操作就可以使用该函数，讲一个流中的值转成一个新的流。 比如，将一组字符串转换成大写形式，传统方式是在循环中对每个字符串调用 toUppercase方法，然后将得到的结果加入到一个新的列表。 传统小写变大写的方法 ArrayList collected = new ArrayList\u003c\u003e(); for (String str : asList(\"a\",\"b\",\"hello\")) { String s = str.toUpperCase(); collected.add(s); } ··· Stream小写变大写 ```java List collect = Stream.of(\"a\", \"b\", \"hello\") .map(str -\u003e str.toUpperCase()) .collect(Collectors.toList()); // 简洁版 List collect = Stream.of(\"a\", \"b\", \"hello\") .map(String::toUpperCase).toList(); 传给map的Lambda表达式只接受一个String类型的参数，返回一个新的String。参数和返回值不必属于同一种类型，但是Lambda表达式必须是Function接口的一个实例，Function接口是只包含一个参数的普通函数接口。 filter遍历数据并检查其中的元素时，可以尝试使用filter。主要作用就是筛选出有用的信息。 例：找出一组字符串中以数字开头的字符串， 传统方法： ArrayList list = new ArrayList\u003c\u003e(); for (String str : asList(\"a\", \"1a\", \"a1\", \"2b\")) { if (Character.isDigit(str.charAt(0))) { list.add(str); } } Stream流方式： List list = Stream.of(\"a\", \"1a\", \"a1\", \"2b\") .filter(s -\u003e Character.isDigit(s.charAt(0))) .toList(); 和map很像，filter接受要给函数作为参数，该函数用Lambda表达式表示。如果for循环中的if条件芋圆可以被filter代替。他们的返回值都是true或false，来过滤Stream中符合条件的。 flatMapflatMap可以将一个类中不同属性但是同类型的元素装载到一个Stream中，但是map做不到 list.stream() .flatMap(artist -\u003e Stream.of(art.getName(), art.getNationality())) .toList(); list.stream() .map(artist -\u003e artist.getName()) .toList(); 这样返回的集合就是说我们想要的艺术家的年龄的集合。 flatMap 和 mapmap可用一个新的值代替Stream中的值。而flatMap是生成新的Stream对象取代之前，或者是之前都n个流对象，flatMap将他们整合后变成一个。 List together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(numbers -\u003e numbers.stream()) .toList(); // Idea推荐写法 List together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(Collection::stream) .toList(); max \u0026 min求最值 // 书中给出的写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(artist -\u003e artist.getAge())) .get(); // Idea推荐写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(Artist::getAge)) .get(); Artist maxArtistAge = list.stream() .max(Comparator.comparing(Artist::getAge)) .get(); Optional max = list.stream() .max(Comparator.comparing(Artist::getAge)); 排序的时候需要用到 Comparator 比较器对象。Java8中提供了新的静态方法 comparing()，使用它可以方便地实现一个比较器。之前需要比较两个对象的某个属性值。现在一个存取方法就够了。 需要深入研究comparing方法，该方法接受一个函数并返回另一个函数，该方法本该早已加入Java标准库，但是由于匿名内部类可读性差且书写冗长，一直未实现。现在在Lambda表达式，可以简单的完成。 调用空Stream的max/min方法，返回的是Optional对象，它代表一个可能存在也可能不存在的值。它里面的实际的值就是我们要得到的对象是在 .max().get() / .min()/get() 得到的。 通用模式max和min方法都属于更通用的一种编程模式 使用for循环重写上面的代码： List list = List.of(new Artist(11, \"China\", \"Sam\"), new Artist(9, \"UK\", \"Die\"), new Artist(22, \"US\", \"Daming\")); Artist artist = list.get(0); for(Artist obj : list){ if(obj.getAge() \u003c artist.getAge()){ artist = obj; } } reduce 聚合方法加减乘除都行。把Stream的所有的元素按照聚合函数聚合成一个结果 传统reduce模式： List numbers = List.of(1, 2, 3, 4); Integer accumulator = 0; for(Integer element : numbers){ accumulator = element + accumulator; } Stream中reduce模式： Integer count = Stream.of(1, 2, 3, 4) .reduce(0, (acc, element) -\u003e acc + element); // idea推荐写法，0表示和列表中进行计算的数字，Integer表示类型，sum表示计算方式 Integer count = Stream.of(1, 2, 3, 4) .reduce(0, Integer::sum); 展开reduce操作： ```java // 在这定义两个元素是相加的 BinaryOperator accumulator = (acc, element) -\u003e acc + element; // 或者 BinaryOperator accumulator1 = Integer::sum; // 这里给accumulator添加元素，apply里就是两个元素 // 这两个元素会按照accumulator中的运算法则进行运算 int count = accumulator1.apply( accumulator1.apply( accumulator1.apply( accumulator1.apply(0, 1), 2), 3), 4); Reduce参数： Performs a reduction on the elements of this stream, using the provided identity value and an associative accumulation function, and returns the reduced value. This is equivalent to: T result = identity; for (T element : this stream) result = accumulator.apply(result, element) return result; ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:5","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#collecttolist"},{"categories":["java"],"content":"\r常用的流操作\rcollect(toList())collect(toList()) 方法由Stream里的值生成要给列表，是一个及早求值操作。 Stream的of方法使用一组初始值生成新的Stream。 // 创建List List list = Stream.of(\"a\", \"b\", \"c\", \"d\").collect(Collectors.toList()); // 方式2 该方法应该是上面的进化版，更简洁 List list = Stream.of(\"a\", \"b\", \"c\", \"d\").toList(); 首先由列表生成一个Stream，然后进行一个Stream上的操作，继而是collect操作，由Stream声称列表。 map如果有一个函数可以将一种类型的值转换成另外一种类型，map操作就可以使用该函数，讲一个流中的值转成一个新的流。 比如，将一组字符串转换成大写形式，传统方式是在循环中对每个字符串调用 toUppercase方法，然后将得到的结果加入到一个新的列表。 传统小写变大写的方法 ArrayList collected = new ArrayList\u003c\u003e(); for (String str : asList(\"a\",\"b\",\"hello\")) { String s = str.toUpperCase(); collected.add(s); } ··· Stream小写变大写 ```java List collect = Stream.of(\"a\", \"b\", \"hello\") .map(str -\u003e str.toUpperCase()) .collect(Collectors.toList()); // 简洁版 List collect = Stream.of(\"a\", \"b\", \"hello\") .map(String::toUpperCase).toList(); 传给map的Lambda表达式只接受一个String类型的参数，返回一个新的String。参数和返回值不必属于同一种类型，但是Lambda表达式必须是Function接口的一个实例，Function接口是只包含一个参数的普通函数接口。 filter遍历数据并检查其中的元素时，可以尝试使用filter。主要作用就是筛选出有用的信息。 例：找出一组字符串中以数字开头的字符串， 传统方法： ArrayList list = new ArrayList\u003c\u003e(); for (String str : asList(\"a\", \"1a\", \"a1\", \"2b\")) { if (Character.isDigit(str.charAt(0))) { list.add(str); } } Stream流方式： List list = Stream.of(\"a\", \"1a\", \"a1\", \"2b\") .filter(s -\u003e Character.isDigit(s.charAt(0))) .toList(); 和map很像，filter接受要给函数作为参数，该函数用Lambda表达式表示。如果for循环中的if条件芋圆可以被filter代替。他们的返回值都是true或false，来过滤Stream中符合条件的。 flatMapflatMap可以将一个类中不同属性但是同类型的元素装载到一个Stream中，但是map做不到 list.stream() .flatMap(artist -\u003e Stream.of(art.getName(), art.getNationality())) .toList(); list.stream() .map(artist -\u003e artist.getName()) .toList(); 这样返回的集合就是说我们想要的艺术家的年龄的集合。 flatMap 和 mapmap可用一个新的值代替Stream中的值。而flatMap是生成新的Stream对象取代之前，或者是之前都n个流对象，flatMap将他们整合后变成一个。 List together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(numbers -\u003e numbers.stream()) .toList(); // Idea推荐写法 List together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(Collection::stream) .toList(); max \u0026 min求最值 // 书中给出的写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(artist -\u003e artist.getAge())) .get(); // Idea推荐写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(Artist::getAge)) .get(); Artist maxArtistAge = list.stream() .max(Comparator.comparing(Artist::getAge)) .get(); Optional max = list.stream() .max(Comparator.comparing(Artist::getAge)); 排序的时候需要用到 Comparator 比较器对象。Java8中提供了新的静态方法 comparing()，使用它可以方便地实现一个比较器。之前需要比较两个对象的某个属性值。现在一个存取方法就够了。 需要深入研究comparing方法，该方法接受一个函数并返回另一个函数，该方法本该早已加入Java标准库，但是由于匿名内部类可读性差且书写冗长，一直未实现。现在在Lambda表达式，可以简单的完成。 调用空Stream的max/min方法，返回的是Optional对象，它代表一个可能存在也可能不存在的值。它里面的实际的值就是我们要得到的对象是在 .max().get() / .min()/get() 得到的。 通用模式max和min方法都属于更通用的一种编程模式 使用for循环重写上面的代码： List list = List.of(new Artist(11, \"China\", \"Sam\"), new Artist(9, \"UK\", \"Die\"), new Artist(22, \"US\", \"Daming\")); Artist artist = list.get(0); for(Artist obj : list){ if(obj.getAge() \u003c artist.getAge()){ artist = obj; } } reduce 聚合方法加减乘除都行。把Stream的所有的元素按照聚合函数聚合成一个结果 传统reduce模式： List numbers = List.of(1, 2, 3, 4); Integer accumulator = 0; for(Integer element : numbers){ accumulator = element + accumulator; } Stream中reduce模式： Integer count = Stream.of(1, 2, 3, 4) .reduce(0, (acc, element) -\u003e acc + element); // idea推荐写法，0表示和列表中进行计算的数字，Integer表示类型，sum表示计算方式 Integer count = Stream.of(1, 2, 3, 4) .reduce(0, Integer::sum); 展开reduce操作： ```java // 在这定义两个元素是相加的 BinaryOperator accumulator = (acc, element) -\u003e acc + element; // 或者 BinaryOperator accumulator1 = Integer::sum; // 这里给accumulator添加元素，apply里就是两个元素 // 这两个元素会按照accumulator中的运算法则进行运算 int count = accumulator1.apply( accumulator1.apply( accumulator1.apply( accumulator1.apply(0, 1), 2), 3), 4); Reduce参数： Performs a reduction on the elements of this stream, using the provided identity value and an associative accumulation function, and returns the reduced value. This is equivalent to: T result = identity; for (T element : this stream) result = accumulator.apply(result, element) return result; ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:5","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#map"},{"categories":["java"],"content":"\r常用的流操作\rcollect(toList())collect(toList()) 方法由Stream里的值生成要给列表，是一个及早求值操作。 Stream的of方法使用一组初始值生成新的Stream。 // 创建List List list = Stream.of(\"a\", \"b\", \"c\", \"d\").collect(Collectors.toList()); // 方式2 该方法应该是上面的进化版，更简洁 List list = Stream.of(\"a\", \"b\", \"c\", \"d\").toList(); 首先由列表生成一个Stream，然后进行一个Stream上的操作，继而是collect操作，由Stream声称列表。 map如果有一个函数可以将一种类型的值转换成另外一种类型，map操作就可以使用该函数，讲一个流中的值转成一个新的流。 比如，将一组字符串转换成大写形式，传统方式是在循环中对每个字符串调用 toUppercase方法，然后将得到的结果加入到一个新的列表。 传统小写变大写的方法 ArrayList collected = new ArrayList\u003c\u003e(); for (String str : asList(\"a\",\"b\",\"hello\")) { String s = str.toUpperCase(); collected.add(s); } ··· Stream小写变大写 ```java List collect = Stream.of(\"a\", \"b\", \"hello\") .map(str -\u003e str.toUpperCase()) .collect(Collectors.toList()); // 简洁版 List collect = Stream.of(\"a\", \"b\", \"hello\") .map(String::toUpperCase).toList(); 传给map的Lambda表达式只接受一个String类型的参数，返回一个新的String。参数和返回值不必属于同一种类型，但是Lambda表达式必须是Function接口的一个实例，Function接口是只包含一个参数的普通函数接口。 filter遍历数据并检查其中的元素时，可以尝试使用filter。主要作用就是筛选出有用的信息。 例：找出一组字符串中以数字开头的字符串， 传统方法： ArrayList list = new ArrayList\u003c\u003e(); for (String str : asList(\"a\", \"1a\", \"a1\", \"2b\")) { if (Character.isDigit(str.charAt(0))) { list.add(str); } } Stream流方式： List list = Stream.of(\"a\", \"1a\", \"a1\", \"2b\") .filter(s -\u003e Character.isDigit(s.charAt(0))) .toList(); 和map很像，filter接受要给函数作为参数，该函数用Lambda表达式表示。如果for循环中的if条件芋圆可以被filter代替。他们的返回值都是true或false，来过滤Stream中符合条件的。 flatMapflatMap可以将一个类中不同属性但是同类型的元素装载到一个Stream中，但是map做不到 list.stream() .flatMap(artist -\u003e Stream.of(art.getName(), art.getNationality())) .toList(); list.stream() .map(artist -\u003e artist.getName()) .toList(); 这样返回的集合就是说我们想要的艺术家的年龄的集合。 flatMap 和 mapmap可用一个新的值代替Stream中的值。而flatMap是生成新的Stream对象取代之前，或者是之前都n个流对象，flatMap将他们整合后变成一个。 List together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(numbers -\u003e numbers.stream()) .toList(); // Idea推荐写法 List together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(Collection::stream) .toList(); max \u0026 min求最值 // 书中给出的写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(artist -\u003e artist.getAge())) .get(); // Idea推荐写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(Artist::getAge)) .get(); Artist maxArtistAge = list.stream() .max(Comparator.comparing(Artist::getAge)) .get(); Optional max = list.stream() .max(Comparator.comparing(Artist::getAge)); 排序的时候需要用到 Comparator 比较器对象。Java8中提供了新的静态方法 comparing()，使用它可以方便地实现一个比较器。之前需要比较两个对象的某个属性值。现在一个存取方法就够了。 需要深入研究comparing方法，该方法接受一个函数并返回另一个函数，该方法本该早已加入Java标准库，但是由于匿名内部类可读性差且书写冗长，一直未实现。现在在Lambda表达式，可以简单的完成。 调用空Stream的max/min方法，返回的是Optional对象，它代表一个可能存在也可能不存在的值。它里面的实际的值就是我们要得到的对象是在 .max().get() / .min()/get() 得到的。 通用模式max和min方法都属于更通用的一种编程模式 使用for循环重写上面的代码： List list = List.of(new Artist(11, \"China\", \"Sam\"), new Artist(9, \"UK\", \"Die\"), new Artist(22, \"US\", \"Daming\")); Artist artist = list.get(0); for(Artist obj : list){ if(obj.getAge() \u003c artist.getAge()){ artist = obj; } } reduce 聚合方法加减乘除都行。把Stream的所有的元素按照聚合函数聚合成一个结果 传统reduce模式： List numbers = List.of(1, 2, 3, 4); Integer accumulator = 0; for(Integer element : numbers){ accumulator = element + accumulator; } Stream中reduce模式： Integer count = Stream.of(1, 2, 3, 4) .reduce(0, (acc, element) -\u003e acc + element); // idea推荐写法，0表示和列表中进行计算的数字，Integer表示类型，sum表示计算方式 Integer count = Stream.of(1, 2, 3, 4) .reduce(0, Integer::sum); 展开reduce操作： ```java // 在这定义两个元素是相加的 BinaryOperator accumulator = (acc, element) -\u003e acc + element; // 或者 BinaryOperator accumulator1 = Integer::sum; // 这里给accumulator添加元素，apply里就是两个元素 // 这两个元素会按照accumulator中的运算法则进行运算 int count = accumulator1.apply( accumulator1.apply( accumulator1.apply( accumulator1.apply(0, 1), 2), 3), 4); Reduce参数： Performs a reduction on the elements of this stream, using the provided identity value and an associative accumulation function, and returns the reduced value. This is equivalent to: T result = identity; for (T element : this stream) result = accumulator.apply(result, element) return result; ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:5","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#filter"},{"categories":["java"],"content":"\r常用的流操作\rcollect(toList())collect(toList()) 方法由Stream里的值生成要给列表，是一个及早求值操作。 Stream的of方法使用一组初始值生成新的Stream。 // 创建List List list = Stream.of(\"a\", \"b\", \"c\", \"d\").collect(Collectors.toList()); // 方式2 该方法应该是上面的进化版，更简洁 List list = Stream.of(\"a\", \"b\", \"c\", \"d\").toList(); 首先由列表生成一个Stream，然后进行一个Stream上的操作，继而是collect操作，由Stream声称列表。 map如果有一个函数可以将一种类型的值转换成另外一种类型，map操作就可以使用该函数，讲一个流中的值转成一个新的流。 比如，将一组字符串转换成大写形式，传统方式是在循环中对每个字符串调用 toUppercase方法，然后将得到的结果加入到一个新的列表。 传统小写变大写的方法 ArrayList collected = new ArrayList\u003c\u003e(); for (String str : asList(\"a\",\"b\",\"hello\")) { String s = str.toUpperCase(); collected.add(s); } ··· Stream小写变大写 ```java List collect = Stream.of(\"a\", \"b\", \"hello\") .map(str -\u003e str.toUpperCase()) .collect(Collectors.toList()); // 简洁版 List collect = Stream.of(\"a\", \"b\", \"hello\") .map(String::toUpperCase).toList(); 传给map的Lambda表达式只接受一个String类型的参数，返回一个新的String。参数和返回值不必属于同一种类型，但是Lambda表达式必须是Function接口的一个实例，Function接口是只包含一个参数的普通函数接口。 filter遍历数据并检查其中的元素时，可以尝试使用filter。主要作用就是筛选出有用的信息。 例：找出一组字符串中以数字开头的字符串， 传统方法： ArrayList list = new ArrayList\u003c\u003e(); for (String str : asList(\"a\", \"1a\", \"a1\", \"2b\")) { if (Character.isDigit(str.charAt(0))) { list.add(str); } } Stream流方式： List list = Stream.of(\"a\", \"1a\", \"a1\", \"2b\") .filter(s -\u003e Character.isDigit(s.charAt(0))) .toList(); 和map很像，filter接受要给函数作为参数，该函数用Lambda表达式表示。如果for循环中的if条件芋圆可以被filter代替。他们的返回值都是true或false，来过滤Stream中符合条件的。 flatMapflatMap可以将一个类中不同属性但是同类型的元素装载到一个Stream中，但是map做不到 list.stream() .flatMap(artist -\u003e Stream.of(art.getName(), art.getNationality())) .toList(); list.stream() .map(artist -\u003e artist.getName()) .toList(); 这样返回的集合就是说我们想要的艺术家的年龄的集合。 flatMap 和 mapmap可用一个新的值代替Stream中的值。而flatMap是生成新的Stream对象取代之前，或者是之前都n个流对象，flatMap将他们整合后变成一个。 List together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(numbers -\u003e numbers.stream()) .toList(); // Idea推荐写法 List together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(Collection::stream) .toList(); max \u0026 min求最值 // 书中给出的写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(artist -\u003e artist.getAge())) .get(); // Idea推荐写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(Artist::getAge)) .get(); Artist maxArtistAge = list.stream() .max(Comparator.comparing(Artist::getAge)) .get(); Optional max = list.stream() .max(Comparator.comparing(Artist::getAge)); 排序的时候需要用到 Comparator 比较器对象。Java8中提供了新的静态方法 comparing()，使用它可以方便地实现一个比较器。之前需要比较两个对象的某个属性值。现在一个存取方法就够了。 需要深入研究comparing方法，该方法接受一个函数并返回另一个函数，该方法本该早已加入Java标准库，但是由于匿名内部类可读性差且书写冗长，一直未实现。现在在Lambda表达式，可以简单的完成。 调用空Stream的max/min方法，返回的是Optional对象，它代表一个可能存在也可能不存在的值。它里面的实际的值就是我们要得到的对象是在 .max().get() / .min()/get() 得到的。 通用模式max和min方法都属于更通用的一种编程模式 使用for循环重写上面的代码： List list = List.of(new Artist(11, \"China\", \"Sam\"), new Artist(9, \"UK\", \"Die\"), new Artist(22, \"US\", \"Daming\")); Artist artist = list.get(0); for(Artist obj : list){ if(obj.getAge() \u003c artist.getAge()){ artist = obj; } } reduce 聚合方法加减乘除都行。把Stream的所有的元素按照聚合函数聚合成一个结果 传统reduce模式： List numbers = List.of(1, 2, 3, 4); Integer accumulator = 0; for(Integer element : numbers){ accumulator = element + accumulator; } Stream中reduce模式： Integer count = Stream.of(1, 2, 3, 4) .reduce(0, (acc, element) -\u003e acc + element); // idea推荐写法，0表示和列表中进行计算的数字，Integer表示类型，sum表示计算方式 Integer count = Stream.of(1, 2, 3, 4) .reduce(0, Integer::sum); 展开reduce操作： ```java // 在这定义两个元素是相加的 BinaryOperator accumulator = (acc, element) -\u003e acc + element; // 或者 BinaryOperator accumulator1 = Integer::sum; // 这里给accumulator添加元素，apply里就是两个元素 // 这两个元素会按照accumulator中的运算法则进行运算 int count = accumulator1.apply( accumulator1.apply( accumulator1.apply( accumulator1.apply(0, 1), 2), 3), 4); Reduce参数： Performs a reduction on the elements of this stream, using the provided identity value and an associative accumulation function, and returns the reduced value. This is equivalent to: T result = identity; for (T element : this stream) result = accumulator.apply(result, element) return result; ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:5","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#flatmap"},{"categories":["java"],"content":"\r常用的流操作\rcollect(toList())collect(toList()) 方法由Stream里的值生成要给列表，是一个及早求值操作。 Stream的of方法使用一组初始值生成新的Stream。 // 创建List List list = Stream.of(\"a\", \"b\", \"c\", \"d\").collect(Collectors.toList()); // 方式2 该方法应该是上面的进化版，更简洁 List list = Stream.of(\"a\", \"b\", \"c\", \"d\").toList(); 首先由列表生成一个Stream，然后进行一个Stream上的操作，继而是collect操作，由Stream声称列表。 map如果有一个函数可以将一种类型的值转换成另外一种类型，map操作就可以使用该函数，讲一个流中的值转成一个新的流。 比如，将一组字符串转换成大写形式，传统方式是在循环中对每个字符串调用 toUppercase方法，然后将得到的结果加入到一个新的列表。 传统小写变大写的方法 ArrayList collected = new ArrayList\u003c\u003e(); for (String str : asList(\"a\",\"b\",\"hello\")) { String s = str.toUpperCase(); collected.add(s); } ··· Stream小写变大写 ```java List collect = Stream.of(\"a\", \"b\", \"hello\") .map(str -\u003e str.toUpperCase()) .collect(Collectors.toList()); // 简洁版 List collect = Stream.of(\"a\", \"b\", \"hello\") .map(String::toUpperCase).toList(); 传给map的Lambda表达式只接受一个String类型的参数，返回一个新的String。参数和返回值不必属于同一种类型，但是Lambda表达式必须是Function接口的一个实例，Function接口是只包含一个参数的普通函数接口。 filter遍历数据并检查其中的元素时，可以尝试使用filter。主要作用就是筛选出有用的信息。 例：找出一组字符串中以数字开头的字符串， 传统方法： ArrayList list = new ArrayList\u003c\u003e(); for (String str : asList(\"a\", \"1a\", \"a1\", \"2b\")) { if (Character.isDigit(str.charAt(0))) { list.add(str); } } Stream流方式： List list = Stream.of(\"a\", \"1a\", \"a1\", \"2b\") .filter(s -\u003e Character.isDigit(s.charAt(0))) .toList(); 和map很像，filter接受要给函数作为参数，该函数用Lambda表达式表示。如果for循环中的if条件芋圆可以被filter代替。他们的返回值都是true或false，来过滤Stream中符合条件的。 flatMapflatMap可以将一个类中不同属性但是同类型的元素装载到一个Stream中，但是map做不到 list.stream() .flatMap(artist -\u003e Stream.of(art.getName(), art.getNationality())) .toList(); list.stream() .map(artist -\u003e artist.getName()) .toList(); 这样返回的集合就是说我们想要的艺术家的年龄的集合。 flatMap 和 mapmap可用一个新的值代替Stream中的值。而flatMap是生成新的Stream对象取代之前，或者是之前都n个流对象，flatMap将他们整合后变成一个。 List together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(numbers -\u003e numbers.stream()) .toList(); // Idea推荐写法 List together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(Collection::stream) .toList(); max \u0026 min求最值 // 书中给出的写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(artist -\u003e artist.getAge())) .get(); // Idea推荐写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(Artist::getAge)) .get(); Artist maxArtistAge = list.stream() .max(Comparator.comparing(Artist::getAge)) .get(); Optional max = list.stream() .max(Comparator.comparing(Artist::getAge)); 排序的时候需要用到 Comparator 比较器对象。Java8中提供了新的静态方法 comparing()，使用它可以方便地实现一个比较器。之前需要比较两个对象的某个属性值。现在一个存取方法就够了。 需要深入研究comparing方法，该方法接受一个函数并返回另一个函数，该方法本该早已加入Java标准库，但是由于匿名内部类可读性差且书写冗长，一直未实现。现在在Lambda表达式，可以简单的完成。 调用空Stream的max/min方法，返回的是Optional对象，它代表一个可能存在也可能不存在的值。它里面的实际的值就是我们要得到的对象是在 .max().get() / .min()/get() 得到的。 通用模式max和min方法都属于更通用的一种编程模式 使用for循环重写上面的代码： List list = List.of(new Artist(11, \"China\", \"Sam\"), new Artist(9, \"UK\", \"Die\"), new Artist(22, \"US\", \"Daming\")); Artist artist = list.get(0); for(Artist obj : list){ if(obj.getAge() \u003c artist.getAge()){ artist = obj; } } reduce 聚合方法加减乘除都行。把Stream的所有的元素按照聚合函数聚合成一个结果 传统reduce模式： List numbers = List.of(1, 2, 3, 4); Integer accumulator = 0; for(Integer element : numbers){ accumulator = element + accumulator; } Stream中reduce模式： Integer count = Stream.of(1, 2, 3, 4) .reduce(0, (acc, element) -\u003e acc + element); // idea推荐写法，0表示和列表中进行计算的数字，Integer表示类型，sum表示计算方式 Integer count = Stream.of(1, 2, 3, 4) .reduce(0, Integer::sum); 展开reduce操作： ```java // 在这定义两个元素是相加的 BinaryOperator accumulator = (acc, element) -\u003e acc + element; // 或者 BinaryOperator accumulator1 = Integer::sum; // 这里给accumulator添加元素，apply里就是两个元素 // 这两个元素会按照accumulator中的运算法则进行运算 int count = accumulator1.apply( accumulator1.apply( accumulator1.apply( accumulator1.apply(0, 1), 2), 3), 4); Reduce参数： Performs a reduction on the elements of this stream, using the provided identity value and an associative accumulation function, and returns the reduced value. This is equivalent to: T result = identity; for (T element : this stream) result = accumulator.apply(result, element) return result; ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:5","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#flatmap-和-map"},{"categories":["java"],"content":"\r常用的流操作\rcollect(toList())collect(toList()) 方法由Stream里的值生成要给列表，是一个及早求值操作。 Stream的of方法使用一组初始值生成新的Stream。 // 创建List List list = Stream.of(\"a\", \"b\", \"c\", \"d\").collect(Collectors.toList()); // 方式2 该方法应该是上面的进化版，更简洁 List list = Stream.of(\"a\", \"b\", \"c\", \"d\").toList(); 首先由列表生成一个Stream，然后进行一个Stream上的操作，继而是collect操作，由Stream声称列表。 map如果有一个函数可以将一种类型的值转换成另外一种类型，map操作就可以使用该函数，讲一个流中的值转成一个新的流。 比如，将一组字符串转换成大写形式，传统方式是在循环中对每个字符串调用 toUppercase方法，然后将得到的结果加入到一个新的列表。 传统小写变大写的方法 ArrayList collected = new ArrayList\u003c\u003e(); for (String str : asList(\"a\",\"b\",\"hello\")) { String s = str.toUpperCase(); collected.add(s); } ··· Stream小写变大写 ```java List collect = Stream.of(\"a\", \"b\", \"hello\") .map(str -\u003e str.toUpperCase()) .collect(Collectors.toList()); // 简洁版 List collect = Stream.of(\"a\", \"b\", \"hello\") .map(String::toUpperCase).toList(); 传给map的Lambda表达式只接受一个String类型的参数，返回一个新的String。参数和返回值不必属于同一种类型，但是Lambda表达式必须是Function接口的一个实例，Function接口是只包含一个参数的普通函数接口。 filter遍历数据并检查其中的元素时，可以尝试使用filter。主要作用就是筛选出有用的信息。 例：找出一组字符串中以数字开头的字符串， 传统方法： ArrayList list = new ArrayList\u003c\u003e(); for (String str : asList(\"a\", \"1a\", \"a1\", \"2b\")) { if (Character.isDigit(str.charAt(0))) { list.add(str); } } Stream流方式： List list = Stream.of(\"a\", \"1a\", \"a1\", \"2b\") .filter(s -\u003e Character.isDigit(s.charAt(0))) .toList(); 和map很像，filter接受要给函数作为参数，该函数用Lambda表达式表示。如果for循环中的if条件芋圆可以被filter代替。他们的返回值都是true或false，来过滤Stream中符合条件的。 flatMapflatMap可以将一个类中不同属性但是同类型的元素装载到一个Stream中，但是map做不到 list.stream() .flatMap(artist -\u003e Stream.of(art.getName(), art.getNationality())) .toList(); list.stream() .map(artist -\u003e artist.getName()) .toList(); 这样返回的集合就是说我们想要的艺术家的年龄的集合。 flatMap 和 mapmap可用一个新的值代替Stream中的值。而flatMap是生成新的Stream对象取代之前，或者是之前都n个流对象，flatMap将他们整合后变成一个。 List together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(numbers -\u003e numbers.stream()) .toList(); // Idea推荐写法 List together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(Collection::stream) .toList(); max \u0026 min求最值 // 书中给出的写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(artist -\u003e artist.getAge())) .get(); // Idea推荐写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(Artist::getAge)) .get(); Artist maxArtistAge = list.stream() .max(Comparator.comparing(Artist::getAge)) .get(); Optional max = list.stream() .max(Comparator.comparing(Artist::getAge)); 排序的时候需要用到 Comparator 比较器对象。Java8中提供了新的静态方法 comparing()，使用它可以方便地实现一个比较器。之前需要比较两个对象的某个属性值。现在一个存取方法就够了。 需要深入研究comparing方法，该方法接受一个函数并返回另一个函数，该方法本该早已加入Java标准库，但是由于匿名内部类可读性差且书写冗长，一直未实现。现在在Lambda表达式，可以简单的完成。 调用空Stream的max/min方法，返回的是Optional对象，它代表一个可能存在也可能不存在的值。它里面的实际的值就是我们要得到的对象是在 .max().get() / .min()/get() 得到的。 通用模式max和min方法都属于更通用的一种编程模式 使用for循环重写上面的代码： List list = List.of(new Artist(11, \"China\", \"Sam\"), new Artist(9, \"UK\", \"Die\"), new Artist(22, \"US\", \"Daming\")); Artist artist = list.get(0); for(Artist obj : list){ if(obj.getAge() \u003c artist.getAge()){ artist = obj; } } reduce 聚合方法加减乘除都行。把Stream的所有的元素按照聚合函数聚合成一个结果 传统reduce模式： List numbers = List.of(1, 2, 3, 4); Integer accumulator = 0; for(Integer element : numbers){ accumulator = element + accumulator; } Stream中reduce模式： Integer count = Stream.of(1, 2, 3, 4) .reduce(0, (acc, element) -\u003e acc + element); // idea推荐写法，0表示和列表中进行计算的数字，Integer表示类型，sum表示计算方式 Integer count = Stream.of(1, 2, 3, 4) .reduce(0, Integer::sum); 展开reduce操作： ```java // 在这定义两个元素是相加的 BinaryOperator accumulator = (acc, element) -\u003e acc + element; // 或者 BinaryOperator accumulator1 = Integer::sum; // 这里给accumulator添加元素，apply里就是两个元素 // 这两个元素会按照accumulator中的运算法则进行运算 int count = accumulator1.apply( accumulator1.apply( accumulator1.apply( accumulator1.apply(0, 1), 2), 3), 4); Reduce参数： Performs a reduction on the elements of this stream, using the provided identity value and an associative accumulation function, and returns the reduced value. This is equivalent to: T result = identity; for (T element : this stream) result = accumulator.apply(result, element) return result; ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:5","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#max--min"},{"categories":["java"],"content":"\r常用的流操作\rcollect(toList())collect(toList()) 方法由Stream里的值生成要给列表，是一个及早求值操作。 Stream的of方法使用一组初始值生成新的Stream。 // 创建List List list = Stream.of(\"a\", \"b\", \"c\", \"d\").collect(Collectors.toList()); // 方式2 该方法应该是上面的进化版，更简洁 List list = Stream.of(\"a\", \"b\", \"c\", \"d\").toList(); 首先由列表生成一个Stream，然后进行一个Stream上的操作，继而是collect操作，由Stream声称列表。 map如果有一个函数可以将一种类型的值转换成另外一种类型，map操作就可以使用该函数，讲一个流中的值转成一个新的流。 比如，将一组字符串转换成大写形式，传统方式是在循环中对每个字符串调用 toUppercase方法，然后将得到的结果加入到一个新的列表。 传统小写变大写的方法 ArrayList collected = new ArrayList\u003c\u003e(); for (String str : asList(\"a\",\"b\",\"hello\")) { String s = str.toUpperCase(); collected.add(s); } ··· Stream小写变大写 ```java List collect = Stream.of(\"a\", \"b\", \"hello\") .map(str -\u003e str.toUpperCase()) .collect(Collectors.toList()); // 简洁版 List collect = Stream.of(\"a\", \"b\", \"hello\") .map(String::toUpperCase).toList(); 传给map的Lambda表达式只接受一个String类型的参数，返回一个新的String。参数和返回值不必属于同一种类型，但是Lambda表达式必须是Function接口的一个实例，Function接口是只包含一个参数的普通函数接口。 filter遍历数据并检查其中的元素时，可以尝试使用filter。主要作用就是筛选出有用的信息。 例：找出一组字符串中以数字开头的字符串， 传统方法： ArrayList list = new ArrayList\u003c\u003e(); for (String str : asList(\"a\", \"1a\", \"a1\", \"2b\")) { if (Character.isDigit(str.charAt(0))) { list.add(str); } } Stream流方式： List list = Stream.of(\"a\", \"1a\", \"a1\", \"2b\") .filter(s -\u003e Character.isDigit(s.charAt(0))) .toList(); 和map很像，filter接受要给函数作为参数，该函数用Lambda表达式表示。如果for循环中的if条件芋圆可以被filter代替。他们的返回值都是true或false，来过滤Stream中符合条件的。 flatMapflatMap可以将一个类中不同属性但是同类型的元素装载到一个Stream中，但是map做不到 list.stream() .flatMap(artist -\u003e Stream.of(art.getName(), art.getNationality())) .toList(); list.stream() .map(artist -\u003e artist.getName()) .toList(); 这样返回的集合就是说我们想要的艺术家的年龄的集合。 flatMap 和 mapmap可用一个新的值代替Stream中的值。而flatMap是生成新的Stream对象取代之前，或者是之前都n个流对象，flatMap将他们整合后变成一个。 List together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(numbers -\u003e numbers.stream()) .toList(); // Idea推荐写法 List together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(Collection::stream) .toList(); max \u0026 min求最值 // 书中给出的写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(artist -\u003e artist.getAge())) .get(); // Idea推荐写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(Artist::getAge)) .get(); Artist maxArtistAge = list.stream() .max(Comparator.comparing(Artist::getAge)) .get(); Optional max = list.stream() .max(Comparator.comparing(Artist::getAge)); 排序的时候需要用到 Comparator 比较器对象。Java8中提供了新的静态方法 comparing()，使用它可以方便地实现一个比较器。之前需要比较两个对象的某个属性值。现在一个存取方法就够了。 需要深入研究comparing方法，该方法接受一个函数并返回另一个函数，该方法本该早已加入Java标准库，但是由于匿名内部类可读性差且书写冗长，一直未实现。现在在Lambda表达式，可以简单的完成。 调用空Stream的max/min方法，返回的是Optional对象，它代表一个可能存在也可能不存在的值。它里面的实际的值就是我们要得到的对象是在 .max().get() / .min()/get() 得到的。 通用模式max和min方法都属于更通用的一种编程模式 使用for循环重写上面的代码： List list = List.of(new Artist(11, \"China\", \"Sam\"), new Artist(9, \"UK\", \"Die\"), new Artist(22, \"US\", \"Daming\")); Artist artist = list.get(0); for(Artist obj : list){ if(obj.getAge() \u003c artist.getAge()){ artist = obj; } } reduce 聚合方法加减乘除都行。把Stream的所有的元素按照聚合函数聚合成一个结果 传统reduce模式： List numbers = List.of(1, 2, 3, 4); Integer accumulator = 0; for(Integer element : numbers){ accumulator = element + accumulator; } Stream中reduce模式： Integer count = Stream.of(1, 2, 3, 4) .reduce(0, (acc, element) -\u003e acc + element); // idea推荐写法，0表示和列表中进行计算的数字，Integer表示类型，sum表示计算方式 Integer count = Stream.of(1, 2, 3, 4) .reduce(0, Integer::sum); 展开reduce操作： ```java // 在这定义两个元素是相加的 BinaryOperator accumulator = (acc, element) -\u003e acc + element; // 或者 BinaryOperator accumulator1 = Integer::sum; // 这里给accumulator添加元素，apply里就是两个元素 // 这两个元素会按照accumulator中的运算法则进行运算 int count = accumulator1.apply( accumulator1.apply( accumulator1.apply( accumulator1.apply(0, 1), 2), 3), 4); Reduce参数： Performs a reduction on the elements of this stream, using the provided identity value and an associative accumulation function, and returns the reduced value. This is equivalent to: T result = identity; for (T element : this stream) result = accumulator.apply(result, element) return result; ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:5","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#通用模式"},{"categories":["java"],"content":"\r常用的流操作\rcollect(toList())collect(toList()) 方法由Stream里的值生成要给列表，是一个及早求值操作。 Stream的of方法使用一组初始值生成新的Stream。 // 创建List List list = Stream.of(\"a\", \"b\", \"c\", \"d\").collect(Collectors.toList()); // 方式2 该方法应该是上面的进化版，更简洁 List list = Stream.of(\"a\", \"b\", \"c\", \"d\").toList(); 首先由列表生成一个Stream，然后进行一个Stream上的操作，继而是collect操作，由Stream声称列表。 map如果有一个函数可以将一种类型的值转换成另外一种类型，map操作就可以使用该函数，讲一个流中的值转成一个新的流。 比如，将一组字符串转换成大写形式，传统方式是在循环中对每个字符串调用 toUppercase方法，然后将得到的结果加入到一个新的列表。 传统小写变大写的方法 ArrayList collected = new ArrayList\u003c\u003e(); for (String str : asList(\"a\",\"b\",\"hello\")) { String s = str.toUpperCase(); collected.add(s); } ··· Stream小写变大写 ```java List collect = Stream.of(\"a\", \"b\", \"hello\") .map(str -\u003e str.toUpperCase()) .collect(Collectors.toList()); // 简洁版 List collect = Stream.of(\"a\", \"b\", \"hello\") .map(String::toUpperCase).toList(); 传给map的Lambda表达式只接受一个String类型的参数，返回一个新的String。参数和返回值不必属于同一种类型，但是Lambda表达式必须是Function接口的一个实例，Function接口是只包含一个参数的普通函数接口。 filter遍历数据并检查其中的元素时，可以尝试使用filter。主要作用就是筛选出有用的信息。 例：找出一组字符串中以数字开头的字符串， 传统方法： ArrayList list = new ArrayList\u003c\u003e(); for (String str : asList(\"a\", \"1a\", \"a1\", \"2b\")) { if (Character.isDigit(str.charAt(0))) { list.add(str); } } Stream流方式： List list = Stream.of(\"a\", \"1a\", \"a1\", \"2b\") .filter(s -\u003e Character.isDigit(s.charAt(0))) .toList(); 和map很像，filter接受要给函数作为参数，该函数用Lambda表达式表示。如果for循环中的if条件芋圆可以被filter代替。他们的返回值都是true或false，来过滤Stream中符合条件的。 flatMapflatMap可以将一个类中不同属性但是同类型的元素装载到一个Stream中，但是map做不到 list.stream() .flatMap(artist -\u003e Stream.of(art.getName(), art.getNationality())) .toList(); list.stream() .map(artist -\u003e artist.getName()) .toList(); 这样返回的集合就是说我们想要的艺术家的年龄的集合。 flatMap 和 mapmap可用一个新的值代替Stream中的值。而flatMap是生成新的Stream对象取代之前，或者是之前都n个流对象，flatMap将他们整合后变成一个。 List together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(numbers -\u003e numbers.stream()) .toList(); // Idea推荐写法 List together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(Collection::stream) .toList(); max \u0026 min求最值 // 书中给出的写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(artist -\u003e artist.getAge())) .get(); // Idea推荐写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(Artist::getAge)) .get(); Artist maxArtistAge = list.stream() .max(Comparator.comparing(Artist::getAge)) .get(); Optional max = list.stream() .max(Comparator.comparing(Artist::getAge)); 排序的时候需要用到 Comparator 比较器对象。Java8中提供了新的静态方法 comparing()，使用它可以方便地实现一个比较器。之前需要比较两个对象的某个属性值。现在一个存取方法就够了。 需要深入研究comparing方法，该方法接受一个函数并返回另一个函数，该方法本该早已加入Java标准库，但是由于匿名内部类可读性差且书写冗长，一直未实现。现在在Lambda表达式，可以简单的完成。 调用空Stream的max/min方法，返回的是Optional对象，它代表一个可能存在也可能不存在的值。它里面的实际的值就是我们要得到的对象是在 .max().get() / .min()/get() 得到的。 通用模式max和min方法都属于更通用的一种编程模式 使用for循环重写上面的代码： List list = List.of(new Artist(11, \"China\", \"Sam\"), new Artist(9, \"UK\", \"Die\"), new Artist(22, \"US\", \"Daming\")); Artist artist = list.get(0); for(Artist obj : list){ if(obj.getAge() \u003c artist.getAge()){ artist = obj; } } reduce 聚合方法加减乘除都行。把Stream的所有的元素按照聚合函数聚合成一个结果 传统reduce模式： List numbers = List.of(1, 2, 3, 4); Integer accumulator = 0; for(Integer element : numbers){ accumulator = element + accumulator; } Stream中reduce模式： Integer count = Stream.of(1, 2, 3, 4) .reduce(0, (acc, element) -\u003e acc + element); // idea推荐写法，0表示和列表中进行计算的数字，Integer表示类型，sum表示计算方式 Integer count = Stream.of(1, 2, 3, 4) .reduce(0, Integer::sum); 展开reduce操作： ```java // 在这定义两个元素是相加的 BinaryOperator accumulator = (acc, element) -\u003e acc + element; // 或者 BinaryOperator accumulator1 = Integer::sum; // 这里给accumulator添加元素，apply里就是两个元素 // 这两个元素会按照accumulator中的运算法则进行运算 int count = accumulator1.apply( accumulator1.apply( accumulator1.apply( accumulator1.apply(0, 1), 2), 3), 4); Reduce参数： Performs a reduction on the elements of this stream, using the provided identity value and an associative accumulation function, and returns the reduced value. This is equivalent to: T result = identity; for (T element : this stream) result = accumulator.apply(result, element) return result; ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:5","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#reduce-聚合方法"},{"categories":["java"],"content":"\r常用的流操作\rcollect(toList())collect(toList()) 方法由Stream里的值生成要给列表，是一个及早求值操作。 Stream的of方法使用一组初始值生成新的Stream。 // 创建List List list = Stream.of(\"a\", \"b\", \"c\", \"d\").collect(Collectors.toList()); // 方式2 该方法应该是上面的进化版，更简洁 List list = Stream.of(\"a\", \"b\", \"c\", \"d\").toList(); 首先由列表生成一个Stream，然后进行一个Stream上的操作，继而是collect操作，由Stream声称列表。 map如果有一个函数可以将一种类型的值转换成另外一种类型，map操作就可以使用该函数，讲一个流中的值转成一个新的流。 比如，将一组字符串转换成大写形式，传统方式是在循环中对每个字符串调用 toUppercase方法，然后将得到的结果加入到一个新的列表。 传统小写变大写的方法 ArrayList collected = new ArrayList\u003c\u003e(); for (String str : asList(\"a\",\"b\",\"hello\")) { String s = str.toUpperCase(); collected.add(s); } ··· Stream小写变大写 ```java List collect = Stream.of(\"a\", \"b\", \"hello\") .map(str -\u003e str.toUpperCase()) .collect(Collectors.toList()); // 简洁版 List collect = Stream.of(\"a\", \"b\", \"hello\") .map(String::toUpperCase).toList(); 传给map的Lambda表达式只接受一个String类型的参数，返回一个新的String。参数和返回值不必属于同一种类型，但是Lambda表达式必须是Function接口的一个实例，Function接口是只包含一个参数的普通函数接口。 filter遍历数据并检查其中的元素时，可以尝试使用filter。主要作用就是筛选出有用的信息。 例：找出一组字符串中以数字开头的字符串， 传统方法： ArrayList list = new ArrayList\u003c\u003e(); for (String str : asList(\"a\", \"1a\", \"a1\", \"2b\")) { if (Character.isDigit(str.charAt(0))) { list.add(str); } } Stream流方式： List list = Stream.of(\"a\", \"1a\", \"a1\", \"2b\") .filter(s -\u003e Character.isDigit(s.charAt(0))) .toList(); 和map很像，filter接受要给函数作为参数，该函数用Lambda表达式表示。如果for循环中的if条件芋圆可以被filter代替。他们的返回值都是true或false，来过滤Stream中符合条件的。 flatMapflatMap可以将一个类中不同属性但是同类型的元素装载到一个Stream中，但是map做不到 list.stream() .flatMap(artist -\u003e Stream.of(art.getName(), art.getNationality())) .toList(); list.stream() .map(artist -\u003e artist.getName()) .toList(); 这样返回的集合就是说我们想要的艺术家的年龄的集合。 flatMap 和 mapmap可用一个新的值代替Stream中的值。而flatMap是生成新的Stream对象取代之前，或者是之前都n个流对象，flatMap将他们整合后变成一个。 List together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(numbers -\u003e numbers.stream()) .toList(); // Idea推荐写法 List together = Stream.of(asList(1, 2), asList(3, 4)) .flatMap(Collection::stream) .toList(); max \u0026 min求最值 // 书中给出的写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(artist -\u003e artist.getAge())) .get(); // Idea推荐写法 Artist minArtistAge1 = list.stream() .min(Comparator.comparing(Artist::getAge)) .get(); Artist maxArtistAge = list.stream() .max(Comparator.comparing(Artist::getAge)) .get(); Optional max = list.stream() .max(Comparator.comparing(Artist::getAge)); 排序的时候需要用到 Comparator 比较器对象。Java8中提供了新的静态方法 comparing()，使用它可以方便地实现一个比较器。之前需要比较两个对象的某个属性值。现在一个存取方法就够了。 需要深入研究comparing方法，该方法接受一个函数并返回另一个函数，该方法本该早已加入Java标准库，但是由于匿名内部类可读性差且书写冗长，一直未实现。现在在Lambda表达式，可以简单的完成。 调用空Stream的max/min方法，返回的是Optional对象，它代表一个可能存在也可能不存在的值。它里面的实际的值就是我们要得到的对象是在 .max().get() / .min()/get() 得到的。 通用模式max和min方法都属于更通用的一种编程模式 使用for循环重写上面的代码： List list = List.of(new Artist(11, \"China\", \"Sam\"), new Artist(9, \"UK\", \"Die\"), new Artist(22, \"US\", \"Daming\")); Artist artist = list.get(0); for(Artist obj : list){ if(obj.getAge() \u003c artist.getAge()){ artist = obj; } } reduce 聚合方法加减乘除都行。把Stream的所有的元素按照聚合函数聚合成一个结果 传统reduce模式： List numbers = List.of(1, 2, 3, 4); Integer accumulator = 0; for(Integer element : numbers){ accumulator = element + accumulator; } Stream中reduce模式： Integer count = Stream.of(1, 2, 3, 4) .reduce(0, (acc, element) -\u003e acc + element); // idea推荐写法，0表示和列表中进行计算的数字，Integer表示类型，sum表示计算方式 Integer count = Stream.of(1, 2, 3, 4) .reduce(0, Integer::sum); 展开reduce操作： ```java // 在这定义两个元素是相加的 BinaryOperator accumulator = (acc, element) -\u003e acc + element; // 或者 BinaryOperator accumulator1 = Integer::sum; // 这里给accumulator添加元素，apply里就是两个元素 // 这两个元素会按照accumulator中的运算法则进行运算 int count = accumulator1.apply( accumulator1.apply( accumulator1.apply( accumulator1.apply(0, 1), 2), 3), 4); Reduce参数： Performs a reduction on the elements of this stream, using the provided identity value and an associative accumulation function, and returns the reduced value. This is equivalent to: T result = identity; for (T element : this stream) result = accumulator.apply(result, element) return result; ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:5","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#整合操作"},{"categories":["java"],"content":"\r重构遗留代码传统的代码 public Set\u003cString\u003e findLongTracls(LList\u003cAlbum\u003e albums){ Set\u003cString\u003e trackNames = new HashSet\u003c\u003e(); for(Album album : albums){ for(Track track : album.getTrackList()){ if(track.getLength() \u003e 60){ String name = track.getName(); trackNames.add(name); } } } } Stream重构 public Set\u003cString\u003e findLongTracks(List\u003cAlbum\u003e albums){ Set\u003cString\u003e trackNames = new HashSet\u003c\u003e(); albums.forEach(album -\u003e { album.getTracks() .forEach(track -\u003e { if(track.getLength() \u003e 60){ String name = track.getName(); trackNames.add(name); } }); }); return trackNames; } 对比发现重构后的代码的可读性比之前的代码还差。 分析：最内层的foreach的作用： 找出长度大于1的曲目 得到符合条件的曲目名称 将其添加到Set 这意味着需要三项Stream操作，找到满足条件的纠结过是 filter功能，得到曲目名称可以用map，最终使用forEach将曲目添加到filter功能。 重构2： public Set\u003cString\u003e findLongTracks(List\u003cAlbum\u003e albums){ Set\u003cString\u003e trackNames = new HashSet\u003c\u003e(); albums.forEach(album -\u003e { album.getTracks() .filter(track -\u003e track.getLength() \u003e 60) .map(track -\u003e track.getName()) .forEach(name -\u003e tracksNames.add(name)) }); return trackNames; } 代码看着还是冗余，继续改： public Set\u003cString\u003e findLongTracks(List\u003cAlbum\u003e albums){ Set\u003cString\u003e trackNames = new HashSet\u003c\u003e(); albums.stream() .flatMap(album -\u003e album.getTracks())) .filter(track -\u003e track.getLength() \u003e 60) .map(track -\u003e track.getName)) .forEach(name -\u003e trackNames.add(name)); return trackNames; } 上面代码已经替换了两个嵌套的for循环，看起来清晰很多，但是并未及家属，仍需要手动创建一个Set对象并将其加入到其中，下面将通过Stream完成。 public Set\u003cString\u003e findLongTracks(List\u003cAlbum\u003e albums){ return albums.stream() .flatMap(album -\u003e album.getTracks()) .filter(track -\u003e track.getLength() \u003e 60) .map(track -\u003e track.getName) .collect(Collectors.toSet()); } 上述步骤中没有提到一个重点，就是每一步代码都要编写单元测试。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:6","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#重构遗留代码"},{"categories":["java"],"content":"\r多次调用流操作虽然可以选择每一步强制对函数求值，而不是所有的方法调用连接在一起，但是最好不要这么做。(能写在一起就不要分开写几行)。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:7","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#多次调用流操作"},{"categories":["java"],"content":"\r正确使用Lambda表达式明确了要达成什么转化，而不是说明如何转化。其另一层含义在于写出的函数没有副作用。没有副作用的函数不会改变程序或外界的状态。比如，向控制台输出了信息，就是一个可观测到的副作用。 下面代码有无副作用？ private ActionEvent lastEvent; private void registerHandler(){ button.addActionListener((ActionEvent event) -\u003e { this.lastEvent = event; }) } 这里将参数event保存至成员变量lastEvent。给变量赋值也是一种副作用，而且更难察觉。的确是改变了程序的状态。 程序鼓励用户使用Lambda表达式获取值而不是变量。获取值使用户更容易写出没有副作用的代码。 无论何时，将Lambda表达式传给Stream上的高阶函数，都应该尽量避免副作用，除了forEach，因为它是一个终结方法。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:1:8","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#正确使用lambda表达式"},{"categories":["java"],"content":"\r类库","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:2:0","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#类库"},{"categories":["java"],"content":"\r在代码中使用Lambda表达式\r降低日志性能开销\r基本类型基本类型对应着装箱类型，int - Integer。后者是java的普通类，是对基本类的封装。 Java泛型是基于对泛型参数类型的擦除。只有装箱类型才能作为泛型参数，基本类型不可以。 但是装箱类型是对象，这表示在内存中存在额外的开销。比如，整型的在内存中占用4字节，整形对象却占16字节。数组这一情况更严重，整型对象数组中，每个元素都是内存的一个指针，只想堆中某个对象。最坏情况，同样的数组，Integer[] 要比 int[] 多占用6倍内存。 而二者互相转换的过程称为装箱和拆箱，过程中都需要额外的计算开销，会影响程序运行速度。 Stream类中某些方法对基本类型和装箱类型做了区分，进而减小些性能开销。 命名规范，返回类型为基本类型，则在基本类型前加To。参数是基本型，则就直接基本类型即可。比如：ToLongFunction 和 LongFunction。 这些处理基本类型线管的Stream和一半的Stream是有区别的，比如LongStream。而这类特殊的Stream中的map方法的实现方式也不同，它接受的是一个LongUnaryOperator函数，将一个长整型映射成另一个长整型值。 public static void example1(){ List\u003cArtist\u003e artists = List.of(new Artist(11, \"China\", \"Sam\"), new Artist(9, \"UK\", \"Die\"), new Artist(22, \"US\", \"Daming\")); IntSummaryStatistics intSummaryStatistics = artists.stream() .mapToInt(Artist::getAge) .summaryStatistics(); System.out.printf(\"Max: %d, Min: %d. Ave: %f, Sum: %d\", intSummaryStatistics.getMax(), intSummaryStatistics.getMin(), intSummaryStatistics.getAverage(), intSummaryStatistics.getSum()); } 重载解析Lambda表达式作为参数时，其类型由它的目标类型推导得出，遵循如下规则： 如果只有一个可能的目标类型，由相应函数接口里的参数类型推导得出； 如果有多个可能的目标类型，由最具体的类型推导得出； 如果有多个可能的目标类型且最具体的类型不明确，则需要认为指定类型。 @FunctionalInterface实际上，每个用作函数接口的接口都应该添加这个注释。该注释糊强制javac检查一个接口是否符合函数接口的标准。 如果该注释添加给一个枚举类型、类或另一个注释，或者接口包含不止一个抽象方法，javac就会报错。 重构代码时，使用它很容易发现问题。 新的语言特性：默认方法为了保证二进制接口的兼容性而出现的。 新关键字: default，这个关键字告诉javac，用户真正需要的是为接口添加一个新方法，从而进行区分。 三定律： 类胜于接口。如果在继承链中有方法体或抽象的方法声明，那么就可以忽略接口中定义的方法。（让代码向后兼容） 子类胜于父类。如果一个接口继承了另一个接口，且两个接口都定义了一个默认方法，那么子类定义的方法胜出。 如果上面两个规则不适用，子类要么需要实现该方法，要么将该方法声明为抽象方法。 Optionalreduce方法有一个重点，它有两种形式： 需要有一个初始值，上面有例子。 不需要有初始值，reduce第一步使用Stream中的前两个元素，此时返回一个Optinal对象。 Optional用来替换null值，因为null最大的问题在于NullPointerException。 Optional两个目的，鼓励程序员适时检查变量是否为空，避免代码缺陷；二它将是一个类的API中可能为空的值文档化，这比阅读实现代码要简单的多。 使用方式，在调用get之前，先使用 isPresent()检查Optional对象是否有值，或者用 orElse()。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:2:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#在代码中使用lambda表达式"},{"categories":["java"],"content":"\r在代码中使用Lambda表达式\r降低日志性能开销\r基本类型基本类型对应着装箱类型，int - Integer。后者是java的普通类，是对基本类的封装。 Java泛型是基于对泛型参数类型的擦除。只有装箱类型才能作为泛型参数，基本类型不可以。 但是装箱类型是对象，这表示在内存中存在额外的开销。比如，整型的在内存中占用4字节，整形对象却占16字节。数组这一情况更严重，整型对象数组中，每个元素都是内存的一个指针，只想堆中某个对象。最坏情况，同样的数组，Integer[] 要比 int[] 多占用6倍内存。 而二者互相转换的过程称为装箱和拆箱，过程中都需要额外的计算开销，会影响程序运行速度。 Stream类中某些方法对基本类型和装箱类型做了区分，进而减小些性能开销。 命名规范，返回类型为基本类型，则在基本类型前加To。参数是基本型，则就直接基本类型即可。比如：ToLongFunction 和 LongFunction。 这些处理基本类型线管的Stream和一半的Stream是有区别的，比如LongStream。而这类特殊的Stream中的map方法的实现方式也不同，它接受的是一个LongUnaryOperator函数，将一个长整型映射成另一个长整型值。 public static void example1(){ List artists = List.of(new Artist(11, \"China\", \"Sam\"), new Artist(9, \"UK\", \"Die\"), new Artist(22, \"US\", \"Daming\")); IntSummaryStatistics intSummaryStatistics = artists.stream() .mapToInt(Artist::getAge) .summaryStatistics(); System.out.printf(\"Max: %d, Min: %d. Ave: %f, Sum: %d\", intSummaryStatistics.getMax(), intSummaryStatistics.getMin(), intSummaryStatistics.getAverage(), intSummaryStatistics.getSum()); } 重载解析Lambda表达式作为参数时，其类型由它的目标类型推导得出，遵循如下规则： 如果只有一个可能的目标类型，由相应函数接口里的参数类型推导得出； 如果有多个可能的目标类型，由最具体的类型推导得出； 如果有多个可能的目标类型且最具体的类型不明确，则需要认为指定类型。 @FunctionalInterface实际上，每个用作函数接口的接口都应该添加这个注释。该注释糊强制javac检查一个接口是否符合函数接口的标准。 如果该注释添加给一个枚举类型、类或另一个注释，或者接口包含不止一个抽象方法，javac就会报错。 重构代码时，使用它很容易发现问题。 新的语言特性：默认方法为了保证二进制接口的兼容性而出现的。 新关键字: default，这个关键字告诉javac，用户真正需要的是为接口添加一个新方法，从而进行区分。 三定律： 类胜于接口。如果在继承链中有方法体或抽象的方法声明，那么就可以忽略接口中定义的方法。（让代码向后兼容） 子类胜于父类。如果一个接口继承了另一个接口，且两个接口都定义了一个默认方法，那么子类定义的方法胜出。 如果上面两个规则不适用，子类要么需要实现该方法，要么将该方法声明为抽象方法。 Optionalreduce方法有一个重点，它有两种形式： 需要有一个初始值，上面有例子。 不需要有初始值，reduce第一步使用Stream中的前两个元素，此时返回一个Optinal对象。 Optional用来替换null值，因为null最大的问题在于NullPointerException。 Optional两个目的，鼓励程序员适时检查变量是否为空，避免代码缺陷；二它将是一个类的API中可能为空的值文档化，这比阅读实现代码要简单的多。 使用方式，在调用get之前，先使用 isPresent()检查Optional对象是否有值，或者用 orElse()。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:2:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#降低日志性能开销"},{"categories":["java"],"content":"\r在代码中使用Lambda表达式\r降低日志性能开销\r基本类型基本类型对应着装箱类型，int - Integer。后者是java的普通类，是对基本类的封装。 Java泛型是基于对泛型参数类型的擦除。只有装箱类型才能作为泛型参数，基本类型不可以。 但是装箱类型是对象，这表示在内存中存在额外的开销。比如，整型的在内存中占用4字节，整形对象却占16字节。数组这一情况更严重，整型对象数组中，每个元素都是内存的一个指针，只想堆中某个对象。最坏情况，同样的数组，Integer[] 要比 int[] 多占用6倍内存。 而二者互相转换的过程称为装箱和拆箱，过程中都需要额外的计算开销，会影响程序运行速度。 Stream类中某些方法对基本类型和装箱类型做了区分，进而减小些性能开销。 命名规范，返回类型为基本类型，则在基本类型前加To。参数是基本型，则就直接基本类型即可。比如：ToLongFunction 和 LongFunction。 这些处理基本类型线管的Stream和一半的Stream是有区别的，比如LongStream。而这类特殊的Stream中的map方法的实现方式也不同，它接受的是一个LongUnaryOperator函数，将一个长整型映射成另一个长整型值。 public static void example1(){ List artists = List.of(new Artist(11, \"China\", \"Sam\"), new Artist(9, \"UK\", \"Die\"), new Artist(22, \"US\", \"Daming\")); IntSummaryStatistics intSummaryStatistics = artists.stream() .mapToInt(Artist::getAge) .summaryStatistics(); System.out.printf(\"Max: %d, Min: %d. Ave: %f, Sum: %d\", intSummaryStatistics.getMax(), intSummaryStatistics.getMin(), intSummaryStatistics.getAverage(), intSummaryStatistics.getSum()); } 重载解析Lambda表达式作为参数时，其类型由它的目标类型推导得出，遵循如下规则： 如果只有一个可能的目标类型，由相应函数接口里的参数类型推导得出； 如果有多个可能的目标类型，由最具体的类型推导得出； 如果有多个可能的目标类型且最具体的类型不明确，则需要认为指定类型。 @FunctionalInterface实际上，每个用作函数接口的接口都应该添加这个注释。该注释糊强制javac检查一个接口是否符合函数接口的标准。 如果该注释添加给一个枚举类型、类或另一个注释，或者接口包含不止一个抽象方法，javac就会报错。 重构代码时，使用它很容易发现问题。 新的语言特性：默认方法为了保证二进制接口的兼容性而出现的。 新关键字: default，这个关键字告诉javac，用户真正需要的是为接口添加一个新方法，从而进行区分。 三定律： 类胜于接口。如果在继承链中有方法体或抽象的方法声明，那么就可以忽略接口中定义的方法。（让代码向后兼容） 子类胜于父类。如果一个接口继承了另一个接口，且两个接口都定义了一个默认方法，那么子类定义的方法胜出。 如果上面两个规则不适用，子类要么需要实现该方法，要么将该方法声明为抽象方法。 Optionalreduce方法有一个重点，它有两种形式： 需要有一个初始值，上面有例子。 不需要有初始值，reduce第一步使用Stream中的前两个元素，此时返回一个Optinal对象。 Optional用来替换null值，因为null最大的问题在于NullPointerException。 Optional两个目的，鼓励程序员适时检查变量是否为空，避免代码缺陷；二它将是一个类的API中可能为空的值文档化，这比阅读实现代码要简单的多。 使用方式，在调用get之前，先使用 isPresent()检查Optional对象是否有值，或者用 orElse()。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:2:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#基本类型"},{"categories":["java"],"content":"\r在代码中使用Lambda表达式\r降低日志性能开销\r基本类型基本类型对应着装箱类型，int - Integer。后者是java的普通类，是对基本类的封装。 Java泛型是基于对泛型参数类型的擦除。只有装箱类型才能作为泛型参数，基本类型不可以。 但是装箱类型是对象，这表示在内存中存在额外的开销。比如，整型的在内存中占用4字节，整形对象却占16字节。数组这一情况更严重，整型对象数组中，每个元素都是内存的一个指针，只想堆中某个对象。最坏情况，同样的数组，Integer[] 要比 int[] 多占用6倍内存。 而二者互相转换的过程称为装箱和拆箱，过程中都需要额外的计算开销，会影响程序运行速度。 Stream类中某些方法对基本类型和装箱类型做了区分，进而减小些性能开销。 命名规范，返回类型为基本类型，则在基本类型前加To。参数是基本型，则就直接基本类型即可。比如：ToLongFunction 和 LongFunction。 这些处理基本类型线管的Stream和一半的Stream是有区别的，比如LongStream。而这类特殊的Stream中的map方法的实现方式也不同，它接受的是一个LongUnaryOperator函数，将一个长整型映射成另一个长整型值。 public static void example1(){ List artists = List.of(new Artist(11, \"China\", \"Sam\"), new Artist(9, \"UK\", \"Die\"), new Artist(22, \"US\", \"Daming\")); IntSummaryStatistics intSummaryStatistics = artists.stream() .mapToInt(Artist::getAge) .summaryStatistics(); System.out.printf(\"Max: %d, Min: %d. Ave: %f, Sum: %d\", intSummaryStatistics.getMax(), intSummaryStatistics.getMin(), intSummaryStatistics.getAverage(), intSummaryStatistics.getSum()); } 重载解析Lambda表达式作为参数时，其类型由它的目标类型推导得出，遵循如下规则： 如果只有一个可能的目标类型，由相应函数接口里的参数类型推导得出； 如果有多个可能的目标类型，由最具体的类型推导得出； 如果有多个可能的目标类型且最具体的类型不明确，则需要认为指定类型。 @FunctionalInterface实际上，每个用作函数接口的接口都应该添加这个注释。该注释糊强制javac检查一个接口是否符合函数接口的标准。 如果该注释添加给一个枚举类型、类或另一个注释，或者接口包含不止一个抽象方法，javac就会报错。 重构代码时，使用它很容易发现问题。 新的语言特性：默认方法为了保证二进制接口的兼容性而出现的。 新关键字: default，这个关键字告诉javac，用户真正需要的是为接口添加一个新方法，从而进行区分。 三定律： 类胜于接口。如果在继承链中有方法体或抽象的方法声明，那么就可以忽略接口中定义的方法。（让代码向后兼容） 子类胜于父类。如果一个接口继承了另一个接口，且两个接口都定义了一个默认方法，那么子类定义的方法胜出。 如果上面两个规则不适用，子类要么需要实现该方法，要么将该方法声明为抽象方法。 Optionalreduce方法有一个重点，它有两种形式： 需要有一个初始值，上面有例子。 不需要有初始值，reduce第一步使用Stream中的前两个元素，此时返回一个Optinal对象。 Optional用来替换null值，因为null最大的问题在于NullPointerException。 Optional两个目的，鼓励程序员适时检查变量是否为空，避免代码缺陷；二它将是一个类的API中可能为空的值文档化，这比阅读实现代码要简单的多。 使用方式，在调用get之前，先使用 isPresent()检查Optional对象是否有值，或者用 orElse()。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:2:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#重载解析"},{"categories":["java"],"content":"\r在代码中使用Lambda表达式\r降低日志性能开销\r基本类型基本类型对应着装箱类型，int - Integer。后者是java的普通类，是对基本类的封装。 Java泛型是基于对泛型参数类型的擦除。只有装箱类型才能作为泛型参数，基本类型不可以。 但是装箱类型是对象，这表示在内存中存在额外的开销。比如，整型的在内存中占用4字节，整形对象却占16字节。数组这一情况更严重，整型对象数组中，每个元素都是内存的一个指针，只想堆中某个对象。最坏情况，同样的数组，Integer[] 要比 int[] 多占用6倍内存。 而二者互相转换的过程称为装箱和拆箱，过程中都需要额外的计算开销，会影响程序运行速度。 Stream类中某些方法对基本类型和装箱类型做了区分，进而减小些性能开销。 命名规范，返回类型为基本类型，则在基本类型前加To。参数是基本型，则就直接基本类型即可。比如：ToLongFunction 和 LongFunction。 这些处理基本类型线管的Stream和一半的Stream是有区别的，比如LongStream。而这类特殊的Stream中的map方法的实现方式也不同，它接受的是一个LongUnaryOperator函数，将一个长整型映射成另一个长整型值。 public static void example1(){ List artists = List.of(new Artist(11, \"China\", \"Sam\"), new Artist(9, \"UK\", \"Die\"), new Artist(22, \"US\", \"Daming\")); IntSummaryStatistics intSummaryStatistics = artists.stream() .mapToInt(Artist::getAge) .summaryStatistics(); System.out.printf(\"Max: %d, Min: %d. Ave: %f, Sum: %d\", intSummaryStatistics.getMax(), intSummaryStatistics.getMin(), intSummaryStatistics.getAverage(), intSummaryStatistics.getSum()); } 重载解析Lambda表达式作为参数时，其类型由它的目标类型推导得出，遵循如下规则： 如果只有一个可能的目标类型，由相应函数接口里的参数类型推导得出； 如果有多个可能的目标类型，由最具体的类型推导得出； 如果有多个可能的目标类型且最具体的类型不明确，则需要认为指定类型。 @FunctionalInterface实际上，每个用作函数接口的接口都应该添加这个注释。该注释糊强制javac检查一个接口是否符合函数接口的标准。 如果该注释添加给一个枚举类型、类或另一个注释，或者接口包含不止一个抽象方法，javac就会报错。 重构代码时，使用它很容易发现问题。 新的语言特性：默认方法为了保证二进制接口的兼容性而出现的。 新关键字: default，这个关键字告诉javac，用户真正需要的是为接口添加一个新方法，从而进行区分。 三定律： 类胜于接口。如果在继承链中有方法体或抽象的方法声明，那么就可以忽略接口中定义的方法。（让代码向后兼容） 子类胜于父类。如果一个接口继承了另一个接口，且两个接口都定义了一个默认方法，那么子类定义的方法胜出。 如果上面两个规则不适用，子类要么需要实现该方法，要么将该方法声明为抽象方法。 Optionalreduce方法有一个重点，它有两种形式： 需要有一个初始值，上面有例子。 不需要有初始值，reduce第一步使用Stream中的前两个元素，此时返回一个Optinal对象。 Optional用来替换null值，因为null最大的问题在于NullPointerException。 Optional两个目的，鼓励程序员适时检查变量是否为空，避免代码缺陷；二它将是一个类的API中可能为空的值文档化，这比阅读实现代码要简单的多。 使用方式，在调用get之前，先使用 isPresent()检查Optional对象是否有值，或者用 orElse()。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:2:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#functionalinterface"},{"categories":["java"],"content":"\r在代码中使用Lambda表达式\r降低日志性能开销\r基本类型基本类型对应着装箱类型，int - Integer。后者是java的普通类，是对基本类的封装。 Java泛型是基于对泛型参数类型的擦除。只有装箱类型才能作为泛型参数，基本类型不可以。 但是装箱类型是对象，这表示在内存中存在额外的开销。比如，整型的在内存中占用4字节，整形对象却占16字节。数组这一情况更严重，整型对象数组中，每个元素都是内存的一个指针，只想堆中某个对象。最坏情况，同样的数组，Integer[] 要比 int[] 多占用6倍内存。 而二者互相转换的过程称为装箱和拆箱，过程中都需要额外的计算开销，会影响程序运行速度。 Stream类中某些方法对基本类型和装箱类型做了区分，进而减小些性能开销。 命名规范，返回类型为基本类型，则在基本类型前加To。参数是基本型，则就直接基本类型即可。比如：ToLongFunction 和 LongFunction。 这些处理基本类型线管的Stream和一半的Stream是有区别的，比如LongStream。而这类特殊的Stream中的map方法的实现方式也不同，它接受的是一个LongUnaryOperator函数，将一个长整型映射成另一个长整型值。 public static void example1(){ List artists = List.of(new Artist(11, \"China\", \"Sam\"), new Artist(9, \"UK\", \"Die\"), new Artist(22, \"US\", \"Daming\")); IntSummaryStatistics intSummaryStatistics = artists.stream() .mapToInt(Artist::getAge) .summaryStatistics(); System.out.printf(\"Max: %d, Min: %d. Ave: %f, Sum: %d\", intSummaryStatistics.getMax(), intSummaryStatistics.getMin(), intSummaryStatistics.getAverage(), intSummaryStatistics.getSum()); } 重载解析Lambda表达式作为参数时，其类型由它的目标类型推导得出，遵循如下规则： 如果只有一个可能的目标类型，由相应函数接口里的参数类型推导得出； 如果有多个可能的目标类型，由最具体的类型推导得出； 如果有多个可能的目标类型且最具体的类型不明确，则需要认为指定类型。 @FunctionalInterface实际上，每个用作函数接口的接口都应该添加这个注释。该注释糊强制javac检查一个接口是否符合函数接口的标准。 如果该注释添加给一个枚举类型、类或另一个注释，或者接口包含不止一个抽象方法，javac就会报错。 重构代码时，使用它很容易发现问题。 新的语言特性：默认方法为了保证二进制接口的兼容性而出现的。 新关键字: default，这个关键字告诉javac，用户真正需要的是为接口添加一个新方法，从而进行区分。 三定律： 类胜于接口。如果在继承链中有方法体或抽象的方法声明，那么就可以忽略接口中定义的方法。（让代码向后兼容） 子类胜于父类。如果一个接口继承了另一个接口，且两个接口都定义了一个默认方法，那么子类定义的方法胜出。 如果上面两个规则不适用，子类要么需要实现该方法，要么将该方法声明为抽象方法。 Optionalreduce方法有一个重点，它有两种形式： 需要有一个初始值，上面有例子。 不需要有初始值，reduce第一步使用Stream中的前两个元素，此时返回一个Optinal对象。 Optional用来替换null值，因为null最大的问题在于NullPointerException。 Optional两个目的，鼓励程序员适时检查变量是否为空，避免代码缺陷；二它将是一个类的API中可能为空的值文档化，这比阅读实现代码要简单的多。 使用方式，在调用get之前，先使用 isPresent()检查Optional对象是否有值，或者用 orElse()。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:2:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#新的语言特性默认方法"},{"categories":["java"],"content":"\r在代码中使用Lambda表达式\r降低日志性能开销\r基本类型基本类型对应着装箱类型，int - Integer。后者是java的普通类，是对基本类的封装。 Java泛型是基于对泛型参数类型的擦除。只有装箱类型才能作为泛型参数，基本类型不可以。 但是装箱类型是对象，这表示在内存中存在额外的开销。比如，整型的在内存中占用4字节，整形对象却占16字节。数组这一情况更严重，整型对象数组中，每个元素都是内存的一个指针，只想堆中某个对象。最坏情况，同样的数组，Integer[] 要比 int[] 多占用6倍内存。 而二者互相转换的过程称为装箱和拆箱，过程中都需要额外的计算开销，会影响程序运行速度。 Stream类中某些方法对基本类型和装箱类型做了区分，进而减小些性能开销。 命名规范，返回类型为基本类型，则在基本类型前加To。参数是基本型，则就直接基本类型即可。比如：ToLongFunction 和 LongFunction。 这些处理基本类型线管的Stream和一半的Stream是有区别的，比如LongStream。而这类特殊的Stream中的map方法的实现方式也不同，它接受的是一个LongUnaryOperator函数，将一个长整型映射成另一个长整型值。 public static void example1(){ List artists = List.of(new Artist(11, \"China\", \"Sam\"), new Artist(9, \"UK\", \"Die\"), new Artist(22, \"US\", \"Daming\")); IntSummaryStatistics intSummaryStatistics = artists.stream() .mapToInt(Artist::getAge) .summaryStatistics(); System.out.printf(\"Max: %d, Min: %d. Ave: %f, Sum: %d\", intSummaryStatistics.getMax(), intSummaryStatistics.getMin(), intSummaryStatistics.getAverage(), intSummaryStatistics.getSum()); } 重载解析Lambda表达式作为参数时，其类型由它的目标类型推导得出，遵循如下规则： 如果只有一个可能的目标类型，由相应函数接口里的参数类型推导得出； 如果有多个可能的目标类型，由最具体的类型推导得出； 如果有多个可能的目标类型且最具体的类型不明确，则需要认为指定类型。 @FunctionalInterface实际上，每个用作函数接口的接口都应该添加这个注释。该注释糊强制javac检查一个接口是否符合函数接口的标准。 如果该注释添加给一个枚举类型、类或另一个注释，或者接口包含不止一个抽象方法，javac就会报错。 重构代码时，使用它很容易发现问题。 新的语言特性：默认方法为了保证二进制接口的兼容性而出现的。 新关键字: default，这个关键字告诉javac，用户真正需要的是为接口添加一个新方法，从而进行区分。 三定律： 类胜于接口。如果在继承链中有方法体或抽象的方法声明，那么就可以忽略接口中定义的方法。（让代码向后兼容） 子类胜于父类。如果一个接口继承了另一个接口，且两个接口都定义了一个默认方法，那么子类定义的方法胜出。 如果上面两个规则不适用，子类要么需要实现该方法，要么将该方法声明为抽象方法。 Optionalreduce方法有一个重点，它有两种形式： 需要有一个初始值，上面有例子。 不需要有初始值，reduce第一步使用Stream中的前两个元素，此时返回一个Optinal对象。 Optional用来替换null值，因为null最大的问题在于NullPointerException。 Optional两个目的，鼓励程序员适时检查变量是否为空，避免代码缺陷；二它将是一个类的API中可能为空的值文档化，这比阅读实现代码要简单的多。 使用方式，在调用get之前，先使用 isPresent()检查Optional对象是否有值，或者用 orElse()。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:2:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#optional"},{"categories":["java"],"content":"\r高级集合类和收集器","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:0","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#高级集合类和收集器"},{"categories":["java"],"content":"\r方法引用Lambda通常表示方式： artist -\u003e artist.getName() 方法引用是一种简写语法： Artist::getName 标准语法为：Classname::methodName 注意：虽然是方法，但是不需要在后面加括号，因为这不属于调用，这是方法引用，当需要的时候，系统会自动调用。 构造函数的缩写形式\r创建对象Lambda方式： (name,nationality) -\u003e new Artist(name,nationality) 方法引用 Artist::new 创建数组 String[]::new ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#方法引用"},{"categories":["java"],"content":"\r方法引用Lambda通常表示方式： artist -\u003e artist.getName() 方法引用是一种简写语法： Artist::getName 标准语法为：Classname::methodName 注意：虽然是方法，但是不需要在后面加括号，因为这不属于调用，这是方法引用，当需要的时候，系统会自动调用。 构造函数的缩写形式\r创建对象Lambda方式： (name,nationality) -\u003e new Artist(name,nationality) 方法引用 Artist::new 创建数组 String[]::new ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#构造函数的缩写形式"},{"categories":["java"],"content":"\r方法引用Lambda通常表示方式： artist -\u003e artist.getName() 方法引用是一种简写语法： Artist::getName 标准语法为：Classname::methodName 注意：虽然是方法，但是不需要在后面加括号，因为这不属于调用，这是方法引用，当需要的时候，系统会自动调用。 构造函数的缩写形式\r创建对象Lambda方式： (name,nationality) -\u003e new Artist(name,nationality) 方法引用 Artist::new 创建数组 String[]::new ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#创建对象"},{"categories":["java"],"content":"\r方法引用Lambda通常表示方式： artist -\u003e artist.getName() 方法引用是一种简写语法： Artist::getName 标准语法为：Classname::methodName 注意：虽然是方法，但是不需要在后面加括号，因为这不属于调用，这是方法引用，当需要的时候，系统会自动调用。 构造函数的缩写形式\r创建对象Lambda方式： (name,nationality) -\u003e new Artist(name,nationality) 方法引用 Artist::new 创建数组 String[]::new ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#创建数组"},{"categories":["java"],"content":"\r元素顺序集合类的内容是流中的元素以何种顺序排列，比如顺序list；无序hashset。增加了流操作后，顺序问题变得更加复杂。 直观上，流是有序的，因为流中的元素都是按照顺序处理的，这叫做出现顺序。它依赖于数据源和对流的操作。 在有序集合中创建流，流中元素就是有序的。集合本身是无序的，生成的流则也是。 List\u003cInteger\u003e numbers = asList(1,2,3,4); List\u003cInteger\u003e sameOder = numbers.stream().toList(); Set\u003cInteger\u003e numbers = new HashSet\u003c\u003e(asList(4,3,2,1)); List\u003cInteger\u003e sameOder = numbers.stream().toList(); 无序集合生成出现顺序 Set\u003cInteger\u003e numbers = newHashSet\u003c\u003e(asList(4,3,2,1)); // 流中选择排序 List\u003cInteger\u003e sameOder = numbers.stream().sorted().toList(); ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:2","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#元素顺序"},{"categories":["java"],"content":"\r使用收集器从java.until.stream.Collectors类中导入 转换成其他集合收集器生成其他集合，比如 toList、toSet、toCollection。 通常情况创建集合需要指定具体类型： List\u003cString\u003e strList = new ArrayList\u003c\u003e(); 而toList或toSet不需要指定具体类型。Stream会组动挑选合适的类型。 指定集合类型： stream.collect(toCollection(TreeSet::new)); 转换成值maxBy 和 minBy 允许用户按照特定的顺序生成值。 public Optional\u003cArtist\u003e biggestGroup(Stream\u003cArtist\u003e artists){ Function\u003cArtist, Long\u003e getCount = artist -\u003e artist.getMembers().count(); return artists.collect(maxBy(comparing(getCount))); // 也可以是 return artists.max(comparing(getCount)); } 求平均值 Double collect = artList.stream() .collect(averagingInt(Artist::getAge)); 数据分块将其分解成两个集合。假设有一个A流，将其分为B流和C流，可以通过两次过滤操作得到两种流。但问题是，为了执行两次过滤操作就需要两个流，其次如果过滤操作复杂，两次的操作就会导致代码的冗余。 解决办法：收集器 partitioningBy，它接受一个流，并将其分为两部分。它使用 Predicate对象判断一个元素应该属于哪个部分，并根据boolean返回一个Map到列表。因此，对于true List中的元素，Predicate返回true，对于其他list中的元素，返回false。 Map\u003cBoolean, List\u003cArtist\u003e\u003e booleanListMap = artList.stream() .collect(partitioningBy(artist -\u003e artist.getAge() \u003c 13)); // 如果方法自带判断属性也可以直接用方法引用 Map\u003cBoolean, List\u003cArtist\u003e\u003e booleanListMap = artList.stream() .collect(partitioningBy(Artist::isSolo)); 数据分组它是更自然的分割数据操作，与数据分块不同（将数据分成True、False两部分不同），它可以使用任意值对数据分组。groupingBy() Map\u003cInteger, List\u003cArtist\u003e\u003e collect = artList.stream().collect(groupingBy(Artist::getAge)); 字符串最后生成一个字符串。 传统方式： StringBuilder builder = new StringBuilder(\"[\"); for(Artist artist : artists){ if(builder.length() \u003e 1){ builder.append(\",\"); } String name = artist.getName(); builder.append(name); } builder.append(\"]\"); String result = builder.toString(); Stream方式： String result = artists.stream() .map(Artist::getName) .collect(Collectors.joining(\",\",\"[\",\"]\")); 组合收集器在数据分组上进行改进，不需要返回两个对象的列表，而是返回对象中的某个属性。 比如，计算一个艺术家的专辑数量： public Map\u003cArtist, Long\u003e numberOfAlbums(Stream\u003cAlbum\u003e albums){ return albums.collect(groupingBy(album -\u003e album.getMainMusician(), counting)); } 重点是另一个收集器，counting，使用它就可以对专辑记数了。 比如，这次不想得到一组专辑，而是要得到专辑名： public Map\u003cArtist, Long\u003e numberOfAlbums(Stream\u003cAlbum\u003e albums){ return albums.collect(groupingBy(Album::getMainMusician, mapping(Album::getName, toList()))); } mapping收集器和map方法一样，接受一个Function对象作为参数。它允许在收集器的容器上执行类似map操作，但是需要致命使用什么样的集合类存储结果。 不管是mapping还是joining都叫做下游收集器。收集器是生成最终结果的一剂配方，下游收集器则是生成部分结果的配方，主收集器中会用到下游收集器。 重构和定制收集器传统连接字符串 StringBuilder builder = new StringBuilder(\"[\"); for(Artist artist : artists){ if(builder.length() \u003e 1){ builder.append(\",\"); } builder.append(artist.getName()); } String str = builder.toString(); map操作： StringBuilder builder = new StringBuilder(\"[\"); artists.stream() .map(Artist::getName) .forEach(name -\u003e { if (builder.length() \u003e 1) builder.append(\",\"); }); builder.append(\"]\"); String str = builder.toString(); 感觉和传统方式没有什么优势，同样都需要大量的代码，可读性依旧不高。 使用reduce操作： StringBuilder reduce = artists.stream() .map(Artist::getName) .reduce(new StringBuilder(), (builder, name) -\u003e { if (builder.length() \u003e 1) builder.append(\",\"); builder.append(name); return builder; }, (left, right) -\u003e left.append(right)); // 这一步也可以写成 , StringBuilder::append reduce.append(\"]\"); String srt = reduce.toString(); 这么一看，更复杂，可读性更差。 重构： // 自定义类 StringCombiner combined = artists.stream() .map(Artist::getName) .reduce(new StringCombiner(\",\",\"[\",\"]\"), StringCombiner::add, StringCombiner::merge); String str = combined.toString(); 看着代码简洁了很多，但是背后的工作是一样的，只不过是将之前繁多的步骤，封装在一个StringCombiner类中。 StringCombiner.add()方法 public StringCombiner add(String element){ // 如果是在初始位置，则添加前缀 if(areStart()){ builder.append(prefix); // 否则添加 界定符（delimit） }else{ builder.append(delim); } // 正常添加元素 builder.append(element); return this; } add方法内其实调用的是一个StringBuilder对象。 StringCombiner.merge()方法 public StringCombiner merge(StringCombiner other){ builder.append(other.builder); return this; } 在重构部分，将reduce操作重构为一个收集器，其实还是有些冗余。既然怎么都要创建一个自定义类，为什么不直接创建一个收集器就叫StringCollector。 重构2： String str = artists.stream() .map(Artist::getName) .collect(new StringCollector(\",\", \"[\", \"]\")); 将所有的对字符串的操作代理交给了定制的收集器StringCollector，应用程序就不需要再关注其内部任何细节。它和框架中其他的Collector对象用起来应该都是一样的。 要实现Collector接口，由于Collector接口支持泛型，因此先得确定一些具体的类型： 待收集元素的类型，这里是String； 累加器得类型StringCombi","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:3","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#使用收集器"},{"categories":["java"],"content":"\r使用收集器从java.until.stream.Collectors类中导入 转换成其他集合收集器生成其他集合，比如 toList、toSet、toCollection。 通常情况创建集合需要指定具体类型： List strList = new ArrayList\u003c\u003e(); 而toList或toSet不需要指定具体类型。Stream会组动挑选合适的类型。 指定集合类型： stream.collect(toCollection(TreeSet::new)); 转换成值maxBy 和 minBy 允许用户按照特定的顺序生成值。 public Optional biggestGroup(Stream artists){ Function getCount = artist -\u003e artist.getMembers().count(); return artists.collect(maxBy(comparing(getCount))); // 也可以是 return artists.max(comparing(getCount)); } 求平均值 Double collect = artList.stream() .collect(averagingInt(Artist::getAge)); 数据分块将其分解成两个集合。假设有一个A流，将其分为B流和C流，可以通过两次过滤操作得到两种流。但问题是，为了执行两次过滤操作就需要两个流，其次如果过滤操作复杂，两次的操作就会导致代码的冗余。 解决办法：收集器 partitioningBy，它接受一个流，并将其分为两部分。它使用 Predicate对象判断一个元素应该属于哪个部分，并根据boolean返回一个Map到列表。因此，对于true List中的元素，Predicate返回true，对于其他list中的元素，返回false。 Map","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:3","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#转换成其他集合"},{"categories":["java"],"content":"\r使用收集器从java.until.stream.Collectors类中导入 转换成其他集合收集器生成其他集合，比如 toList、toSet、toCollection。 通常情况创建集合需要指定具体类型： List strList = new ArrayList\u003c\u003e(); 而toList或toSet不需要指定具体类型。Stream会组动挑选合适的类型。 指定集合类型： stream.collect(toCollection(TreeSet::new)); 转换成值maxBy 和 minBy 允许用户按照特定的顺序生成值。 public Optional biggestGroup(Stream artists){ Function getCount = artist -\u003e artist.getMembers().count(); return artists.collect(maxBy(comparing(getCount))); // 也可以是 return artists.max(comparing(getCount)); } 求平均值 Double collect = artList.stream() .collect(averagingInt(Artist::getAge)); 数据分块将其分解成两个集合。假设有一个A流，将其分为B流和C流，可以通过两次过滤操作得到两种流。但问题是，为了执行两次过滤操作就需要两个流，其次如果过滤操作复杂，两次的操作就会导致代码的冗余。 解决办法：收集器 partitioningBy，它接受一个流，并将其分为两部分。它使用 Predicate对象判断一个元素应该属于哪个部分，并根据boolean返回一个Map到列表。因此，对于true List中的元素，Predicate返回true，对于其他list中的元素，返回false。 Map","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:3","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#转换成值"},{"categories":["java"],"content":"\r使用收集器从java.until.stream.Collectors类中导入 转换成其他集合收集器生成其他集合，比如 toList、toSet、toCollection。 通常情况创建集合需要指定具体类型： List strList = new ArrayList\u003c\u003e(); 而toList或toSet不需要指定具体类型。Stream会组动挑选合适的类型。 指定集合类型： stream.collect(toCollection(TreeSet::new)); 转换成值maxBy 和 minBy 允许用户按照特定的顺序生成值。 public Optional biggestGroup(Stream artists){ Function getCount = artist -\u003e artist.getMembers().count(); return artists.collect(maxBy(comparing(getCount))); // 也可以是 return artists.max(comparing(getCount)); } 求平均值 Double collect = artList.stream() .collect(averagingInt(Artist::getAge)); 数据分块将其分解成两个集合。假设有一个A流，将其分为B流和C流，可以通过两次过滤操作得到两种流。但问题是，为了执行两次过滤操作就需要两个流，其次如果过滤操作复杂，两次的操作就会导致代码的冗余。 解决办法：收集器 partitioningBy，它接受一个流，并将其分为两部分。它使用 Predicate对象判断一个元素应该属于哪个部分，并根据boolean返回一个Map到列表。因此，对于true List中的元素，Predicate返回true，对于其他list中的元素，返回false。 Map","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:3","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#数据分块"},{"categories":["java"],"content":"\r使用收集器从java.until.stream.Collectors类中导入 转换成其他集合收集器生成其他集合，比如 toList、toSet、toCollection。 通常情况创建集合需要指定具体类型： List strList = new ArrayList\u003c\u003e(); 而toList或toSet不需要指定具体类型。Stream会组动挑选合适的类型。 指定集合类型： stream.collect(toCollection(TreeSet::new)); 转换成值maxBy 和 minBy 允许用户按照特定的顺序生成值。 public Optional biggestGroup(Stream artists){ Function getCount = artist -\u003e artist.getMembers().count(); return artists.collect(maxBy(comparing(getCount))); // 也可以是 return artists.max(comparing(getCount)); } 求平均值 Double collect = artList.stream() .collect(averagingInt(Artist::getAge)); 数据分块将其分解成两个集合。假设有一个A流，将其分为B流和C流，可以通过两次过滤操作得到两种流。但问题是，为了执行两次过滤操作就需要两个流，其次如果过滤操作复杂，两次的操作就会导致代码的冗余。 解决办法：收集器 partitioningBy，它接受一个流，并将其分为两部分。它使用 Predicate对象判断一个元素应该属于哪个部分，并根据boolean返回一个Map到列表。因此，对于true List中的元素，Predicate返回true，对于其他list中的元素，返回false。 Map","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:3","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#数据分组"},{"categories":["java"],"content":"\r使用收集器从java.until.stream.Collectors类中导入 转换成其他集合收集器生成其他集合，比如 toList、toSet、toCollection。 通常情况创建集合需要指定具体类型： List strList = new ArrayList\u003c\u003e(); 而toList或toSet不需要指定具体类型。Stream会组动挑选合适的类型。 指定集合类型： stream.collect(toCollection(TreeSet::new)); 转换成值maxBy 和 minBy 允许用户按照特定的顺序生成值。 public Optional biggestGroup(Stream artists){ Function getCount = artist -\u003e artist.getMembers().count(); return artists.collect(maxBy(comparing(getCount))); // 也可以是 return artists.max(comparing(getCount)); } 求平均值 Double collect = artList.stream() .collect(averagingInt(Artist::getAge)); 数据分块将其分解成两个集合。假设有一个A流，将其分为B流和C流，可以通过两次过滤操作得到两种流。但问题是，为了执行两次过滤操作就需要两个流，其次如果过滤操作复杂，两次的操作就会导致代码的冗余。 解决办法：收集器 partitioningBy，它接受一个流，并将其分为两部分。它使用 Predicate对象判断一个元素应该属于哪个部分，并根据boolean返回一个Map到列表。因此，对于true List中的元素，Predicate返回true，对于其他list中的元素，返回false。 Map","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:3","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#字符串"},{"categories":["java"],"content":"\r使用收集器从java.until.stream.Collectors类中导入 转换成其他集合收集器生成其他集合，比如 toList、toSet、toCollection。 通常情况创建集合需要指定具体类型： List strList = new ArrayList\u003c\u003e(); 而toList或toSet不需要指定具体类型。Stream会组动挑选合适的类型。 指定集合类型： stream.collect(toCollection(TreeSet::new)); 转换成值maxBy 和 minBy 允许用户按照特定的顺序生成值。 public Optional biggestGroup(Stream artists){ Function getCount = artist -\u003e artist.getMembers().count(); return artists.collect(maxBy(comparing(getCount))); // 也可以是 return artists.max(comparing(getCount)); } 求平均值 Double collect = artList.stream() .collect(averagingInt(Artist::getAge)); 数据分块将其分解成两个集合。假设有一个A流，将其分为B流和C流，可以通过两次过滤操作得到两种流。但问题是，为了执行两次过滤操作就需要两个流，其次如果过滤操作复杂，两次的操作就会导致代码的冗余。 解决办法：收集器 partitioningBy，它接受一个流，并将其分为两部分。它使用 Predicate对象判断一个元素应该属于哪个部分，并根据boolean返回一个Map到列表。因此，对于true List中的元素，Predicate返回true，对于其他list中的元素，返回false。 Map","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:3","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#组合收集器"},{"categories":["java"],"content":"\r使用收集器从java.until.stream.Collectors类中导入 转换成其他集合收集器生成其他集合，比如 toList、toSet、toCollection。 通常情况创建集合需要指定具体类型： List strList = new ArrayList\u003c\u003e(); 而toList或toSet不需要指定具体类型。Stream会组动挑选合适的类型。 指定集合类型： stream.collect(toCollection(TreeSet::new)); 转换成值maxBy 和 minBy 允许用户按照特定的顺序生成值。 public Optional biggestGroup(Stream artists){ Function getCount = artist -\u003e artist.getMembers().count(); return artists.collect(maxBy(comparing(getCount))); // 也可以是 return artists.max(comparing(getCount)); } 求平均值 Double collect = artList.stream() .collect(averagingInt(Artist::getAge)); 数据分块将其分解成两个集合。假设有一个A流，将其分为B流和C流，可以通过两次过滤操作得到两种流。但问题是，为了执行两次过滤操作就需要两个流，其次如果过滤操作复杂，两次的操作就会导致代码的冗余。 解决办法：收集器 partitioningBy，它接受一个流，并将其分为两部分。它使用 Predicate对象判断一个元素应该属于哪个部分，并根据boolean返回一个Map到列表。因此，对于true List中的元素，Predicate返回true，对于其他list中的元素，返回false。 Map","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:3","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#重构和定制收集器"},{"categories":["java"],"content":"\r使用收集器从java.until.stream.Collectors类中导入 转换成其他集合收集器生成其他集合，比如 toList、toSet、toCollection。 通常情况创建集合需要指定具体类型： List strList = new ArrayList\u003c\u003e(); 而toList或toSet不需要指定具体类型。Stream会组动挑选合适的类型。 指定集合类型： stream.collect(toCollection(TreeSet::new)); 转换成值maxBy 和 minBy 允许用户按照特定的顺序生成值。 public Optional biggestGroup(Stream artists){ Function getCount = artist -\u003e artist.getMembers().count(); return artists.collect(maxBy(comparing(getCount))); // 也可以是 return artists.max(comparing(getCount)); } 求平均值 Double collect = artList.stream() .collect(averagingInt(Artist::getAge)); 数据分块将其分解成两个集合。假设有一个A流，将其分为B流和C流，可以通过两次过滤操作得到两种流。但问题是，为了执行两次过滤操作就需要两个流，其次如果过滤操作复杂，两次的操作就会导致代码的冗余。 解决办法：收集器 partitioningBy，它接受一个流，并将其分为两部分。它使用 Predicate对象判断一个元素应该属于哪个部分，并根据boolean返回一个Map到列表。因此，对于true List中的元素，Predicate返回true，对于其他list中的元素，返回false。 Map","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:3","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#对收集器的归一化处理"},{"categories":["java"],"content":"\r使用收集器从java.until.stream.Collectors类中导入 转换成其他集合收集器生成其他集合，比如 toList、toSet、toCollection。 通常情况创建集合需要指定具体类型： List strList = new ArrayList\u003c\u003e(); 而toList或toSet不需要指定具体类型。Stream会组动挑选合适的类型。 指定集合类型： stream.collect(toCollection(TreeSet::new)); 转换成值maxBy 和 minBy 允许用户按照特定的顺序生成值。 public Optional biggestGroup(Stream artists){ Function getCount = artist -\u003e artist.getMembers().count(); return artists.collect(maxBy(comparing(getCount))); // 也可以是 return artists.max(comparing(getCount)); } 求平均值 Double collect = artList.stream() .collect(averagingInt(Artist::getAge)); 数据分块将其分解成两个集合。假设有一个A流，将其分为B流和C流，可以通过两次过滤操作得到两种流。但问题是，为了执行两次过滤操作就需要两个流，其次如果过滤操作复杂，两次的操作就会导致代码的冗余。 解决办法：收集器 partitioningBy，它接受一个流，并将其分为两部分。它使用 Predicate对象判断一个元素应该属于哪个部分，并根据boolean返回一个Map到列表。因此，对于true List中的元素，Predicate返回true，对于其他list中的元素，返回false。 Map","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:3","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#map迭代方式"},{"categories":["java"],"content":"\r补充\rboxed()IntSream是int类型的流，Stream\u003cInteger\u003e 是Integer类型的流；而 boxed()值得就是对基本类型的装箱。 mapToInt()就是将参数转成IntStream。然后接着 .toArray()就是转成数组。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:4","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#补充"},{"categories":["java"],"content":"\r补充\rboxed()IntSream是int类型的流，Stream 是Integer类型的流；而 boxed()值得就是对基本类型的装箱。 mapToInt()就是将参数转成IntStream。然后接着 .toArray()就是转成数组。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:4","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#boxed"},{"categories":["java"],"content":"\r补充\rboxed()IntSream是int类型的流，Stream 是Integer类型的流；而 boxed()值得就是对基本类型的装箱。 mapToInt()就是将参数转成IntStream。然后接着 .toArray()就是转成数组。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:3:4","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#maptoint"},{"categories":["java"],"content":"\r数据并行化赘述为什么需要并行化和什么时候会带来性能的提升 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:4:0","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#数据并行化"},{"categories":["java"],"content":"\r并行和并发并发：一个时间段内处理两个任务，他们之间还是有先后顺序（单核）。 并行：两个任务同时执行（多核cpu）。并行化，是指为缩短任务执行时间，将一个任务分解成几部分，然后并行执行。举例，很多马来拉车，省时省力。相应地，cpu承载的工作量比顺序执行要大。 数据并行化，将数据分成块，每块数据分配单独的处理单元。举例，原来一个马车拉的货，现在分给另一个马车，一边拉一半。 在处理大量数据上，数据并行化管用。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:4:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#并行和并发"},{"categories":["java"],"content":"\r并行化的重要性以前计算能力依赖单核cpu的时钟频率的提升，比如5mhz到60mhz。现在单核cpu的时钟频率提升不大，大家都转向了多核处理器，因此，想提升计算能力，就要将任务分解交个各个核同时处理，拆分的越多分配的核数越多，处理时间越迅速。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:4:2","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#并行化的重要性"},{"categories":["java"],"content":"\r并行化流操作并行化操作流只需要改变一个方法调用，将 .stream()替换成 .parallelStream()。 思考，并行化运行基于流的代码是否比串行化运行更快？影响因素有很多，比如输入流的大小，假设样本数量为10，那么串行肯定优于并行；样本数量10k，二者速度可能相仿；样本数量千万级或者远远更大，此时，并行优于串行。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:4:3","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#并行化流操作"},{"categories":["java"],"content":"\r模拟系统搭建一个简易的模拟系统来理解摇色子，蒙特卡洛模拟法，它会重复相同的模拟很多次，每次都随机生成种子，结果被记录下来。汇总得到一个对系统的全面模拟。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:4:4","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#模拟系统"},{"categories":["java"],"content":"\r限制流框架会自己处理同步操作，所以要避免给数据结构加锁，否则可能会自找麻烦。 除了parallel方法还有一个叫sequential的方法，它们分别是串行和并行。如果同时调用两个方法，最后调用的有用。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:4:5","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#限制"},{"categories":["java"],"content":"\r性能影响性能的因素： 数据大小，数据足够大，并行处理才有意义。 源数据结构，通常是集合。 装箱，处理基本类比装箱快。 核的数量，核数越多，潜在性能提升越大。 单元处理开销，花在流中每个元素身上的时间越长，并行操作带来的性能提升越明显。（就是单个元素越复杂，并行效果越好）。 在底层，并行流还是沿用了 fork/join框架，fork递归式分解问题，然后每段并行执行，最终由join合并结果。 将数据结构分类： 性能好：ArrayList、数组、IntStream.range，这些数据结构支持随机读取，可以很轻易地被任意分解。 性能一般：HashSer、TreeSet，不易公平地被分解，大多数分解是可能的。 性能差，LinkedList、Streams.iterate、BufferedReader.lines，可能要花费O(N)甚至更高的时间复杂度来分解问题，很难分解。 初始数据结构影响巨大，样本数量足够大的时候，ArrayList比LinkedList快10倍。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:4:6","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#性能"},{"categories":["java"],"content":"\r并行化数组操作对数组的操作都在工具类Arrays中， parallelSetAll，更新数组元素： double[] values = new doulbe[size]; Arrays.parallelSetAll(values, i -\u003e i); parallelPrefix，对时间序列数据做累加 计算简单滑动平均数 double[] sums = Arrays.of(values, values.length); // 复制一份输入数据 Arrays.parallelPrefix(sums, Double::sum); // 将数组元素相加，用sums保留求和结果。比如输入0，1，2，3，4，3.5，计算后的值为 0.0，1.0，3.0，6.0，10.0，13.5 int start = n - 1; return IntStream.range(start, sums.length) .mapToDouble(i -\u003e { double prefix = i == start ? 0 : sums[i - n]; return (sums[i] - prefix) / n; // 使用总和减去窗口起始值，然后再除以n得到平均值 }) .toArray(); ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:4:7","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#并行化数组操作"},{"categories":["java"],"content":"\r测试、调试、重构","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:5:0","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#测试调试重构"},{"categories":["java"],"content":"\r重构候选项\r进进出出、摇摇晃晃在记录日志时，isDebugEnabled()用来检查是否启用调试级别。仔细想想，一段代码不断地查询和操作对象目的就只是为了给该对象设个值。那么在检查状态这部分不应该是个内部状态么，一查询就暴露了。 传统debug Logger logger = new Logger(); if(logger.isDebugEnabled()){ logger.debug(\"Look at this: \" + exoensiveOperation()); } lambda debug Logger logger = new Logger(); logger.debug(() -\u003e \"Look at this: \" + expensiveOperation()); Lambda表达式更好地面向对象编程（OOP），面向对象编程的核心之一就是封装局部状态，比如日志的级别。 传统方式做的不好，isDebugEnabled方法暴露了内部封装。使用Lambda表达式，外面的代码根本不需要检查日志级别。 孤独的覆盖假设在数据库中查找艺术家，每个线程只做一次这种查询： ThreadLocal\u003cAlbum\u003e thisAlbum = new ThreadLocal\u003cAlbum\u003e(){ @Override protected Album initialValue(){ return database.lookupCurrentAlbum(); } }; 这段代码异味是使用继承，其目的只是为了覆盖一个方法。ThreadLocal能创建一个工厂，为每个线程最多产生一个值。确保非线程安全的类在并发环境下安全使用的一种简单方式。 Java8中，可以为工厂方法withInitial传入一个Supplier对象的实例来创建对象： ThreadLocal\u003cAlubm\u003e thisAlbum = ThreadLocal.withInitial(() -\u003e database.lookupCurrentAlbum()); 此代码优于前者是因为： 任何已有Supplier\u003cAlbum\u003e实例不需要重新封装，就可以在此使用，这鼓励了重用和组合。 代码简短，不用花时间在继承的样本代码上。 JVM会稍加在一个类。 同样的东西写两遍Don’t Repeat YourSelf, DRY模式。还有其反面 Write Everything Twice, WET模式。后者代码异味多见于重复的样本代码，难于重构。 什么时候该将WET代码Lambda化？如果有一个整体上大概相似的模式，只是行为上有所不同，就可以试着加入一个Lambda表达式。 Order类的命令式实现 public long countRunningTIme(){ long count = 0; for(Album album : albums){ for(Track track : tracks){ count += track.getLength(); } } return count; } public long countMusicians(){ long count = 0; for(Album album : albums){ count += album.getMusicianList().size(); } return count; } public long countTracks(){ long count = 0; for(Album album : albums){ count += album.getTrackList().size(); } return count; } 三个方法，里面的的代码基本一样。 流重写 public long countRunningTimg(){ return albums.stream() .mapToLong(album -\u003e album.getTracks() .mapToLong(track -\u003e track.getLength()) .sum()) .sum(); } public long countMusicians(){ return albums.stream() .mapToLong(album -\u003e album.getMusicians().count()) .sum(); } public long countTracks(){ return albums.stream() .mapToLong(album -\u003e album.getTracks().count()) .sum(); } 这段代码仍然有重用性问题。可以实现一个函数，返回long，统计所有专辑的某些特性，还需要一个lambda表达式，告诉统计专辑上的信息。java8核心类库提供这样一个类型 ToLongFunction\u003c\u003e。 将专辑转换成流，将专辑映射为long，然后求和。 public long countFeature(ToLongFunction\u003cAlbum\u003e function){ return albums.stream() .mapToLong(function) .sum(); } public long countTracks(){ return countFeature(album -\u003e album.getTracks().count()); } public long countRunningTime(){ return countFeature(album -\u003e album.getTracks() .mapToLong(track -\u003e track.getLength()) .sum()); } public long countMusicians(){ return countFeature(album -\u003e album.getMusicians().count()); } ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:5:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#重构候选项"},{"categories":["java"],"content":"\r重构候选项\r进进出出、摇摇晃晃在记录日志时，isDebugEnabled()用来检查是否启用调试级别。仔细想想，一段代码不断地查询和操作对象目的就只是为了给该对象设个值。那么在检查状态这部分不应该是个内部状态么，一查询就暴露了。 传统debug Logger logger = new Logger(); if(logger.isDebugEnabled()){ logger.debug(\"Look at this: \" + exoensiveOperation()); } lambda debug Logger logger = new Logger(); logger.debug(() -\u003e \"Look at this: \" + expensiveOperation()); Lambda表达式更好地面向对象编程（OOP），面向对象编程的核心之一就是封装局部状态，比如日志的级别。 传统方式做的不好，isDebugEnabled方法暴露了内部封装。使用Lambda表达式，外面的代码根本不需要检查日志级别。 孤独的覆盖假设在数据库中查找艺术家，每个线程只做一次这种查询： ThreadLocal thisAlbum = new ThreadLocal(){ @Override protected Album initialValue(){ return database.lookupCurrentAlbum(); } }; 这段代码异味是使用继承，其目的只是为了覆盖一个方法。ThreadLocal能创建一个工厂，为每个线程最多产生一个值。确保非线程安全的类在并发环境下安全使用的一种简单方式。 Java8中，可以为工厂方法withInitial传入一个Supplier对象的实例来创建对象： ThreadLocal thisAlbum = ThreadLocal.withInitial(() -\u003e database.lookupCurrentAlbum()); 此代码优于前者是因为： 任何已有Supplier实例不需要重新封装，就可以在此使用，这鼓励了重用和组合。 代码简短，不用花时间在继承的样本代码上。 JVM会稍加在一个类。 同样的东西写两遍Don’t Repeat YourSelf, DRY模式。还有其反面 Write Everything Twice, WET模式。后者代码异味多见于重复的样本代码，难于重构。 什么时候该将WET代码Lambda化？如果有一个整体上大概相似的模式，只是行为上有所不同，就可以试着加入一个Lambda表达式。 Order类的命令式实现 public long countRunningTIme(){ long count = 0; for(Album album : albums){ for(Track track : tracks){ count += track.getLength(); } } return count; } public long countMusicians(){ long count = 0; for(Album album : albums){ count += album.getMusicianList().size(); } return count; } public long countTracks(){ long count = 0; for(Album album : albums){ count += album.getTrackList().size(); } return count; } 三个方法，里面的的代码基本一样。 流重写 public long countRunningTimg(){ return albums.stream() .mapToLong(album -\u003e album.getTracks() .mapToLong(track -\u003e track.getLength()) .sum()) .sum(); } public long countMusicians(){ return albums.stream() .mapToLong(album -\u003e album.getMusicians().count()) .sum(); } public long countTracks(){ return albums.stream() .mapToLong(album -\u003e album.getTracks().count()) .sum(); } 这段代码仍然有重用性问题。可以实现一个函数，返回long，统计所有专辑的某些特性，还需要一个lambda表达式，告诉统计专辑上的信息。java8核心类库提供这样一个类型 ToLongFunction\u003c\u003e。 将专辑转换成流，将专辑映射为long，然后求和。 public long countFeature(ToLongFunction function){ return albums.stream() .mapToLong(function) .sum(); } public long countTracks(){ return countFeature(album -\u003e album.getTracks().count()); } public long countRunningTime(){ return countFeature(album -\u003e album.getTracks() .mapToLong(track -\u003e track.getLength()) .sum()); } public long countMusicians(){ return countFeature(album -\u003e album.getMusicians().count()); } ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:5:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#进进出出摇摇晃晃"},{"categories":["java"],"content":"\r重构候选项\r进进出出、摇摇晃晃在记录日志时，isDebugEnabled()用来检查是否启用调试级别。仔细想想，一段代码不断地查询和操作对象目的就只是为了给该对象设个值。那么在检查状态这部分不应该是个内部状态么，一查询就暴露了。 传统debug Logger logger = new Logger(); if(logger.isDebugEnabled()){ logger.debug(\"Look at this: \" + exoensiveOperation()); } lambda debug Logger logger = new Logger(); logger.debug(() -\u003e \"Look at this: \" + expensiveOperation()); Lambda表达式更好地面向对象编程（OOP），面向对象编程的核心之一就是封装局部状态，比如日志的级别。 传统方式做的不好，isDebugEnabled方法暴露了内部封装。使用Lambda表达式，外面的代码根本不需要检查日志级别。 孤独的覆盖假设在数据库中查找艺术家，每个线程只做一次这种查询： ThreadLocal thisAlbum = new ThreadLocal(){ @Override protected Album initialValue(){ return database.lookupCurrentAlbum(); } }; 这段代码异味是使用继承，其目的只是为了覆盖一个方法。ThreadLocal能创建一个工厂，为每个线程最多产生一个值。确保非线程安全的类在并发环境下安全使用的一种简单方式。 Java8中，可以为工厂方法withInitial传入一个Supplier对象的实例来创建对象： ThreadLocal thisAlbum = ThreadLocal.withInitial(() -\u003e database.lookupCurrentAlbum()); 此代码优于前者是因为： 任何已有Supplier实例不需要重新封装，就可以在此使用，这鼓励了重用和组合。 代码简短，不用花时间在继承的样本代码上。 JVM会稍加在一个类。 同样的东西写两遍Don’t Repeat YourSelf, DRY模式。还有其反面 Write Everything Twice, WET模式。后者代码异味多见于重复的样本代码，难于重构。 什么时候该将WET代码Lambda化？如果有一个整体上大概相似的模式，只是行为上有所不同，就可以试着加入一个Lambda表达式。 Order类的命令式实现 public long countRunningTIme(){ long count = 0; for(Album album : albums){ for(Track track : tracks){ count += track.getLength(); } } return count; } public long countMusicians(){ long count = 0; for(Album album : albums){ count += album.getMusicianList().size(); } return count; } public long countTracks(){ long count = 0; for(Album album : albums){ count += album.getTrackList().size(); } return count; } 三个方法，里面的的代码基本一样。 流重写 public long countRunningTimg(){ return albums.stream() .mapToLong(album -\u003e album.getTracks() .mapToLong(track -\u003e track.getLength()) .sum()) .sum(); } public long countMusicians(){ return albums.stream() .mapToLong(album -\u003e album.getMusicians().count()) .sum(); } public long countTracks(){ return albums.stream() .mapToLong(album -\u003e album.getTracks().count()) .sum(); } 这段代码仍然有重用性问题。可以实现一个函数，返回long，统计所有专辑的某些特性，还需要一个lambda表达式，告诉统计专辑上的信息。java8核心类库提供这样一个类型 ToLongFunction\u003c\u003e。 将专辑转换成流，将专辑映射为long，然后求和。 public long countFeature(ToLongFunction function){ return albums.stream() .mapToLong(function) .sum(); } public long countTracks(){ return countFeature(album -\u003e album.getTracks().count()); } public long countRunningTime(){ return countFeature(album -\u003e album.getTracks() .mapToLong(track -\u003e track.getLength()) .sum()); } public long countMusicians(){ return countFeature(album -\u003e album.getMusicians().count()); } ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:5:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#孤独的覆盖"},{"categories":["java"],"content":"\r重构候选项\r进进出出、摇摇晃晃在记录日志时，isDebugEnabled()用来检查是否启用调试级别。仔细想想，一段代码不断地查询和操作对象目的就只是为了给该对象设个值。那么在检查状态这部分不应该是个内部状态么，一查询就暴露了。 传统debug Logger logger = new Logger(); if(logger.isDebugEnabled()){ logger.debug(\"Look at this: \" + exoensiveOperation()); } lambda debug Logger logger = new Logger(); logger.debug(() -\u003e \"Look at this: \" + expensiveOperation()); Lambda表达式更好地面向对象编程（OOP），面向对象编程的核心之一就是封装局部状态，比如日志的级别。 传统方式做的不好，isDebugEnabled方法暴露了内部封装。使用Lambda表达式，外面的代码根本不需要检查日志级别。 孤独的覆盖假设在数据库中查找艺术家，每个线程只做一次这种查询： ThreadLocal thisAlbum = new ThreadLocal(){ @Override protected Album initialValue(){ return database.lookupCurrentAlbum(); } }; 这段代码异味是使用继承，其目的只是为了覆盖一个方法。ThreadLocal能创建一个工厂，为每个线程最多产生一个值。确保非线程安全的类在并发环境下安全使用的一种简单方式。 Java8中，可以为工厂方法withInitial传入一个Supplier对象的实例来创建对象： ThreadLocal thisAlbum = ThreadLocal.withInitial(() -\u003e database.lookupCurrentAlbum()); 此代码优于前者是因为： 任何已有Supplier实例不需要重新封装，就可以在此使用，这鼓励了重用和组合。 代码简短，不用花时间在继承的样本代码上。 JVM会稍加在一个类。 同样的东西写两遍Don’t Repeat YourSelf, DRY模式。还有其反面 Write Everything Twice, WET模式。后者代码异味多见于重复的样本代码，难于重构。 什么时候该将WET代码Lambda化？如果有一个整体上大概相似的模式，只是行为上有所不同，就可以试着加入一个Lambda表达式。 Order类的命令式实现 public long countRunningTIme(){ long count = 0; for(Album album : albums){ for(Track track : tracks){ count += track.getLength(); } } return count; } public long countMusicians(){ long count = 0; for(Album album : albums){ count += album.getMusicianList().size(); } return count; } public long countTracks(){ long count = 0; for(Album album : albums){ count += album.getTrackList().size(); } return count; } 三个方法，里面的的代码基本一样。 流重写 public long countRunningTimg(){ return albums.stream() .mapToLong(album -\u003e album.getTracks() .mapToLong(track -\u003e track.getLength()) .sum()) .sum(); } public long countMusicians(){ return albums.stream() .mapToLong(album -\u003e album.getMusicians().count()) .sum(); } public long countTracks(){ return albums.stream() .mapToLong(album -\u003e album.getTracks().count()) .sum(); } 这段代码仍然有重用性问题。可以实现一个函数，返回long，统计所有专辑的某些特性，还需要一个lambda表达式，告诉统计专辑上的信息。java8核心类库提供这样一个类型 ToLongFunction\u003c\u003e。 将专辑转换成流，将专辑映射为long，然后求和。 public long countFeature(ToLongFunction function){ return albums.stream() .mapToLong(function) .sum(); } public long countTracks(){ return countFeature(album -\u003e album.getTracks().count()); } public long countRunningTime(){ return countFeature(album -\u003e album.getTracks() .mapToLong(track -\u003e track.getLength()) .sum()); } public long countMusicians(){ return countFeature(album -\u003e album.getMusicians().count()); } ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:5:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#同样的东西写两遍"},{"categories":["java"],"content":"\r日志和打印消息之前我们在流中想打印是在 foreach(a -\u003e System.out.println(“例如：” + a)); 这样操作的缺点是，当调试代码时，比如debug，由于流只能使用一次，在某一段打断点，如果还想继续就必须重新创建流。这个就是惰性求值。 解决方案：peek 流中有一个方法peek()可以查看每一个值，同时可以继续操作流。 Set\u003cString\u003e nationalities = album.getMusicians() .filter(artist -\u003e artist.getName().startsWith(\"The\")) .map(artist -\u003e artist.getNationality()) .peek(nation -\u003e System.out,println(\"例如：\" + nation)) .collect(Collectors.\u003cString\u003etoSet()); 记录日志是peek的用途之一，可以设置断点。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:5:2","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#日志和打印消息"},{"categories":["java"],"content":"\r设计和架构的原则探索如何使用Lambda表达式实现SOLID原则。 ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:6:0","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#设计和架构的原则"},{"categories":["java"],"content":"\rLambda表达式改变了设计模式单例模式确保只生产一个对象实例，然而它却让程序变得更脆弱，难于测试。敏捷开发的流行，让测试变得更加重要，因此单例模式变成了反模式，一种要避免的模式。 本章主要讨论如何使用Lambda让设计模式变得更好。 命令者模式命令者是一个对象，它封装了调用另一个方法的所有细节。 客户端 -创建- 发起者，发起者 -调用- 命令者，客户端 -使用- 具体命令者，具体命令者 -实现- 命令者，具体命令者 -调用- 命令接受者。 命令者：封装了所有调用命令执行者的信息。 发起者：控制一个或多个命令的顺序和执行。 客户端：创建具体的命令者实例。 public interface Editor{ public void save(); public void open(); public void close(); } public interface Action{ public void perform(); } public class Save implements Action{ private final Editor editor; public Save(Editor editor){ this.editor = editor; } @Override public void perform(){ editor.save(); } } public class Open implements Action{ private final Editor editor; public Open(Editor editor){ this.editor = editor; } @Override public void perform(){ editor.open(); } } public class Macro{ private final List\u003cAction\u003e actions; public Macro(){ actions = new ArrayList\u003c\u003e(); } public void record(Action action){ actions.add(action); } public void run(){ actions.forEach(Action::perform); } } 不同方式的调用 Macro macro = new Macro(); macro.record(new Open(editor)); ... macro.record(() -\u003e editor.open()); ... macro.record(editor::open); ... 策略模式策略模式能在运行时改变软件的算法行为。一个问题可以由多种算法实现，将它们封装在一个统一的接口背后，选择哪个算法就是策略模式决定的。 文件压缩就是一个例子，它提供各种压缩方式，比如 zip、gzip等。 整体流程就是： 压缩器 调用 压缩策略，具体的压缩算法实现压缩策略。 定义压缩数据的策略借口 public interface CompressionStrategy{ public OutputStream compress(OutputStream data) throws IOException; } 两个类实现该接口，zip和gzip。 使用gzip算法压缩数据 public class GzipCompressionStrategy implements CompressionStrategy{ @Override public OutpuStream compress(OutputStream data) throws IOException{ return new GZIPOutputStream(data); } } 使用zip算法压缩数据 public class ZipCompressionStrategy implements CompressionStrategy{ @Override public OutpuStream compress(OutputStream data) throws IOException{ return new ZIPOutputStream(data); } } 实现Compressor类，这就是使用策略模式的地方。它有一个compress方法，读入文件，压缩后输出。它的构造函数有一个CompressionStrategy参数，调用代码后可以在运行期使用该参数决定使用哪种压缩策略，比如，可以等待用户输入选择。 在构造类时提供压缩策略 public class Compressor{ private final CompressionStrategy strategy; public Compressor(CompressionStrategy strategy){ this.strategy = strategy; } public void compress(Path inFile, File outFile) throws IOException{ try(OutputStream outStream = new FileOutputStream(outFile)){ Files.copy(inFile, strategy.compress(outStream)); } } } 通过这种方式，只需要new一个Compressor就可以使用我们想要的策略。 使用具体策略类初始化Compressor Compressor gzipCom = new Compressor(new GzipCompressionStrategy()); gzipCom.compress(inFile, outFile); Compressor zipCom = new Compressor(new ZipCompressionStrategy()); zipCom.compress(inFile, outFile); Lambda表达式或者方法引用去掉样本代码 Compressor gzipCom = new Compressor(GzipCompressionStrategy::new); gzipCom.compress(inFile, outFile); Compressor zipCom = new Compressor(ZipCompressionStrategy::new); zipCom.compress(inFile, outFile); 观察者模式被观察者持有一个观察者列表，当被观察者的状态发生改变时，会通知观察者。这种模式被大量应用于基于MVC的GUI工具中，以此让模型状态发生变化时，自动刷新视图模块，达到二者解耦。 比如，观察的是月球，NASA和Aliens都对月球感兴趣。 观察登陆到月球组织的接口 public interface LandingObserver{ public void observeLanding(String name); } Moon类 public class Moon{ private final List\u003cLaningObserver\u003e observers = new ArfrayList\u003c\u003e(); public void land(String name){ for(LandingObserver observer : observers){ observer.observeLanding(name); } } public void startSpying(LandingObserver observer){ observers.add(observer); } } 外星人观察到人类登陆月球 public class Aliens implements LandingObserver{ @Override public void observeLanding(String name){ if(name.contains(\"Apollo\")){ System.out.println(\"They're distracted, lets invade earch!\"); } } } NASA观察有人登陆月球 public class Nasa implements LandingObserver{ @Override public void observeLanding(String name){ if(name.contains(\"Apollo\")){ System.out.println(\"We made it\"); } } } 使用类的方式构建 Moon moon = new Moon(); moon.startSpying(new Nasa()); moon.startSpying(new Aliens()); moon.land(\"An asteroid\"); moon.land(\"Apollo 11\"); Lambda方式构建 Moon moon = new Moon(); moon.startSpying(name -\u003e { if (name.contains(\"Apollo\")) System.out.println(\"We made it\"); }); moon.startSpying(name -\u003e { if(name.contains(\"Apollo\")) System.out.println(\"They're distracted, l","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:6:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#lambda表达式改变了设计模式"},{"categories":["java"],"content":"\rLambda表达式改变了设计模式单例模式确保只生产一个对象实例，然而它却让程序变得更脆弱，难于测试。敏捷开发的流行，让测试变得更加重要，因此单例模式变成了反模式，一种要避免的模式。 本章主要讨论如何使用Lambda让设计模式变得更好。 命令者模式命令者是一个对象，它封装了调用另一个方法的所有细节。 客户端 -创建- 发起者，发起者 -调用- 命令者，客户端 -使用- 具体命令者，具体命令者 -实现- 命令者，具体命令者 -调用- 命令接受者。 命令者：封装了所有调用命令执行者的信息。 发起者：控制一个或多个命令的顺序和执行。 客户端：创建具体的命令者实例。 public interface Editor{ public void save(); public void open(); public void close(); } public interface Action{ public void perform(); } public class Save implements Action{ private final Editor editor; public Save(Editor editor){ this.editor = editor; } @Override public void perform(){ editor.save(); } } public class Open implements Action{ private final Editor editor; public Open(Editor editor){ this.editor = editor; } @Override public void perform(){ editor.open(); } } public class Macro{ private final List actions; public Macro(){ actions = new ArrayList\u003c\u003e(); } public void record(Action action){ actions.add(action); } public void run(){ actions.forEach(Action::perform); } } 不同方式的调用 Macro macro = new Macro(); macro.record(new Open(editor)); ... macro.record(() -\u003e editor.open()); ... macro.record(editor::open); ... 策略模式策略模式能在运行时改变软件的算法行为。一个问题可以由多种算法实现，将它们封装在一个统一的接口背后，选择哪个算法就是策略模式决定的。 文件压缩就是一个例子，它提供各种压缩方式，比如 zip、gzip等。 整体流程就是： 压缩器 调用 压缩策略，具体的压缩算法实现压缩策略。 定义压缩数据的策略借口 public interface CompressionStrategy{ public OutputStream compress(OutputStream data) throws IOException; } 两个类实现该接口，zip和gzip。 使用gzip算法压缩数据 public class GzipCompressionStrategy implements CompressionStrategy{ @Override public OutpuStream compress(OutputStream data) throws IOException{ return new GZIPOutputStream(data); } } 使用zip算法压缩数据 public class ZipCompressionStrategy implements CompressionStrategy{ @Override public OutpuStream compress(OutputStream data) throws IOException{ return new ZIPOutputStream(data); } } 实现Compressor类，这就是使用策略模式的地方。它有一个compress方法，读入文件，压缩后输出。它的构造函数有一个CompressionStrategy参数，调用代码后可以在运行期使用该参数决定使用哪种压缩策略，比如，可以等待用户输入选择。 在构造类时提供压缩策略 public class Compressor{ private final CompressionStrategy strategy; public Compressor(CompressionStrategy strategy){ this.strategy = strategy; } public void compress(Path inFile, File outFile) throws IOException{ try(OutputStream outStream = new FileOutputStream(outFile)){ Files.copy(inFile, strategy.compress(outStream)); } } } 通过这种方式，只需要new一个Compressor就可以使用我们想要的策略。 使用具体策略类初始化Compressor Compressor gzipCom = new Compressor(new GzipCompressionStrategy()); gzipCom.compress(inFile, outFile); Compressor zipCom = new Compressor(new ZipCompressionStrategy()); zipCom.compress(inFile, outFile); Lambda表达式或者方法引用去掉样本代码 Compressor gzipCom = new Compressor(GzipCompressionStrategy::new); gzipCom.compress(inFile, outFile); Compressor zipCom = new Compressor(ZipCompressionStrategy::new); zipCom.compress(inFile, outFile); 观察者模式被观察者持有一个观察者列表，当被观察者的状态发生改变时，会通知观察者。这种模式被大量应用于基于MVC的GUI工具中，以此让模型状态发生变化时，自动刷新视图模块，达到二者解耦。 比如，观察的是月球，NASA和Aliens都对月球感兴趣。 观察登陆到月球组织的接口 public interface LandingObserver{ public void observeLanding(String name); } Moon类 public class Moon{ private final List observers = new ArfrayList\u003c\u003e(); public void land(String name){ for(LandingObserver observer : observers){ observer.observeLanding(name); } } public void startSpying(LandingObserver observer){ observers.add(observer); } } 外星人观察到人类登陆月球 public class Aliens implements LandingObserver{ @Override public void observeLanding(String name){ if(name.contains(\"Apollo\")){ System.out.println(\"They're distracted, lets invade earch!\"); } } } NASA观察有人登陆月球 public class Nasa implements LandingObserver{ @Override public void observeLanding(String name){ if(name.contains(\"Apollo\")){ System.out.println(\"We made it\"); } } } 使用类的方式构建 Moon moon = new Moon(); moon.startSpying(new Nasa()); moon.startSpying(new Aliens()); moon.land(\"An asteroid\"); moon.land(\"Apollo 11\"); Lambda方式构建 Moon moon = new Moon(); moon.startSpying(name -\u003e { if (name.contains(\"Apollo\")) System.out.println(\"We made it\"); }); moon.startSpying(name -\u003e { if(name.contains(\"Apollo\")) System.out.println(\"They're distracted, l","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:6:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#命令者模式"},{"categories":["java"],"content":"\rLambda表达式改变了设计模式单例模式确保只生产一个对象实例，然而它却让程序变得更脆弱，难于测试。敏捷开发的流行，让测试变得更加重要，因此单例模式变成了反模式，一种要避免的模式。 本章主要讨论如何使用Lambda让设计模式变得更好。 命令者模式命令者是一个对象，它封装了调用另一个方法的所有细节。 客户端 -创建- 发起者，发起者 -调用- 命令者，客户端 -使用- 具体命令者，具体命令者 -实现- 命令者，具体命令者 -调用- 命令接受者。 命令者：封装了所有调用命令执行者的信息。 发起者：控制一个或多个命令的顺序和执行。 客户端：创建具体的命令者实例。 public interface Editor{ public void save(); public void open(); public void close(); } public interface Action{ public void perform(); } public class Save implements Action{ private final Editor editor; public Save(Editor editor){ this.editor = editor; } @Override public void perform(){ editor.save(); } } public class Open implements Action{ private final Editor editor; public Open(Editor editor){ this.editor = editor; } @Override public void perform(){ editor.open(); } } public class Macro{ private final List actions; public Macro(){ actions = new ArrayList\u003c\u003e(); } public void record(Action action){ actions.add(action); } public void run(){ actions.forEach(Action::perform); } } 不同方式的调用 Macro macro = new Macro(); macro.record(new Open(editor)); ... macro.record(() -\u003e editor.open()); ... macro.record(editor::open); ... 策略模式策略模式能在运行时改变软件的算法行为。一个问题可以由多种算法实现，将它们封装在一个统一的接口背后，选择哪个算法就是策略模式决定的。 文件压缩就是一个例子，它提供各种压缩方式，比如 zip、gzip等。 整体流程就是： 压缩器 调用 压缩策略，具体的压缩算法实现压缩策略。 定义压缩数据的策略借口 public interface CompressionStrategy{ public OutputStream compress(OutputStream data) throws IOException; } 两个类实现该接口，zip和gzip。 使用gzip算法压缩数据 public class GzipCompressionStrategy implements CompressionStrategy{ @Override public OutpuStream compress(OutputStream data) throws IOException{ return new GZIPOutputStream(data); } } 使用zip算法压缩数据 public class ZipCompressionStrategy implements CompressionStrategy{ @Override public OutpuStream compress(OutputStream data) throws IOException{ return new ZIPOutputStream(data); } } 实现Compressor类，这就是使用策略模式的地方。它有一个compress方法，读入文件，压缩后输出。它的构造函数有一个CompressionStrategy参数，调用代码后可以在运行期使用该参数决定使用哪种压缩策略，比如，可以等待用户输入选择。 在构造类时提供压缩策略 public class Compressor{ private final CompressionStrategy strategy; public Compressor(CompressionStrategy strategy){ this.strategy = strategy; } public void compress(Path inFile, File outFile) throws IOException{ try(OutputStream outStream = new FileOutputStream(outFile)){ Files.copy(inFile, strategy.compress(outStream)); } } } 通过这种方式，只需要new一个Compressor就可以使用我们想要的策略。 使用具体策略类初始化Compressor Compressor gzipCom = new Compressor(new GzipCompressionStrategy()); gzipCom.compress(inFile, outFile); Compressor zipCom = new Compressor(new ZipCompressionStrategy()); zipCom.compress(inFile, outFile); Lambda表达式或者方法引用去掉样本代码 Compressor gzipCom = new Compressor(GzipCompressionStrategy::new); gzipCom.compress(inFile, outFile); Compressor zipCom = new Compressor(ZipCompressionStrategy::new); zipCom.compress(inFile, outFile); 观察者模式被观察者持有一个观察者列表，当被观察者的状态发生改变时，会通知观察者。这种模式被大量应用于基于MVC的GUI工具中，以此让模型状态发生变化时，自动刷新视图模块，达到二者解耦。 比如，观察的是月球，NASA和Aliens都对月球感兴趣。 观察登陆到月球组织的接口 public interface LandingObserver{ public void observeLanding(String name); } Moon类 public class Moon{ private final List observers = new ArfrayList\u003c\u003e(); public void land(String name){ for(LandingObserver observer : observers){ observer.observeLanding(name); } } public void startSpying(LandingObserver observer){ observers.add(observer); } } 外星人观察到人类登陆月球 public class Aliens implements LandingObserver{ @Override public void observeLanding(String name){ if(name.contains(\"Apollo\")){ System.out.println(\"They're distracted, lets invade earch!\"); } } } NASA观察有人登陆月球 public class Nasa implements LandingObserver{ @Override public void observeLanding(String name){ if(name.contains(\"Apollo\")){ System.out.println(\"We made it\"); } } } 使用类的方式构建 Moon moon = new Moon(); moon.startSpying(new Nasa()); moon.startSpying(new Aliens()); moon.land(\"An asteroid\"); moon.land(\"Apollo 11\"); Lambda方式构建 Moon moon = new Moon(); moon.startSpying(name -\u003e { if (name.contains(\"Apollo\")) System.out.println(\"We made it\"); }); moon.startSpying(name -\u003e { if(name.contains(\"Apollo\")) System.out.println(\"They're distracted, l","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:6:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#策略模式"},{"categories":["java"],"content":"\rLambda表达式改变了设计模式单例模式确保只生产一个对象实例，然而它却让程序变得更脆弱，难于测试。敏捷开发的流行，让测试变得更加重要，因此单例模式变成了反模式，一种要避免的模式。 本章主要讨论如何使用Lambda让设计模式变得更好。 命令者模式命令者是一个对象，它封装了调用另一个方法的所有细节。 客户端 -创建- 发起者，发起者 -调用- 命令者，客户端 -使用- 具体命令者，具体命令者 -实现- 命令者，具体命令者 -调用- 命令接受者。 命令者：封装了所有调用命令执行者的信息。 发起者：控制一个或多个命令的顺序和执行。 客户端：创建具体的命令者实例。 public interface Editor{ public void save(); public void open(); public void close(); } public interface Action{ public void perform(); } public class Save implements Action{ private final Editor editor; public Save(Editor editor){ this.editor = editor; } @Override public void perform(){ editor.save(); } } public class Open implements Action{ private final Editor editor; public Open(Editor editor){ this.editor = editor; } @Override public void perform(){ editor.open(); } } public class Macro{ private final List actions; public Macro(){ actions = new ArrayList\u003c\u003e(); } public void record(Action action){ actions.add(action); } public void run(){ actions.forEach(Action::perform); } } 不同方式的调用 Macro macro = new Macro(); macro.record(new Open(editor)); ... macro.record(() -\u003e editor.open()); ... macro.record(editor::open); ... 策略模式策略模式能在运行时改变软件的算法行为。一个问题可以由多种算法实现，将它们封装在一个统一的接口背后，选择哪个算法就是策略模式决定的。 文件压缩就是一个例子，它提供各种压缩方式，比如 zip、gzip等。 整体流程就是： 压缩器 调用 压缩策略，具体的压缩算法实现压缩策略。 定义压缩数据的策略借口 public interface CompressionStrategy{ public OutputStream compress(OutputStream data) throws IOException; } 两个类实现该接口，zip和gzip。 使用gzip算法压缩数据 public class GzipCompressionStrategy implements CompressionStrategy{ @Override public OutpuStream compress(OutputStream data) throws IOException{ return new GZIPOutputStream(data); } } 使用zip算法压缩数据 public class ZipCompressionStrategy implements CompressionStrategy{ @Override public OutpuStream compress(OutputStream data) throws IOException{ return new ZIPOutputStream(data); } } 实现Compressor类，这就是使用策略模式的地方。它有一个compress方法，读入文件，压缩后输出。它的构造函数有一个CompressionStrategy参数，调用代码后可以在运行期使用该参数决定使用哪种压缩策略，比如，可以等待用户输入选择。 在构造类时提供压缩策略 public class Compressor{ private final CompressionStrategy strategy; public Compressor(CompressionStrategy strategy){ this.strategy = strategy; } public void compress(Path inFile, File outFile) throws IOException{ try(OutputStream outStream = new FileOutputStream(outFile)){ Files.copy(inFile, strategy.compress(outStream)); } } } 通过这种方式，只需要new一个Compressor就可以使用我们想要的策略。 使用具体策略类初始化Compressor Compressor gzipCom = new Compressor(new GzipCompressionStrategy()); gzipCom.compress(inFile, outFile); Compressor zipCom = new Compressor(new ZipCompressionStrategy()); zipCom.compress(inFile, outFile); Lambda表达式或者方法引用去掉样本代码 Compressor gzipCom = new Compressor(GzipCompressionStrategy::new); gzipCom.compress(inFile, outFile); Compressor zipCom = new Compressor(ZipCompressionStrategy::new); zipCom.compress(inFile, outFile); 观察者模式被观察者持有一个观察者列表，当被观察者的状态发生改变时，会通知观察者。这种模式被大量应用于基于MVC的GUI工具中，以此让模型状态发生变化时，自动刷新视图模块，达到二者解耦。 比如，观察的是月球，NASA和Aliens都对月球感兴趣。 观察登陆到月球组织的接口 public interface LandingObserver{ public void observeLanding(String name); } Moon类 public class Moon{ private final List observers = new ArfrayList\u003c\u003e(); public void land(String name){ for(LandingObserver observer : observers){ observer.observeLanding(name); } } public void startSpying(LandingObserver observer){ observers.add(observer); } } 外星人观察到人类登陆月球 public class Aliens implements LandingObserver{ @Override public void observeLanding(String name){ if(name.contains(\"Apollo\")){ System.out.println(\"They're distracted, lets invade earch!\"); } } } NASA观察有人登陆月球 public class Nasa implements LandingObserver{ @Override public void observeLanding(String name){ if(name.contains(\"Apollo\")){ System.out.println(\"We made it\"); } } } 使用类的方式构建 Moon moon = new Moon(); moon.startSpying(new Nasa()); moon.startSpying(new Aliens()); moon.land(\"An asteroid\"); moon.land(\"Apollo 11\"); Lambda方式构建 Moon moon = new Moon(); moon.startSpying(name -\u003e { if (name.contains(\"Apollo\")) System.out.println(\"We made it\"); }); moon.startSpying(name -\u003e { if(name.contains(\"Apollo\")) System.out.println(\"They're distracted, l","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:6:1","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#观察者模式"},{"categories":["java"],"content":"\rNew相关创建一个List/Stream: List.of(1,2,3,4) Stream.of(1,2,3,4,5) ","date":"2023-09-21","objectID":"/java-stream%E6%93%8D%E4%BD%9C/:7:0","series":["java-stream"],"tags":["java","stream","lambda表达式"],"title":"了解Java Stream流操作","uri":"/java-stream%E6%93%8D%E4%BD%9C/#new相关"},{"categories":["design-patterns"],"content":"\r创建型模式 用于描述\"怎么创建对象\"，特点是将对象的创建和使用分离。 如：单例、原型、工厂方法、抽象工厂、建造者等。 ","date":"2023-08-07","objectID":"/singleton-pattern/:1:0","series":["design patterns for java"],"tags":["design-patterns","java"],"title":"Singleton Pattern for Java","uri":"/singleton-pattern/#创建型模式"},{"categories":["design-patterns"],"content":"\rSingleton Pattern涉及到一个单一的类，负责创建自己的的对象，同时保证只有单个对象被创建，并提供一个访问其唯一对象的方式来访问，而不需要被额外实例化。 ","date":"2023-08-07","objectID":"/singleton-pattern/:2:0","series":["design patterns for java"],"tags":["design-patterns","java"],"title":"Singleton Pattern for Java","uri":"/singleton-pattern/#singleton-pattern"},{"categories":["design-patterns"],"content":"\r单例模式的结构 单例类：只创建一个实例的类。 访问类：使用单例类。 ","date":"2023-08-07","objectID":"/singleton-pattern/:2:1","series":["design patterns for java"],"tags":["design-patterns","java"],"title":"Singleton Pattern for Java","uri":"/singleton-pattern/#单例模式的结构"},{"categories":["design-patterns"],"content":"\r1.1.2 单例模式的实现分类两种： 饿汉式 懒汉式 饿汉式 饿汉方式1：静态变量方式 public class Singleton { //私有构造方法 private Singleton() {} ​ //在成员位置创建该类的对象 private static Singleton instance = new Singleton(); ​ //对外提供静态方法获取该对象 public static Singleton getInstance() { return instance; } } 在成员位置声明 Singleton类型的静态变量，并创建Singleton类的对象instance。instance对象是随着类的加载而创建的。 弊端：如果长期不使用就会造成内存浪费。 饿汉方式2：静态代码块 public class Singleton { ​ //私有构造方法 private Singleton() {} ​ //在成员位置创建该类的对象 private static Singleton instance; ​ static { instance = new Singleton(); } ​ //对外提供静态方法获取该对象 public static Singleton getInstance() { return instance; } } 饿汉方式3：枚举 public enum Singleton { INSTANCE; } 枚举类型时线程安全的，并且只会装载一次。另外，其写法非常简单，是所有单例实现中唯一不会被破坏的单例实现模式。 在成员位置声明Singleton类型的静态变量，而对象的创建是在静态代码块中，同样也是随着类的加载而创建。因此弊端同方式1一样。 懒汉式 懒汉方式1：线程不安全 public class Singleton { //私有构造方法 private Singleton() {} ​ //在成员位置创建该类的对象 private static Singleton instance; ​ //对外提供静态方法获取该对象 public static Singleton getInstance() { ​ if(instance == null) { instance = new Singleton(); } return instance; } } 在成员位置声明Singletonde类型的静态变量，在调用getInstance()时，Singleton类的对象为null时才创建，实现了懒加载效果。在多线程环境下，有线程安全问题。 懒汉方式2：线程安全 public class Singleton { //私有构造方法 private Singleton() {} ​ //在成员位置创建该类的对象 private static Singleton instance; ​ //对外提供静态方法获取该对象 public static synchronized Singleton getInstance() { ​ if(instance == null) { instance = new Singleton(); } return instance; } } 在getInstance()加了一个同步关键字synchonized，虽然解决安全问题，但是执行效率低（为了第一次的初始化线程安全问题而牺牲性能）。 懒汉方式3：双重检查锁 public class Singleton { ​ //私有构造方法 private Singleton() {} ​ private static Singleton instance; ​ //对外提供静态方法获取该对象 public static Singleton getInstance() { //第一次判断，如果instance不为null，不进入抢锁阶段，直接返回实例 if(instance == null) { synchronized (Singleton.class) { //抢到锁之后再次判断是否为null if(instance == null) { instance = new Singleton(); } } } return instance; } } 解决了性能问题，又保证了线程安全。但在多线程下，可能会出现空指针问题（因为JVM在实例化对象时会进行优化和指令重排操作）。 可以通过volatile关键字来解决双重锁带来的空指针异常问题，volatile可以保证可见性和有序性。 public class Singleton { ​ //私有构造方法 private Singleton() {} ​ private static volatile Singleton instance; ​ //对外提供静态方法获取该对象 public static Singleton getInstance() { //第一次判断，如果instance不为null，不进入抢锁阶段，直接返回实际 if(instance == null) { synchronized (Singleton.class) { //抢到锁之后再次判断是否为空 if(instance == null) { instance = new Singleton(); } } } return instance; } } 懒汉方式4：静态内部类 public class Singleton { ​ //私有构造方法 private Singleton() {} ​ private static class SingletonHolder { private static final Singleton INSTANCE = new Singleton(); } ​ //对外提供静态方法获取该对象 public static Singleton getInstance() { return SingletonHolder.INSTANCE; } } 第一次加载Singleton类时不会去初始化INSTANCE，只有第一次调用getInstance，虚拟机加载SingletonHolder并初始化INSTANCE，这样不仅确保线程安全，也保证Singleton类的唯一性。 注：开源项目中比较常用的一种单例模式。在没有加任何锁的情况下，保证了多线程下的安全，并且没有任何性能影响和空间的浪费。 ","date":"2023-08-07","objectID":"/singleton-pattern/:2:2","series":["design patterns for java"],"tags":["design-patterns","java"],"title":"Singleton Pattern for Java","uri":"/singleton-pattern/#112-单例模式的实现"},{"categories":["design-patterns"],"content":"\r1.1.2 单例模式的实现分类两种： 饿汉式 懒汉式 饿汉式 饿汉方式1：静态变量方式 public class Singleton { //私有构造方法 private Singleton() {} ​ //在成员位置创建该类的对象 private static Singleton instance = new Singleton(); ​ //对外提供静态方法获取该对象 public static Singleton getInstance() { return instance; } } 在成员位置声明 Singleton类型的静态变量，并创建Singleton类的对象instance。instance对象是随着类的加载而创建的。 弊端：如果长期不使用就会造成内存浪费。 饿汉方式2：静态代码块 public class Singleton { ​ //私有构造方法 private Singleton() {} ​ //在成员位置创建该类的对象 private static Singleton instance; ​ static { instance = new Singleton(); } ​ //对外提供静态方法获取该对象 public static Singleton getInstance() { return instance; } } 饿汉方式3：枚举 public enum Singleton { INSTANCE; } 枚举类型时线程安全的，并且只会装载一次。另外，其写法非常简单，是所有单例实现中唯一不会被破坏的单例实现模式。 在成员位置声明Singleton类型的静态变量，而对象的创建是在静态代码块中，同样也是随着类的加载而创建。因此弊端同方式1一样。 懒汉式 懒汉方式1：线程不安全 public class Singleton { //私有构造方法 private Singleton() {} ​ //在成员位置创建该类的对象 private static Singleton instance; ​ //对外提供静态方法获取该对象 public static Singleton getInstance() { ​ if(instance == null) { instance = new Singleton(); } return instance; } } 在成员位置声明Singletonde类型的静态变量，在调用getInstance()时，Singleton类的对象为null时才创建，实现了懒加载效果。在多线程环境下，有线程安全问题。 懒汉方式2：线程安全 public class Singleton { //私有构造方法 private Singleton() {} ​ //在成员位置创建该类的对象 private static Singleton instance; ​ //对外提供静态方法获取该对象 public static synchronized Singleton getInstance() { ​ if(instance == null) { instance = new Singleton(); } return instance; } } 在getInstance()加了一个同步关键字synchonized，虽然解决安全问题，但是执行效率低（为了第一次的初始化线程安全问题而牺牲性能）。 懒汉方式3：双重检查锁 public class Singleton { ​ //私有构造方法 private Singleton() {} ​ private static Singleton instance; ​ //对外提供静态方法获取该对象 public static Singleton getInstance() { //第一次判断，如果instance不为null，不进入抢锁阶段，直接返回实例 if(instance == null) { synchronized (Singleton.class) { //抢到锁之后再次判断是否为null if(instance == null) { instance = new Singleton(); } } } return instance; } } 解决了性能问题，又保证了线程安全。但在多线程下，可能会出现空指针问题（因为JVM在实例化对象时会进行优化和指令重排操作）。 可以通过volatile关键字来解决双重锁带来的空指针异常问题，volatile可以保证可见性和有序性。 public class Singleton { ​ //私有构造方法 private Singleton() {} ​ private static volatile Singleton instance; ​ //对外提供静态方法获取该对象 public static Singleton getInstance() { //第一次判断，如果instance不为null，不进入抢锁阶段，直接返回实际 if(instance == null) { synchronized (Singleton.class) { //抢到锁之后再次判断是否为空 if(instance == null) { instance = new Singleton(); } } } return instance; } } 懒汉方式4：静态内部类 public class Singleton { ​ //私有构造方法 private Singleton() {} ​ private static class SingletonHolder { private static final Singleton INSTANCE = new Singleton(); } ​ //对外提供静态方法获取该对象 public static Singleton getInstance() { return SingletonHolder.INSTANCE; } } 第一次加载Singleton类时不会去初始化INSTANCE，只有第一次调用getInstance，虚拟机加载SingletonHolder并初始化INSTANCE，这样不仅确保线程安全，也保证Singleton类的唯一性。 注：开源项目中比较常用的一种单例模式。在没有加任何锁的情况下，保证了多线程下的安全，并且没有任何性能影响和空间的浪费。 ","date":"2023-08-07","objectID":"/singleton-pattern/:2:2","series":["design patterns for java"],"tags":["design-patterns","java"],"title":"Singleton Pattern for Java","uri":"/singleton-pattern/#饿汉式"},{"categories":["design-patterns"],"content":"\r1.1.2 单例模式的实现分类两种： 饿汉式 懒汉式 饿汉式 饿汉方式1：静态变量方式 public class Singleton { //私有构造方法 private Singleton() {} ​ //在成员位置创建该类的对象 private static Singleton instance = new Singleton(); ​ //对外提供静态方法获取该对象 public static Singleton getInstance() { return instance; } } 在成员位置声明 Singleton类型的静态变量，并创建Singleton类的对象instance。instance对象是随着类的加载而创建的。 弊端：如果长期不使用就会造成内存浪费。 饿汉方式2：静态代码块 public class Singleton { ​ //私有构造方法 private Singleton() {} ​ //在成员位置创建该类的对象 private static Singleton instance; ​ static { instance = new Singleton(); } ​ //对外提供静态方法获取该对象 public static Singleton getInstance() { return instance; } } 饿汉方式3：枚举 public enum Singleton { INSTANCE; } 枚举类型时线程安全的，并且只会装载一次。另外，其写法非常简单，是所有单例实现中唯一不会被破坏的单例实现模式。 在成员位置声明Singleton类型的静态变量，而对象的创建是在静态代码块中，同样也是随着类的加载而创建。因此弊端同方式1一样。 懒汉式 懒汉方式1：线程不安全 public class Singleton { //私有构造方法 private Singleton() {} ​ //在成员位置创建该类的对象 private static Singleton instance; ​ //对外提供静态方法获取该对象 public static Singleton getInstance() { ​ if(instance == null) { instance = new Singleton(); } return instance; } } 在成员位置声明Singletonde类型的静态变量，在调用getInstance()时，Singleton类的对象为null时才创建，实现了懒加载效果。在多线程环境下，有线程安全问题。 懒汉方式2：线程安全 public class Singleton { //私有构造方法 private Singleton() {} ​ //在成员位置创建该类的对象 private static Singleton instance; ​ //对外提供静态方法获取该对象 public static synchronized Singleton getInstance() { ​ if(instance == null) { instance = new Singleton(); } return instance; } } 在getInstance()加了一个同步关键字synchonized，虽然解决安全问题，但是执行效率低（为了第一次的初始化线程安全问题而牺牲性能）。 懒汉方式3：双重检查锁 public class Singleton { ​ //私有构造方法 private Singleton() {} ​ private static Singleton instance; ​ //对外提供静态方法获取该对象 public static Singleton getInstance() { //第一次判断，如果instance不为null，不进入抢锁阶段，直接返回实例 if(instance == null) { synchronized (Singleton.class) { //抢到锁之后再次判断是否为null if(instance == null) { instance = new Singleton(); } } } return instance; } } 解决了性能问题，又保证了线程安全。但在多线程下，可能会出现空指针问题（因为JVM在实例化对象时会进行优化和指令重排操作）。 可以通过volatile关键字来解决双重锁带来的空指针异常问题，volatile可以保证可见性和有序性。 public class Singleton { ​ //私有构造方法 private Singleton() {} ​ private static volatile Singleton instance; ​ //对外提供静态方法获取该对象 public static Singleton getInstance() { //第一次判断，如果instance不为null，不进入抢锁阶段，直接返回实际 if(instance == null) { synchronized (Singleton.class) { //抢到锁之后再次判断是否为空 if(instance == null) { instance = new Singleton(); } } } return instance; } } 懒汉方式4：静态内部类 public class Singleton { ​ //私有构造方法 private Singleton() {} ​ private static class SingletonHolder { private static final Singleton INSTANCE = new Singleton(); } ​ //对外提供静态方法获取该对象 public static Singleton getInstance() { return SingletonHolder.INSTANCE; } } 第一次加载Singleton类时不会去初始化INSTANCE，只有第一次调用getInstance，虚拟机加载SingletonHolder并初始化INSTANCE，这样不仅确保线程安全，也保证Singleton类的唯一性。 注：开源项目中比较常用的一种单例模式。在没有加任何锁的情况下，保证了多线程下的安全，并且没有任何性能影响和空间的浪费。 ","date":"2023-08-07","objectID":"/singleton-pattern/:2:2","series":["design patterns for java"],"tags":["design-patterns","java"],"title":"Singleton Pattern for Java","uri":"/singleton-pattern/#懒汉式"},{"categories":["mq"],"content":"\r1. 概述例子： 日常： 前端埋点记录用户的行为数据（浏览、点赞、收藏、评论等） 。 将上述存储到日志中，通过Flume对日志进行实时监控并传送给Hadoop。 双11： Flume采集的速度跟不上流量速度，因此需要在log和hadoop中间加一个Kafka集群来进行处理（缓冲）。 ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:1:0","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#1-概述"},{"categories":["mq"],"content":"\r定义传统定义：分布式的基于发布/订阅模式的消息队列（Message Queue），主要应用与大数据实时处理领域。 其中，发布/订阅：消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息分为不同的类别、订阅者只接收感兴趣的消息。 最新定义：开源的分布式事件流平台（Event Streaming Platform），主要被用于高性能数据管道、流分析、数据采集和关键任务应用。 ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:1:1","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#定义"},{"categories":["mq"],"content":"\r技术场景Kafka主要应用于大数据场景，JavaEE中主要采用ActiveMQ、RabbitMQ、RocketMQ。 ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:1:2","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#技术场景"},{"categories":["mq"],"content":"\r应用场景缓存/消峰（双11商城）、解耦（数据源传到多个目的地）和异步通信（不需要立即响应和处理）。 异步通信和同步通信对比： 同步：1. 填写注册信息；2.注册信息写入到数据库；3. 调用短信发送接口；4. 发送短信；5. 页面响应注册成功。 异步：1. 填写注册信息；2.注册信息写入到数据库；3. 发送短信请求写入到消息队列，通过短信服务订阅消息队列来接收消息然后发送短信；4.页面响应注册成功（此步骤发生在数据库写入成功后和3步骤属于并行非串行）。 注：发送短信只是告诉用户注册成功，并不是用来做验证码验证。 ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:1:3","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#应用场景"},{"categories":["mq"],"content":"\r消息队列的两种模式点对点模式：producer -\u003e mq -\u003e consumer（1对1） producer发送信息到mq，consumer主动到mq消费信息，并给mq发送一个确认信号，mq会将确认收到的消息进行删除。 发布/订阅模式：producer -\u003emq’s topics -\u003e consumers producer发布不同的消息到mq中，mq中的消息根据topic不同而分类，consumers订阅自己感兴趣的topic，不需要确认收到，mq也不会立即删除（consumer互相独立）。 ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:1:4","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#消息队列的两种模式"},{"categories":["mq"],"content":"\r基础架构 在一个topic中，它会把数据分割为多个 partition，每个partition存储在不同服务器中，而每个服务器中的kafka被称为broker（分区概念）。 比如，producer在一个topic中的数据有90T，在一个服务器中存不下，就切割为3分，partition0、1、2，分别存储到3个服务器broker0、1、2中。而这三个组成了Kafka cluster(集群)。 同样地，海量数据很难被一个consumer消费，因此在consumer端会有group概念，一个group下有多个consumer，它们分别处理同一个topic下不同partition的数据（消费者组概念，并行消费）。 注：一个partition在同group中只能被一个consumer消费。 为了提高可用性（防止某个partition挂掉），为每个parition增加若干副本， 但是副本间会有leader和follower，producer和consumer只会对leader进行生产和消费。只有当leader挂掉后，follower中会有一个称为新的leader。 另外，kafka还会有一部分数据是存储到zookeeper中， 它存储着kafka哪些服务器正在工作（上线） /brokers/ids/[0,1,2]； 还会记录每一个分区中的leader相关信息 /brokers/topics/first/partitions/0/state/“leader”:0,“isr”:[0,2]。 注意：4中的使用在kafka2.8.0是强制要求，后续版本配置可以不采用zookeeper。不采用zookeeper的模式叫做 kraft（去zookeeper化，大势所趋）。 ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:1:5","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#基础架构"},{"categories":["mq"],"content":"\r2. 入门","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:2:0","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#2-入门"},{"categories":["mq"],"content":"\r2.1 安装部署\r2.1.1 集群规划 hadoop102 hadoop103 hadoop104 zk zk zk kafka kafka kafka 2.1.2 安装下载地址：https://kafka.apache.org/downloads 解压： tar -zxvf kafka-3.6.0-src.tgz -C /opt/module/ 重命名： mv kafka-3.6.0-src/ kafka 其中，主要目录： bin，启动目录 kafka-consle-consumer.sh kafka-consle-producer.sh kafka-server-start.sh kafka-server-stop.sh config，配置目录 consumer.properties producer.properties server.properties server.properties ############################# Server Basics ############################# # The id of the broker. This must be set to a unique integer for each broker. # 配置服务器id，每个id要是唯一的，因此要修改 broker.id=0 ############################# Socket Server Settings ############################# # 具体看源文件 ############################# Log Basics ############################# # A comma separated list of directories under which to store log files # 存放数据的地址，不能是临时地址，会有被清除的风险 # 默认配置 log.dirs=/tmp/kafka-logs log.dirs=/opt/module/kafka/data ############################# Zookeeper ############################# # Zookeeper connection string (see zookeeper docs for details). # This is a comma separated host:port pairs, each corresponding to a zk # server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\". # You can also append an optional chroot string to the urls to specify the # root directory for all kafka znodes. # 默认配置 zookeeper.connect=localhost:2181 # 由于我们是集群，因此只配置本地是不行的 zookeeper.connect=kaka01:2181,kafka02:2181,kafka03:2181/kafka # 其中hadoop10*是已经在hosts配置好的地址 此外，我们还要安装zookeeper 官网：https://zookeeper.apache.org/ 启动命令：bin/kafka-server-start.sh -daemon config/server.properties （kafka目录下执行） 查看是否启动成功：jps ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:2:1","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#21-安装部署"},{"categories":["mq"],"content":"\r2.1 安装部署\r2.1.1 集群规划 hadoop102 hadoop103 hadoop104 zk zk zk kafka kafka kafka 2.1.2 安装下载地址：https://kafka.apache.org/downloads 解压： tar -zxvf kafka-3.6.0-src.tgz -C /opt/module/ 重命名： mv kafka-3.6.0-src/ kafka 其中，主要目录： bin，启动目录 kafka-consle-consumer.sh kafka-consle-producer.sh kafka-server-start.sh kafka-server-stop.sh config，配置目录 consumer.properties producer.properties server.properties server.properties ############################# Server Basics ############################# # The id of the broker. This must be set to a unique integer for each broker. # 配置服务器id，每个id要是唯一的，因此要修改 broker.id=0 ############################# Socket Server Settings ############################# # 具体看源文件 ############################# Log Basics ############################# # A comma separated list of directories under which to store log files # 存放数据的地址，不能是临时地址，会有被清除的风险 # 默认配置 log.dirs=/tmp/kafka-logs log.dirs=/opt/module/kafka/data ############################# Zookeeper ############################# # Zookeeper connection string (see zookeeper docs for details). # This is a comma separated host:port pairs, each corresponding to a zk # server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\". # You can also append an optional chroot string to the urls to specify the # root directory for all kafka znodes. # 默认配置 zookeeper.connect=localhost:2181 # 由于我们是集群，因此只配置本地是不行的 zookeeper.connect=kaka01:2181,kafka02:2181,kafka03:2181/kafka # 其中hadoop10*是已经在hosts配置好的地址 此外，我们还要安装zookeeper 官网：https://zookeeper.apache.org/ 启动命令：bin/kafka-server-start.sh -daemon config/server.properties （kafka目录下执行） 查看是否启动成功：jps ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:2:1","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#211-集群规划"},{"categories":["mq"],"content":"\r2.1 安装部署\r2.1.1 集群规划 hadoop102 hadoop103 hadoop104 zk zk zk kafka kafka kafka 2.1.2 安装下载地址：https://kafka.apache.org/downloads 解压： tar -zxvf kafka-3.6.0-src.tgz -C /opt/module/ 重命名： mv kafka-3.6.0-src/ kafka 其中，主要目录： bin，启动目录 kafka-consle-consumer.sh kafka-consle-producer.sh kafka-server-start.sh kafka-server-stop.sh config，配置目录 consumer.properties producer.properties server.properties server.properties ############################# Server Basics ############################# # The id of the broker. This must be set to a unique integer for each broker. # 配置服务器id，每个id要是唯一的，因此要修改 broker.id=0 ############################# Socket Server Settings ############################# # 具体看源文件 ############################# Log Basics ############################# # A comma separated list of directories under which to store log files # 存放数据的地址，不能是临时地址，会有被清除的风险 # 默认配置 log.dirs=/tmp/kafka-logs log.dirs=/opt/module/kafka/data ############################# Zookeeper ############################# # Zookeeper connection string (see zookeeper docs for details). # This is a comma separated host:port pairs, each corresponding to a zk # server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\". # You can also append an optional chroot string to the urls to specify the # root directory for all kafka znodes. # 默认配置 zookeeper.connect=localhost:2181 # 由于我们是集群，因此只配置本地是不行的 zookeeper.connect=kaka01:2181,kafka02:2181,kafka03:2181/kafka # 其中hadoop10*是已经在hosts配置好的地址 此外，我们还要安装zookeeper 官网：https://zookeeper.apache.org/ 启动命令：bin/kafka-server-start.sh -daemon config/server.properties （kafka目录下执行） 查看是否启动成功：jps ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:2:1","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#212-安装"},{"categories":["mq"],"content":"\r集群启动脚本 #!/bin/bash case $1 in \"start\") { for i in kafka01 kafka02 kafka03 do echo ------------------------------------ kafka $i started --------------------------------------- ssh $i \"/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties\" done } ;; \"stop\") { for i in kafka01 kafka02 kafka03 do echo ------------------------------------ kafka $i stoped --------------------------------------- ssh $i \"/opt/module/kafka/bin/kafka-server-stop.sh stop\" done } ;; esac 注：start命令失效，stop命令可用 ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:2:2","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#集群启动脚本"},{"categories":["mq"],"content":"\rTopic命令kafka分为 producer、topic、consumer，对应的脚本分别为： kafka-console-producer.sh kafka-topics.sh kafka-console-consumer.sh 其中，有关topic的，我们执行 bin/kafka-topics.sh查看 --alter 更改分区数和副本分配 --at-min-isr-partitions 仅显示 ISR 计数等于配置的最小值的分区 --bootstrap-server \u003cString: server to REQUIRED: 连接到kafka broker主机名称和端口号 connect to\u003e --command-config \u003cString: command 仅与--bootstrap-server一起使用，传递给Admin config property file\u003e Client用于描述和更改代理配置 --config \u003cString: name=value\u003e 更新系统默认配置 --create Create a new topic. --delete Delete a topic --delete-config \u003cString: name\u003e --describe List details for the given topics. --exclude-internal 运行 list 或 describe 命令时排除内部主题 --help Print usage information. --if-exists --if-not-exists --list List all available topics. --partitions \u003cInteger: # of partitions\u003e 设置分区数 --replica-assignment \u003cString: A list of manual partition-to-broker broker_id_for_part1_replica1 : assignments for the topic being broker_id_for_part1_replica2 , created or altered. broker_id_for_part2_replica1 : broker_id_for_part2_replica2 , ...\u003e --replication-factor \u003cInteger: 设置分区副本 replication factor\u003e --topic \u003cString: topic\u003e --topic-id \u003cString: topic-id\u003e --topics-with-overrides --unavailable-partitions --under-min-isr-partitions --under-replicated-partitions --version 查看连接的topic节点：bin/kafka-topics.sh –bootstrap-server kafka01:9092,kafka02:9092 –list (个人环境下，一个即可，即只需要kafka01:9092) 创建topic节点：bin/kafka-topics.sh –bootstrap-server kafka01:9092 –topic first –create –partitions 1 –replication-factor 3 总的来说需要4步： 指定连接的服务器名称和端口号，–bootstrap-server 指定 topic（主题），–topic first 创建，–create 指定 paritions（分区），–paritions 1 指定 replication-factor（分区副本），–replication-factor 3 创建成功显示：Created topic first. 查看节点：bin/kafka-topics.sh –bootstrap-server kafka01:9092 –list 查看节点详情：bin/kafka-topics.sh –bootstrap-server kafka01:9092 –topic first –describe 修改分区：bin/kafka-topics.sh –bootstrap-server kafka01:9092 –topic first –alter –partitions 3 注意，分区数的修改只能增加，不能减少。 验证就是查看节点详情 修改分区备份：不能在命令行上修改 ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:2:3","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#topic命令"},{"categories":["mq"],"content":"\r生产者和生产者命令展示 生产者发送数据命令：bin/kafka-consle-prodcer.sh –bootstrap-server kafka01:9092 –topic first，输入命令后就可以发送消息。 消费者消费数据命令：bin/kafka-consle-consumer.sh –bootstrap-server kafka01:9092 –topic first，该命令可以消费到的是实时数据，对于历史数据，需要在末尾加上 –from-beginning。 ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:2:4","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#生产者和生产者命令展示"},{"categories":["mq"],"content":"\r3. 生产者","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:3:0","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#3-生产者"},{"categories":["mq"],"content":"\r原理生产者中： main线程（producer）通过调用send(producerRecord)方法来发送消息 发送的数据先会经过 interceptors（拦截器），此拦截器是required，不是必须（生产环境中更多的是用flume中的拦截器） 紧接着数据会经过 serializer（序列化器） Java中本身就序列化，为什么不走Java的序列化？ 原因是Java的序列化传输的数据太重，除了本身要传输的数据，它还有很大的其他辅助数据的占比，来保证安全传输等。在大数据场景，大数据量下再额外加重每个数据整体的效率就会差很多。 序列化后会经过 partitioner（分区器），其作用是将数据发送到RecordAccumulator中。 在RecordAccumulator中，会根据不同分区会创建对应队列（DQueue），partitioner会决定将数据发送到哪个分区下的DQueue。 RecordAccumulator是在内存中进行的，其默认内存大小32M，一批次的大小16K（producerBatch）。 RecordAccumulator缓冲队列中的数据会通过 sender线程下的sender()方法将数据准备发送到Kafka集群，其触发契机有两个： 每批次满了，即batch.size（默认情况下16k），就会启用sender来发送数据。 达到等待时间就会发送，linger.ms，如果数据未达到batch.size，sender等到时间就会发送数据，单位ms，默认值0ms。 sender读取了数据后，通过NetworkClient中的request来讲数据发送到Kafka集群，默认集群中每个broker节点最多缓存5个请求。 第6步的发送数据，需要通过 selector打通producer和broker的链路后才能实现。 Kafka集群收到数据后，会有一个应答机制acks， 应答0：producer发送来的数据，不需要等数据落盘应答。 应答1：producer发送来的数据，leader接收到数据后应答。 应答-1：producer发送来的数据，leader 和 ISR队列里面所有节点收齐数据后应答，输入-1或者all都可以（默认）。 成功，Selector收到应答后，首先会清除掉队对应请求（NetworkClient中的request），并把对应的分区队列中数据清除掉（RecordAccumulator中对应的DQueue中存的数据） 失败，重试，重试次数int最大值，可修改。 ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:3:1","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#原理"},{"categories":["mq"],"content":"\r异步发送API先设置kafka集群配置文件，并进行序列化设置 private static Producer\u003cString, String\u003e getStringStringProducer() { // 0. 配置 + 连接集群 Properties properties = new Properties(); properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"kafka01:9092,kafka02:9092\"); // 对key、value进行序列化 properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // 1. 创建生产对象 return new KafkaProducer\u003c\u003e(properties); } ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:3:2","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#异步发送api"},{"categories":["mq"],"content":"\r普通异步发送异步发送发生在外部数据发送到RecordAccumulator中的DQueue期间，它不会关注队列是否成功把数据发送到kafka集群，外部数据只会一批批地发送数据到队列。 public static void commonAsync() { Producer\u003cString, String\u003e producer = getStringStringProducer(); // 2. 发送数据 for (int i = 1; i \u003c= 10; i++) { producer.send(new ProducerRecord\u003cString, String\u003e(\"first\", \"send message by common async \" + i + \" times\")); } // 3. 关闭资源，不关闭资源就收不到消息 producer.close(); } ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:3:3","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#普通异步发送"},{"categories":["mq"],"content":"\r回调异步发送在发送函数 send() 中多了一个返回函数Callback，里面有topic、partition等信息。 其中，需要重写CallBack中的 onCompletion()方法，通过RecordMetadata类型查看要返回的信息。 public static void callBackAsync() { Producer\u003cString, String\u003e producer = getStringStringProducer(); for (int i = 1; i \u003c= 10; i++) { producer.send(new ProducerRecord\u003cString, String\u003e(\"first\", \"send message by call back async \" + i + \" times\"), new Callback() { @Override public void onCompletion(RecordMetadata metadata, Exception e) { if (e == null){ System.out.println(metadata.topic() + \", \" + metadata.partition()); } } }); } // 3. 关闭资源 producer.close(); } ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:3:4","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#回调异步发送"},{"categories":["mq"],"content":"\r同步发送API同步数据与异步数据的不同是在于， 异步数据发送时，不需要等待RecodrAccumulator把队列中消息发送到Kafka集群，就能发送下次的数据。 同步数据发送时，必须等待RecodrAccumulator把队列中消息发送到Kafka集群完成后，才能发送下次数据。 在API调用方面，和 common sync的区别就在于，它会在方法后面加上一个.get()。 public static void sync() throws ExecutionException, InterruptedException { Producer\u003cString, String\u003e producer = getStringStringProducer(); for (int i = 1; i \u003c= 10; i++) { // 多了一个.get() producer.send(new ProducerRecord\u003cString, String\u003e(\"first\", \"send message by sync \" + i + \" times\")).get(); } // 3. 关闭资源 producer.close(); } ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:3:5","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#同步发送api"},{"categories":["mq"],"content":"\r分区好处： 便于合理使用存储资源，每个 partition 在 一个 broker 上存储，可以把海量数据按照分区切割存储在多个 broker上，实现负载均衡。 提升并行度， producer 可以以 分区为单位 发送数据；consumer 额可以以 分为单位 消费数据。 ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:3:6","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#分区"},{"categories":["mq"],"content":"\r分区策略 默认分区器 DefaultPartitioner（弃用） @Deprecated public class DefaultPartitioner implements Partitioner { private final StickyPartitionCache stickyPartitionCache = new StickyPartitionCache(); public DefaultPartitioner() { } public void configure(Map\u003cString, ?\u003e configs) { } public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) { return this.partition(topic, key, keyBytes, value, valueBytes, cluster, cluster.partitionsForTopic(topic).size()); } public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster, int numPartitions) { return keyBytes == null ? this.stickyPartitionCache.partition(topic, cluster) : BuiltInPartitioner.partitionForKey(keyBytes, numPartitions); } public void close() { } public void onNewBatch(String topic, Cluster cluster, int prevPartition) { this.stickyPartitionCache.nextPartition(topic, cluster, prevPartition); } } 这段代码实现了 Partitioner 接口的 DefaultPartitioner 类，通常用在 Apache Kafka 的客户端库中。Kafka 使用分区器来确定将每条消息发送到哪个分区。 解读代码： 类定义：@Deprecated注解表示此类已经被弃用，未来版本可能被移除。 成员变量：stickyPartitionCache用于缓存分区信息，其作用是在连续的消息发送中保持对同一分区的引用，减少分区切换，从而可能提高效率。 主要方法： partition() 重载方法， 第一个采用基本参数调用第二个方法。 第二个通过检查 keyBytes（消息键的字节表示）是否为空。 为空，使用 stickyPartitionCache 决定分区； 不为空，使用 BuiltInPartitioner.partitionForKey 方法根据键的字节数据和分区数量计算分区。 onNewBatch()，当开始新的消息批次时，用来更新 stickyPartitionCache 中分区信息。 也就是说，当partition指定情况下，直接将数据写入指定分区；当没有指定partition但key存在的情况下，通过key的hash值与topic的partition数 进行取余 得到partition值。如果partition值 和 key 都没有的情况下，就会通过 黏性分区（sticky partition）来随机选择一个分区，等该分区的batch满了或完成，才会再次随机一个分区进行使用。 RoundRobinPartitioner（Kafka中唯一正在用） public class RoundRobinPartitioner implements Partitioner { private final ConcurrentMap\u003cString, AtomicInteger\u003e topicCounterMap = new ConcurrentHashMap(); public RoundRobinPartitioner() { } public void configure(Map\u003cString, ?\u003e configs) { } public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) { List\u003cPartitionInfo\u003e partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); int nextValue = this.nextValue(topic); List\u003cPartitionInfo\u003e availablePartitions = cluster.availablePartitionsForTopic(topic); if (!availablePartitions.isEmpty()) { int part = Utils.toPositive(nextValue) % availablePartitions.size(); return ((PartitionInfo)availablePartitions.get(part)).partition(); } else { return Utils.toPositive(nextValue) % numPartitions; } } private int nextValue(String topic) { AtomicInteger counter = (AtomicInteger)this.topicCounterMap.computeIfAbsent(topic, (k) -\u003e { return new AtomicInteger(0); }); return counter.getAndIncrement(); } public void close() { } } 成员变量，ConcurrentMap\u003cString, AtomicInteger\u003e topicCounterMap，一个线程安全的映射，用于每个主题（topic）维护一个原子计数器。这个计数器用于实现轮询逻辑，以确保消息均匀地分布到各个分区。 主要方法： partition()（核心方法），用于决定消息发送到哪个分区。 partitions 获取给定主题的所有分区信息。 numPartitions 确定总分区数。 nextValue 获取当前主题的下一个轮询值。 availablePartitions 获取可用的分区列表。 如果有可用分区，该方法将基于轮询值选择一个分区；如果没有可用分区，则它将根据所有分区的数量来选择。无可用分区时的选择依据是，对所有分区的总数取模（即 Utils.toPositive(nextValue) % numPartitions）。 nextValue(String topic)，辅助方法，用于根据主题获取并增加计数器值。如果主题不存在于 topicCounterMap 中，则会创建一个新的计数器并初始化为 0。 两个策略对比， DefaultPartitioner 依赖于key的hash值来选择分区，相同 key 的所有消息将被路由到同一个分区。当没有提供 key 时，sticky 会尽可能地将连续消息发送到同一个分区，直到发生重新平衡或其他事件。确保特定用户的消息顺序或聚合特定类型的数据。 RoundRobinPartitioner 使用轮询算法，在所有可用分区之间均匀地分配消息。确保所有分区都被平等利用，减少特定分区的过载。 ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:3:7","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#分区策略"},{"categories":["mq"],"content":"\r自定义分区器自定义一个简单的分区器。 public class IndividualPartitioner implements Partitioner { @Override public int partition(String s, Object o, byte[] bytes, Object o1, byte[] bytes1, Cluster cluster) { // o表示key，o1是value String value = o1.toString(); int partition; if(value.contains(\"individual_partitioner\")){ partition = 0; }else { partition = 1; } return partition; } @Override public void close() { } @Override public void configure(Map\u003cString, ?\u003e map) { } } 测试 public static void useIndividualPartitioner(){ // 0. 配置 + 连接集群 Properties properties = new Properties(); properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"kafka01:9092,kafka02:9092\"); // 对key、value进行序列化 properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // 使用自定义分区器 properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, \"org.kafka.producer.IndividualPartitioner\"); Producer\u003cString, String\u003e producer = new KafkaProducer\u003c\u003e(properties); producer.send(new ProducerRecord\u003c\u003e(\"first\", \"individual_partitioner\"), new Callback() { @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) { if(e == null){ System.out.println(recordMetadata.topic() + \" \" + recordMetadata.partition()); } } }); producer.close(); } ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:3:8","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#自定义分区器"},{"categories":["mq"],"content":"\r提高吞吐量 修改 batch.size，批次大小，默认16k。 修改 linger.ms，等待时间，默认0ms，可修改为5-100ms。 修改 compression.tpye，压缩snappy。 修改 RecordAccumulator，缓冲区大小，修改为64m。 public static void setProducerParameters(){ // 配置 Properties properties = new Properties(); // 连接集群 properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"kafka01:9092, kafka02:9092\"); // 序列化 properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // 缓冲区大小 properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432); // 批次大小 properties.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384); // 徘徊时间 properties.put(ProducerConfig.LINGER_MS_CONFIG, 1); // 压缩 properties.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, CompressionType.SNAPPY.name); // 创建Producer Producer\u003cString, String\u003e producer = new KafkaProducer\u003c\u003e(properties); // 发送数据 for (int i = 0; i \u003c 100000; i++) { producer.send(new ProducerRecord\u003c\u003e(\"first\", \"test setting parameters\" + i), new Callback() { @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) { if(e == null){ System.out.println(\"Topic: \" + recordMetadata.topic() + \", Partition: \" + recordMetadata.partition()); } } }); } // 关闭流 producer.close(); } ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:3:9","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#提高吞吐量"},{"categories":["mq"],"content":"\r数据可靠性数据可靠性通过，kafka集群收到数据后返回给selector的应答acks来设置。 0，producer发送数据后，不需要等数据落盘就应答（可靠性最低）。 数据丢失场景，producer发送数据，leader突然挂了，leader没有拿到数据，follower也没有同步到数据，导致数据丢失。 1，producer发送数据后，Leader收到数据后应答。 数据丢失场景，producer发送数据，leader拿到数据应答后，还没同步副本就挂了，虽然follower会选举出新的leader继续接收数据，但之前没有同步到副本的数据已经丢失（因为之前挂掉的leader接收到丢失的数据已经发送应答了，那么producer就不会将之前的数据发送给新leader）。 比0的优势，会存在leader消息落盘，但没来得及给应答挂了，producer就会认为发送失败，会进行重试发送。 -1（或者 all），producer发送数据后，Leader 和 isr队列 里面所有节点收齐数据后应答，可靠性最高。 比1的优势，leader拿到数据，并且所有的follower都同步完数据后，leader才会ack应答producer。 可能存在的问题，如果其中某一个follower由于某种故障，不能和leader进行同步，那么leader就迟迟不能应答producer？ 实际上，leader维护了一个动态的 in-sync replica set（ISR），即 和leader保持同步的 follower+leader的集合（leader:0,isr:0,1,2）。 如果某个follower在规定时间内未向leader已发送通信请求或同步数据，则会将该follower踢出ISR。 该时间阈值由 replica.lag.time.max.ms 参数设定，默认30s。 通过该机制来保证不被故障节点影响。 注：ISR中，应答的最小副本数量为1（min.insync.replicas），默认值也为1，即只有leader本身，也就是说，不修改该值，那么效果等同于 ack=1。 因此，确保数据完全可靠条件（理论上），ACK级别=-1，分区副本\u003e=2，ISR中应答最小副本数量\u003e=2。 总结： ack=0，生产环境基本不用。 ack=1，一般用于传输普通日志，允许丢失个别数据。 ack=-1，一般用于传输和钱（或类似重要级别的）相关的数据，对可靠性要求高。 // 设置ack，默认 all或-1 properties.put(ProducerConfig.ACKS_CONFIG, \"1\"); // 重试次数，无应答时的重试次数，默认是int最大值 properties.put(ProducerConfig.RETRIES_CONFIG, 3); ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:3:10","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#数据可靠性"},{"categories":["mq"],"content":"\r数据重复数据传递语义 At Least Once，ack=-1 + 分区副本 \u003e= 2 + ISR 应答的最小副本数量 \u003e= 2。保证数据不丢失，但不能保证数据不重复。 At Most Once，ack=0，保证数据不重复，但不能保证数据不丢失。 Exactly Once，比如钱场景下，保证数据既不重复也不丢失。 Kafka0.11版本后，引入了幂等性和事务。幂等性就是不论 producer向 broker发送多少次重复数据，broker都只会持久化一条，避免消息重复。 重复数据的判断标准：\u003cPID, partition, seqNumber\u003e 相同主键的消息提交时，broker只会持久化一条。其中， PID，每次kafka重启会分配一个新的。 partition表示分区号。 sequence number 是单调递增。 所以，幂等性，只能保证在单分区会话内不重复。 因此，exactly once = 幂等性 + at least once。 如何使用幂等性？ 开启 enable.idempotence参数，默认为true，false关闭。 在kafka中开启事务的前提，必须开启幂等性。处理事务是由 Transaction Coordinator（事务协调器） 完成的。每个broker都有自己的事务协调器（不同broker之间的事务如何协调，可能和底层hashcode有关）。 public static void setProducerTransaction(){ Properties properties = new Properties(); properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"kafka01:9092,kafka02:9092\"); properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // 指定事务ID，不然会报错 properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"testing_transaction_id_01\"); Producer\u003cObject, Object\u003e producer = new KafkaProducer\u003c\u003e(properties); // 初始化事务 producer.initTransactions(); // 开启事务 producer.beginTransaction(); try{ for (int i = 0; i \u003c 5; i++) { producer.send(new ProducerRecord\u003c\u003e(\"first\", \"test setting transaction\" + i)); } // 提交事务 producer.commitTransaction(); }catch (Exception e){ // 出现异常，就终止事务 producer.abortTransaction(); }finally { producer.close(); } } ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:3:11","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#数据重复"},{"categories":["mq"],"content":"\r数据有序单分区内，消息有序（有一定条件）；多分区下，分区之间数据无序。 ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:3:12","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#数据有序"},{"categories":["mq"],"content":"\r数据乱序InFlightRequests，默认每个broker最多缓存5个请求。正常情况下，每次请求集群都成功，那么消息就是顺序，如果前面的request第一次请求不成功，就会造成后面的请求完成在在前面的请求之前。就会造成乱序的情况（单分区）。 为了解决乱序，单分区下，分两种情况， 未开启幂等性，需要将 max.in.flight.requests.per.connnection 设置为1。 开启幂等性，需要将 max.in.flight.requests.per.connnection 设置 \u003c= 5。 其底层是通过，开启幂等性后，kafka服务端会缓存producer发来的最近的5个 request的元数据，无论如何，都可以保证最近5个的request的数据是有序的。 ","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:3:13","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#数据乱序"},{"categories":["mq"],"content":"\r4. Broker","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:4:0","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#4-broker"},{"categories":["mq"],"content":"\r5. 消费者","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:5:0","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#5-消费者"},{"categories":["mq"],"content":"\r6. Eagle监控","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:6:0","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#6-eagle监控"},{"categories":["mq"],"content":"\r7. Kraft模式","date":"2023-03-14","objectID":"/1.-kafka-%E5%85%A5%E9%97%A8/:7:0","series":["mq - kafka"],"tags":["mq","kafka"],"title":"Kafka 入门","uri":"/1.-kafka-%E5%85%A5%E9%97%A8/#7-kraft模式"},{"categories":["java"],"content":"\r1 内存与垃圾回收","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:1:0","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#1-内存与垃圾回收"},{"categories":["java"],"content":"\r1.1 JVM与Java体系结构java比c++优势：内存动态分配、垃圾收集技术 jvm字节码：特定的二进制文件格式，class文件格式所关联。各种语言通过编译器编译生成的可以供虚拟机识别的语言，比如java编译器。不同语言的编译器编译成相同的字节码然后在jvm上运行。所以jvm现在已经不单独属于java。 多语言混合编程：由于越来越多的语言都开始支持在jvm上运行，多语言混合变成开始成为主流。比如，一个项目中，处理层用Clojure，展示层用Jruby/Rails，中间层用Java。每个应用层都使用不同的语言来完成。由于大家都运行在jvm上，所以跨语言之间的交互和调用就像调用自己语言中的api一样。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:1:1","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#11-jvm与java体系结构"},{"categories":["java"],"content":"\r1.1.1 虚拟机和Java虚拟机虚拟机：系统虚拟机：VMwware、Visual Box；成虚拟机：JVM Java虚拟机： 作用：执行字节码，二进制字节码的运行环境，负责装字节码到其内部，解释/编译对应平台的机器指令。每个指令，在java虚拟机规范中都有说明。 特点：一次编译、到处运行；自动内存管理；自动垃圾回收功能 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:1:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#111-虚拟机和java虚拟机"},{"categories":["java"],"content":"\r1.1.2 JVM整体结构HotSpot VM 是主流高性能虚拟机的代表作之一，采用解释器与即时编译器并存的架构 Class Loader就是将class files加载到内存当中，生成一个大的class对象，过程中涉及到加载、链接和初始化。 Runtime Data Area中，方法区和堆是多线程共享的，Java Stack(VM Stack), Native Method Stack and Program Counter Register是每个线程独一份的 Execution Engine 有三个部分，Interpreter(解释器), JIT Compiler(即时编译器) and Garbage Collection(垃圾回收器) ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:1:3","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#112-jvm整体结构"},{"categories":["java"],"content":"\r1.1.3 Java代码执行流程\r其中，JIT编译器主要负责提升编译速度，因为有些数据希望是热启动，比如放在缓存中，达到随用随取，而不是用时再编译。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:1:4","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#113-java代码执行流程"},{"categories":["java"],"content":"\r1.1.4 JVM架构模型HotSpot基于的是栈的指令集架构。因为要实现跨平台就不能基于寄存器，因为不用平台cpu架构不同，这就导致性能下降，实现同样功能需要更多的指令。 Java编译器输入的两种指令流： 栈的指令集结构 寄存器的指令集架构 二者区别： 基于栈式架构的特点： 设计和实现更简单，适用于资源受限的系统 避开了寄存器的分配难题：使用零地址指令方式分配 指令流中的指令大部分是零地址指令，起执行过程中依赖于操作栈。指令集更小。编译器更容易实现。 不需要硬件支持、可移植性更好、更好实现跨平台 基于寄存器架构特点： 典型的应用于x86的二进制指令集：比如传统的PC以及Android的Davlik虚拟机 指令架构则完全依赖硬件、可移植性差 性能优秀和执行更高效 花费更少的指令去完成一项操作 在大部分情况下，基于寄存器架构的指令集往往都以一地指令、二地址指令和三地址指令为主，而基于栈式架构的指令集却以零地址指令为主。 零地址指令是没有地址，只有操作数 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:1:5","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#114-jvm架构模型"},{"categories":["java"],"content":"\r1.1.5 JVM生命周期 启动，通过引导类加载器(bootstrap class loader)创建一个初始类(initial class)来完成的，这个类由虚拟机得具体实现指定的。 执行，程序开始执行的时候才运行，程序结束时它就结束。执行一个所谓java程序的时候，真真正正在执行的是一个叫做java虚拟机的进程。 退出： 程序正常执行结束； 程序在执行过程中遇到了异常或错误而异常终止； 由于操作系统出现错误导致java虚拟机进程终止； 某线程调用Runtime类或System的exit方法，或Runtime类的halt方法，并且Java安全管理器也允许这次exit或halt操作。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:1:6","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#115-jvm生命周期"},{"categories":["java"],"content":"\r1.2 类加载器子系统 (Class Loader SubSystem)\r类的加载过程： 加载阶段 链接阶段 初始化阶段 类加载器子系统的作用：从文件系统或网络中加载class文件。 ClassLoader只负责class文件的加载，至于它是否可以运行，则由Execution Engine决定。 加载类信息存放于方法区的内存空间。不仅如此，方法区还会存放运行时常量池信息和包括字符串字面量和数字常量(这部分常量信息是Class文件中常量池(constant pool)部分的内存映射)。 class file存在于本地硬盘上，最终执行的时候是要在JVM中根据这个文件实例化出n个一摸一样的实例。 class file加载到JVM中，被称为DNA元数据模板，放在方法区。 在.class文件 -\u003e JVM -\u003e 最终成为元数据模板，此过程就需要一个运输工具(类装载器 Class Loader)，扮演一个快递员的角色 (这个加载方式是通过二进制流的方式加载进来的)。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:1:7","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#12-类加载器子系统-class-loader-subsystem"},{"categories":["java"],"content":"\r1.2.1 Loading阶段 通过一个类的全限定名获取定义此类的二进制字节流。 将这个字节流所代表的静态存储结构转化味方法区的运行时数据结构。 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类对各种数据的访问入口。 加载.class的方式： 从本地系统中直接加载 通过网络获取，典型场景：Web Applet 从zip压缩包中读取，成为日后jar、war格式得基础 运行时计算生成，使用最多的是：动态代理技术 由其他文件生成，典型场景：JSP应用 从专有数据库提取.class文件，比较少见 从加密文件中获取，典型的防Class文件被反编译的保护措施 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:1:8","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#121-loading阶段"},{"categories":["java"],"content":"\r1.2.2 Linking阶段 验证(Verify)： 目的在于确保Class文件的字节流中包含信息符合当前虚拟机要求，保证被加载类的正确性，不会危害虚拟机自身安全。 四种验证：文件格式验证、元数据验证、字节码验证、符号引用验证。 准备(prepare)： 为类变量分配内存并且设置该类变量的默认初始值，即零值。 这里不包含用final修饰的static，因为final在编译的时候就会分配了，准备阶段会显示初始化。 这里不会为实例变量分配初始化，类变量会分配在方法区中，而实例变量是会随着对象一起分配到Java堆中。 解析(Resolve)： 将常量池内的符号引用转换为直接引用的过程。 事实上，解析操作往往会伴随着JVM在执行完初始化之后再执行。 符号引用就是一组符号来描述所引用的目标。负号引用的字面量形式明确定义子java虚拟机规范的Class文件格式中。直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。 解析动作主要针对类、接口、字段、类方法、接口方法、方法类型等。对应常量池中的 CONSTANT_Class_info、CONSTANT_Fieldref_info、CONSTANT_Methodref_info等。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:2:0","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#122-linking阶段"},{"categories":["java"],"content":"\r1.2.3 Initialization阶段 初始化阶段就是执行类构造器方法\u003cclinit\u003e()的过程。 此方法不需要定义，是javac编译器自动收集类中的所有类变量的赋值动作和静态代码块中的语句合并而来。(有赋值这类动作，\u003cclinit\u003e()才存在，否则就不存在) 构造器方法中指令按语句源文件中出现的顺序执行。 \u003cclinit\u003e()不同于类的构造器。(关联：构造器是虚拟机视角下的\u003cinit\u003e()) 若该类具有父类，JVM会保证子类的\u003cclinit\u003e()执行前，父类的\u003cclinit\u003e()已经执行完毕。 虚拟机必须保证一个类的\u003cclinit\u003e()方法在多线程下被同步加锁。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:2:1","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#123-initialization阶段"},{"categories":["java"],"content":"\r1.2.4 类加载器分类 JVM支持两种类型的加载器，分别为引导类加载器 (Bootstrap ClassLoader) 和 自定义类加载器(User-Defined ClassLoader) 从概念上讲，自定义类加载器一般指的是程序中由开发人员自定义的一类加载器，但是Java虚拟机规范却没有这么定义，而是将所有派生于抽象类ClassLoader的类加载器都划分为自定义类加载器。 无论类加载器的类型如何划分，在程序中最常见的类加载器始终只有三个： 虚拟机自带的加载器 启动类加载器 (引导类加载器，Bootstrap ClassLoader) a. 这个类加载使用 C/C++语言实现，嵌套在JVM内部 b. 它用来加载Java核心库 (JAVA_HOME/jre/lib/rt.jar、resources.jar或sun.boot.class.path路径下的内容)，用于提供JVM自身需要的类 c. 并不继承自java.lang.CLassLoader，没有父加载器 d. 加载扩展类和应用程序类加载器，并指定为他们的父类加载器 e. 出于安全考虑，Bootstrap启动类加载器只加载包名为java、javax、sun等开头的类 扩展类加载器 (Extension ClassLoader) a. Java语言编写，由sun.misc.Launcher$ExtClassLoader实现 b. 派生于ClassLoader类 c. 父类加载器为启动类加载器 d. 从java.ext.dirs系统属性所指定的目录中加载类库，或从JDK的安装目录的jre/lib/ext子目录 (扩展目录) 下加载类库。如果用户创建的JAR放在此目录下，也会自动由扩展类加载器加载 应用程序类加载器 (系统类加载器 AppClassLoader) a. Java编写，由sun.misc.Launcher$AppClassLoader实现 b. 派生于ClassLoader类 c. 父类加载器为扩展类加载器 d. 它负责加载环境变量classpath或系统属性 java.class.path制定路径下的类库 e. 该类加载是程序中默认的类加载器，一般来说，java应用的类都是由它类完成加载 f. 通过ClassLoader#getSystemClassLoader() 方法可以获取该类加载器 用户自定义类加载在java的日常应用程序开发中，类的加载几乎是由上述3种类加载器的相互配合执行的，在必要时，还可以自定义类加载器，来定制类的加载方式 为什么要自定义类加载器？ 隔离加载类 (比如，避免类的冲突，不同包可能有一样的名字) 修改类加载的方式 扩展加载源 防止源码泄露 用户自定义类加载器实现步骤： 通过继承抽象类 java.lang.ClassLoader类的方式，实现自己的类加载器，以满足一些特殊的需求 在jdk1.2前，自定义类加载器需要继承ClassLoader类并重写loadClass()方法。但是在1.2之后，不需要重写loadClass()，而是吧自定义的类加载逻辑编辑在findClass()中 在没有复杂的需求，可以直接继承URLClassLoader类，这样可以避免自己编写findClass()方法及其获取字节码流的方式，使用自定义类加载器编写更简洁 关于ClassLoader，它是一个抽象类，其后所有的类加载器都继承自ClassLoader (不包括启动类加载器)。 获取ClassLoader的途径 双亲委派机制Java虚拟机对class文件采用的是按需加载的方式，也就是说当需要使用该类时才会将它的class文件加载到内存生成class对象。而且加载某个类的class文件时，java虚拟机采用的是双亲委派机制，即把请求交由父类处理，它是一种任务委派模式 工作原理： 如果一个类加载器收到类加载请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行 如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归，请求最终将达到顶层的启动类加载器 如果父类加载器可以完成加载任务，子加载器才会尝试自己去加载，这就是双亲委派模式。 举例2： 双亲委派机制的优势: 避免类的重复加载 保护程序安全，防止核心API被随意篡改 a. 自定义类: java.lang.String b. 自定义类: java.lang.ShkStart 双亲委派机制拓展：沙箱安全机制 自定义String类，但是在加载自定义String类的时候会率先使用引导类加载器加载，而引导类加载器在加载的过程中会先加载jdk自带的文件 (rt.jar包中java\\lang\\String.class)，报错信息说没有main方法，就是因为加载的rt.jar包中的String类。这样可以敢保证对java核心源代码的保护，这就是沙箱安全机制 拓展在JVM中表示两个Class对象是否为同一个类存在的两个必要条件： 类的完成类名必须一致，包括包名。 家在这个类的ClassLoader (指ClassLoader实例对象) 必须相同 换句话说，在JVM中，即使两个类对象来源于同一个class文件，被用一个虚拟机所加载，但是只要加载他们的classLoader实例对象不同，那么他们两个类的对象也是不相等的 JVM必须知道一个类型是由启动加载器加载的还是由用户类加载器加载的。由用户类型加载器加载的，那么jvm会将这个类加载器的一个引用作为类型信息的一部分保存在方法区中。当解析一个类型到另一个类型的引用的时候，jvm需要保证这两个类型的类加载器是相同的。 类的主动使用和被动使用: 主动使用： a. 创建类的实例 b. 访问某个类或接口的静态变量，或者对该静态变量赋值 c. 调用类的静态方法 d. 反射 e. 初始化一个类的子类 f. Java虚拟机启动时被标明为启动类的类 g. jdk7开始提供的动态语言支持 除了这7种情况，其他使用java类的方式都被看作是对类的被动使用，都不会到之类的初始化 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:2:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#124-类加载器分类"},{"categories":["java"],"content":"\r1.2.4 类加载器分类 JVM支持两种类型的加载器，分别为引导类加载器 (Bootstrap ClassLoader) 和 自定义类加载器(User-Defined ClassLoader) 从概念上讲，自定义类加载器一般指的是程序中由开发人员自定义的一类加载器，但是Java虚拟机规范却没有这么定义，而是将所有派生于抽象类ClassLoader的类加载器都划分为自定义类加载器。 无论类加载器的类型如何划分，在程序中最常见的类加载器始终只有三个： 虚拟机自带的加载器 启动类加载器 (引导类加载器，Bootstrap ClassLoader) a. 这个类加载使用 C/C++语言实现，嵌套在JVM内部 b. 它用来加载Java核心库 (JAVA_HOME/jre/lib/rt.jar、resources.jar或sun.boot.class.path路径下的内容)，用于提供JVM自身需要的类 c. 并不继承自java.lang.CLassLoader，没有父加载器 d. 加载扩展类和应用程序类加载器，并指定为他们的父类加载器 e. 出于安全考虑，Bootstrap启动类加载器只加载包名为java、javax、sun等开头的类 扩展类加载器 (Extension ClassLoader) a. Java语言编写，由sun.misc.Launcher$ExtClassLoader实现 b. 派生于ClassLoader类 c. 父类加载器为启动类加载器 d. 从java.ext.dirs系统属性所指定的目录中加载类库，或从JDK的安装目录的jre/lib/ext子目录 (扩展目录) 下加载类库。如果用户创建的JAR放在此目录下，也会自动由扩展类加载器加载 应用程序类加载器 (系统类加载器 AppClassLoader) a. Java编写，由sun.misc.Launcher$AppClassLoader实现 b. 派生于ClassLoader类 c. 父类加载器为扩展类加载器 d. 它负责加载环境变量classpath或系统属性 java.class.path制定路径下的类库 e. 该类加载是程序中默认的类加载器，一般来说，java应用的类都是由它类完成加载 f. 通过ClassLoader#getSystemClassLoader() 方法可以获取该类加载器 用户自定义类加载在java的日常应用程序开发中，类的加载几乎是由上述3种类加载器的相互配合执行的，在必要时，还可以自定义类加载器，来定制类的加载方式 为什么要自定义类加载器？ 隔离加载类 (比如，避免类的冲突，不同包可能有一样的名字) 修改类加载的方式 扩展加载源 防止源码泄露 用户自定义类加载器实现步骤： 通过继承抽象类 java.lang.ClassLoader类的方式，实现自己的类加载器，以满足一些特殊的需求 在jdk1.2前，自定义类加载器需要继承ClassLoader类并重写loadClass()方法。但是在1.2之后，不需要重写loadClass()，而是吧自定义的类加载逻辑编辑在findClass()中 在没有复杂的需求，可以直接继承URLClassLoader类，这样可以避免自己编写findClass()方法及其获取字节码流的方式，使用自定义类加载器编写更简洁 关于ClassLoader，它是一个抽象类，其后所有的类加载器都继承自ClassLoader (不包括启动类加载器)。 获取ClassLoader的途径 双亲委派机制Java虚拟机对class文件采用的是按需加载的方式，也就是说当需要使用该类时才会将它的class文件加载到内存生成class对象。而且加载某个类的class文件时，java虚拟机采用的是双亲委派机制，即把请求交由父类处理，它是一种任务委派模式 工作原理： 如果一个类加载器收到类加载请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行 如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归，请求最终将达到顶层的启动类加载器 如果父类加载器可以完成加载任务，子加载器才会尝试自己去加载，这就是双亲委派模式。 举例2： 双亲委派机制的优势: 避免类的重复加载 保护程序安全，防止核心API被随意篡改 a. 自定义类: java.lang.String b. 自定义类: java.lang.ShkStart 双亲委派机制拓展：沙箱安全机制 自定义String类，但是在加载自定义String类的时候会率先使用引导类加载器加载，而引导类加载器在加载的过程中会先加载jdk自带的文件 (rt.jar包中java\\lang\\String.class)，报错信息说没有main方法，就是因为加载的rt.jar包中的String类。这样可以敢保证对java核心源代码的保护，这就是沙箱安全机制 拓展在JVM中表示两个Class对象是否为同一个类存在的两个必要条件： 类的完成类名必须一致，包括包名。 家在这个类的ClassLoader (指ClassLoader实例对象) 必须相同 换句话说，在JVM中，即使两个类对象来源于同一个class文件，被用一个虚拟机所加载，但是只要加载他们的classLoader实例对象不同，那么他们两个类的对象也是不相等的 JVM必须知道一个类型是由启动加载器加载的还是由用户类加载器加载的。由用户类型加载器加载的，那么jvm会将这个类加载器的一个引用作为类型信息的一部分保存在方法区中。当解析一个类型到另一个类型的引用的时候，jvm需要保证这两个类型的类加载器是相同的。 类的主动使用和被动使用: 主动使用： a. 创建类的实例 b. 访问某个类或接口的静态变量，或者对该静态变量赋值 c. 调用类的静态方法 d. 反射 e. 初始化一个类的子类 f. Java虚拟机启动时被标明为启动类的类 g. jdk7开始提供的动态语言支持 除了这7种情况，其他使用java类的方式都被看作是对类的被动使用，都不会到之类的初始化 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:2:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#虚拟机自带的加载器"},{"categories":["java"],"content":"\r1.2.4 类加载器分类 JVM支持两种类型的加载器，分别为引导类加载器 (Bootstrap ClassLoader) 和 自定义类加载器(User-Defined ClassLoader) 从概念上讲，自定义类加载器一般指的是程序中由开发人员自定义的一类加载器，但是Java虚拟机规范却没有这么定义，而是将所有派生于抽象类ClassLoader的类加载器都划分为自定义类加载器。 无论类加载器的类型如何划分，在程序中最常见的类加载器始终只有三个： 虚拟机自带的加载器 启动类加载器 (引导类加载器，Bootstrap ClassLoader) a. 这个类加载使用 C/C++语言实现，嵌套在JVM内部 b. 它用来加载Java核心库 (JAVA_HOME/jre/lib/rt.jar、resources.jar或sun.boot.class.path路径下的内容)，用于提供JVM自身需要的类 c. 并不继承自java.lang.CLassLoader，没有父加载器 d. 加载扩展类和应用程序类加载器，并指定为他们的父类加载器 e. 出于安全考虑，Bootstrap启动类加载器只加载包名为java、javax、sun等开头的类 扩展类加载器 (Extension ClassLoader) a. Java语言编写，由sun.misc.Launcher$ExtClassLoader实现 b. 派生于ClassLoader类 c. 父类加载器为启动类加载器 d. 从java.ext.dirs系统属性所指定的目录中加载类库，或从JDK的安装目录的jre/lib/ext子目录 (扩展目录) 下加载类库。如果用户创建的JAR放在此目录下，也会自动由扩展类加载器加载 应用程序类加载器 (系统类加载器 AppClassLoader) a. Java编写，由sun.misc.Launcher$AppClassLoader实现 b. 派生于ClassLoader类 c. 父类加载器为扩展类加载器 d. 它负责加载环境变量classpath或系统属性 java.class.path制定路径下的类库 e. 该类加载是程序中默认的类加载器，一般来说，java应用的类都是由它类完成加载 f. 通过ClassLoader#getSystemClassLoader() 方法可以获取该类加载器 用户自定义类加载在java的日常应用程序开发中，类的加载几乎是由上述3种类加载器的相互配合执行的，在必要时，还可以自定义类加载器，来定制类的加载方式 为什么要自定义类加载器？ 隔离加载类 (比如，避免类的冲突，不同包可能有一样的名字) 修改类加载的方式 扩展加载源 防止源码泄露 用户自定义类加载器实现步骤： 通过继承抽象类 java.lang.ClassLoader类的方式，实现自己的类加载器，以满足一些特殊的需求 在jdk1.2前，自定义类加载器需要继承ClassLoader类并重写loadClass()方法。但是在1.2之后，不需要重写loadClass()，而是吧自定义的类加载逻辑编辑在findClass()中 在没有复杂的需求，可以直接继承URLClassLoader类，这样可以避免自己编写findClass()方法及其获取字节码流的方式，使用自定义类加载器编写更简洁 关于ClassLoader，它是一个抽象类，其后所有的类加载器都继承自ClassLoader (不包括启动类加载器)。 获取ClassLoader的途径 双亲委派机制Java虚拟机对class文件采用的是按需加载的方式，也就是说当需要使用该类时才会将它的class文件加载到内存生成class对象。而且加载某个类的class文件时，java虚拟机采用的是双亲委派机制，即把请求交由父类处理，它是一种任务委派模式 工作原理： 如果一个类加载器收到类加载请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行 如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归，请求最终将达到顶层的启动类加载器 如果父类加载器可以完成加载任务，子加载器才会尝试自己去加载，这就是双亲委派模式。 举例2： 双亲委派机制的优势: 避免类的重复加载 保护程序安全，防止核心API被随意篡改 a. 自定义类: java.lang.String b. 自定义类: java.lang.ShkStart 双亲委派机制拓展：沙箱安全机制 自定义String类，但是在加载自定义String类的时候会率先使用引导类加载器加载，而引导类加载器在加载的过程中会先加载jdk自带的文件 (rt.jar包中java\\lang\\String.class)，报错信息说没有main方法，就是因为加载的rt.jar包中的String类。这样可以敢保证对java核心源代码的保护，这就是沙箱安全机制 拓展在JVM中表示两个Class对象是否为同一个类存在的两个必要条件： 类的完成类名必须一致，包括包名。 家在这个类的ClassLoader (指ClassLoader实例对象) 必须相同 换句话说，在JVM中，即使两个类对象来源于同一个class文件，被用一个虚拟机所加载，但是只要加载他们的classLoader实例对象不同，那么他们两个类的对象也是不相等的 JVM必须知道一个类型是由启动加载器加载的还是由用户类加载器加载的。由用户类型加载器加载的，那么jvm会将这个类加载器的一个引用作为类型信息的一部分保存在方法区中。当解析一个类型到另一个类型的引用的时候，jvm需要保证这两个类型的类加载器是相同的。 类的主动使用和被动使用: 主动使用： a. 创建类的实例 b. 访问某个类或接口的静态变量，或者对该静态变量赋值 c. 调用类的静态方法 d. 反射 e. 初始化一个类的子类 f. Java虚拟机启动时被标明为启动类的类 g. jdk7开始提供的动态语言支持 除了这7种情况，其他使用java类的方式都被看作是对类的被动使用，都不会到之类的初始化 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:2:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#用户自定义类加载"},{"categories":["java"],"content":"\r1.2.4 类加载器分类 JVM支持两种类型的加载器，分别为引导类加载器 (Bootstrap ClassLoader) 和 自定义类加载器(User-Defined ClassLoader) 从概念上讲，自定义类加载器一般指的是程序中由开发人员自定义的一类加载器，但是Java虚拟机规范却没有这么定义，而是将所有派生于抽象类ClassLoader的类加载器都划分为自定义类加载器。 无论类加载器的类型如何划分，在程序中最常见的类加载器始终只有三个： 虚拟机自带的加载器 启动类加载器 (引导类加载器，Bootstrap ClassLoader) a. 这个类加载使用 C/C++语言实现，嵌套在JVM内部 b. 它用来加载Java核心库 (JAVA_HOME/jre/lib/rt.jar、resources.jar或sun.boot.class.path路径下的内容)，用于提供JVM自身需要的类 c. 并不继承自java.lang.CLassLoader，没有父加载器 d. 加载扩展类和应用程序类加载器，并指定为他们的父类加载器 e. 出于安全考虑，Bootstrap启动类加载器只加载包名为java、javax、sun等开头的类 扩展类加载器 (Extension ClassLoader) a. Java语言编写，由sun.misc.Launcher$ExtClassLoader实现 b. 派生于ClassLoader类 c. 父类加载器为启动类加载器 d. 从java.ext.dirs系统属性所指定的目录中加载类库，或从JDK的安装目录的jre/lib/ext子目录 (扩展目录) 下加载类库。如果用户创建的JAR放在此目录下，也会自动由扩展类加载器加载 应用程序类加载器 (系统类加载器 AppClassLoader) a. Java编写，由sun.misc.Launcher$AppClassLoader实现 b. 派生于ClassLoader类 c. 父类加载器为扩展类加载器 d. 它负责加载环境变量classpath或系统属性 java.class.path制定路径下的类库 e. 该类加载是程序中默认的类加载器，一般来说，java应用的类都是由它类完成加载 f. 通过ClassLoader#getSystemClassLoader() 方法可以获取该类加载器 用户自定义类加载在java的日常应用程序开发中，类的加载几乎是由上述3种类加载器的相互配合执行的，在必要时，还可以自定义类加载器，来定制类的加载方式 为什么要自定义类加载器？ 隔离加载类 (比如，避免类的冲突，不同包可能有一样的名字) 修改类加载的方式 扩展加载源 防止源码泄露 用户自定义类加载器实现步骤： 通过继承抽象类 java.lang.ClassLoader类的方式，实现自己的类加载器，以满足一些特殊的需求 在jdk1.2前，自定义类加载器需要继承ClassLoader类并重写loadClass()方法。但是在1.2之后，不需要重写loadClass()，而是吧自定义的类加载逻辑编辑在findClass()中 在没有复杂的需求，可以直接继承URLClassLoader类，这样可以避免自己编写findClass()方法及其获取字节码流的方式，使用自定义类加载器编写更简洁 关于ClassLoader，它是一个抽象类，其后所有的类加载器都继承自ClassLoader (不包括启动类加载器)。 获取ClassLoader的途径 双亲委派机制Java虚拟机对class文件采用的是按需加载的方式，也就是说当需要使用该类时才会将它的class文件加载到内存生成class对象。而且加载某个类的class文件时，java虚拟机采用的是双亲委派机制，即把请求交由父类处理，它是一种任务委派模式 工作原理： 如果一个类加载器收到类加载请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行 如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归，请求最终将达到顶层的启动类加载器 如果父类加载器可以完成加载任务，子加载器才会尝试自己去加载，这就是双亲委派模式。 举例2： 双亲委派机制的优势: 避免类的重复加载 保护程序安全，防止核心API被随意篡改 a. 自定义类: java.lang.String b. 自定义类: java.lang.ShkStart 双亲委派机制拓展：沙箱安全机制 自定义String类，但是在加载自定义String类的时候会率先使用引导类加载器加载，而引导类加载器在加载的过程中会先加载jdk自带的文件 (rt.jar包中java\\lang\\String.class)，报错信息说没有main方法，就是因为加载的rt.jar包中的String类。这样可以敢保证对java核心源代码的保护，这就是沙箱安全机制 拓展在JVM中表示两个Class对象是否为同一个类存在的两个必要条件： 类的完成类名必须一致，包括包名。 家在这个类的ClassLoader (指ClassLoader实例对象) 必须相同 换句话说，在JVM中，即使两个类对象来源于同一个class文件，被用一个虚拟机所加载，但是只要加载他们的classLoader实例对象不同，那么他们两个类的对象也是不相等的 JVM必须知道一个类型是由启动加载器加载的还是由用户类加载器加载的。由用户类型加载器加载的，那么jvm会将这个类加载器的一个引用作为类型信息的一部分保存在方法区中。当解析一个类型到另一个类型的引用的时候，jvm需要保证这两个类型的类加载器是相同的。 类的主动使用和被动使用: 主动使用： a. 创建类的实例 b. 访问某个类或接口的静态变量，或者对该静态变量赋值 c. 调用类的静态方法 d. 反射 e. 初始化一个类的子类 f. Java虚拟机启动时被标明为启动类的类 g. jdk7开始提供的动态语言支持 除了这7种情况，其他使用java类的方式都被看作是对类的被动使用，都不会到之类的初始化 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:2:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#双亲委派机制"},{"categories":["java"],"content":"\r1.2.4 类加载器分类 JVM支持两种类型的加载器，分别为引导类加载器 (Bootstrap ClassLoader) 和 自定义类加载器(User-Defined ClassLoader) 从概念上讲，自定义类加载器一般指的是程序中由开发人员自定义的一类加载器，但是Java虚拟机规范却没有这么定义，而是将所有派生于抽象类ClassLoader的类加载器都划分为自定义类加载器。 无论类加载器的类型如何划分，在程序中最常见的类加载器始终只有三个： 虚拟机自带的加载器 启动类加载器 (引导类加载器，Bootstrap ClassLoader) a. 这个类加载使用 C/C++语言实现，嵌套在JVM内部 b. 它用来加载Java核心库 (JAVA_HOME/jre/lib/rt.jar、resources.jar或sun.boot.class.path路径下的内容)，用于提供JVM自身需要的类 c. 并不继承自java.lang.CLassLoader，没有父加载器 d. 加载扩展类和应用程序类加载器，并指定为他们的父类加载器 e. 出于安全考虑，Bootstrap启动类加载器只加载包名为java、javax、sun等开头的类 扩展类加载器 (Extension ClassLoader) a. Java语言编写，由sun.misc.Launcher$ExtClassLoader实现 b. 派生于ClassLoader类 c. 父类加载器为启动类加载器 d. 从java.ext.dirs系统属性所指定的目录中加载类库，或从JDK的安装目录的jre/lib/ext子目录 (扩展目录) 下加载类库。如果用户创建的JAR放在此目录下，也会自动由扩展类加载器加载 应用程序类加载器 (系统类加载器 AppClassLoader) a. Java编写，由sun.misc.Launcher$AppClassLoader实现 b. 派生于ClassLoader类 c. 父类加载器为扩展类加载器 d. 它负责加载环境变量classpath或系统属性 java.class.path制定路径下的类库 e. 该类加载是程序中默认的类加载器，一般来说，java应用的类都是由它类完成加载 f. 通过ClassLoader#getSystemClassLoader() 方法可以获取该类加载器 用户自定义类加载在java的日常应用程序开发中，类的加载几乎是由上述3种类加载器的相互配合执行的，在必要时，还可以自定义类加载器，来定制类的加载方式 为什么要自定义类加载器？ 隔离加载类 (比如，避免类的冲突，不同包可能有一样的名字) 修改类加载的方式 扩展加载源 防止源码泄露 用户自定义类加载器实现步骤： 通过继承抽象类 java.lang.ClassLoader类的方式，实现自己的类加载器，以满足一些特殊的需求 在jdk1.2前，自定义类加载器需要继承ClassLoader类并重写loadClass()方法。但是在1.2之后，不需要重写loadClass()，而是吧自定义的类加载逻辑编辑在findClass()中 在没有复杂的需求，可以直接继承URLClassLoader类，这样可以避免自己编写findClass()方法及其获取字节码流的方式，使用自定义类加载器编写更简洁 关于ClassLoader，它是一个抽象类，其后所有的类加载器都继承自ClassLoader (不包括启动类加载器)。 获取ClassLoader的途径 双亲委派机制Java虚拟机对class文件采用的是按需加载的方式，也就是说当需要使用该类时才会将它的class文件加载到内存生成class对象。而且加载某个类的class文件时，java虚拟机采用的是双亲委派机制，即把请求交由父类处理，它是一种任务委派模式 工作原理： 如果一个类加载器收到类加载请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行 如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归，请求最终将达到顶层的启动类加载器 如果父类加载器可以完成加载任务，子加载器才会尝试自己去加载，这就是双亲委派模式。 举例2： 双亲委派机制的优势: 避免类的重复加载 保护程序安全，防止核心API被随意篡改 a. 自定义类: java.lang.String b. 自定义类: java.lang.ShkStart 双亲委派机制拓展：沙箱安全机制 自定义String类，但是在加载自定义String类的时候会率先使用引导类加载器加载，而引导类加载器在加载的过程中会先加载jdk自带的文件 (rt.jar包中java\\lang\\String.class)，报错信息说没有main方法，就是因为加载的rt.jar包中的String类。这样可以敢保证对java核心源代码的保护，这就是沙箱安全机制 拓展在JVM中表示两个Class对象是否为同一个类存在的两个必要条件： 类的完成类名必须一致，包括包名。 家在这个类的ClassLoader (指ClassLoader实例对象) 必须相同 换句话说，在JVM中，即使两个类对象来源于同一个class文件，被用一个虚拟机所加载，但是只要加载他们的classLoader实例对象不同，那么他们两个类的对象也是不相等的 JVM必须知道一个类型是由启动加载器加载的还是由用户类加载器加载的。由用户类型加载器加载的，那么jvm会将这个类加载器的一个引用作为类型信息的一部分保存在方法区中。当解析一个类型到另一个类型的引用的时候，jvm需要保证这两个类型的类加载器是相同的。 类的主动使用和被动使用: 主动使用： a. 创建类的实例 b. 访问某个类或接口的静态变量，或者对该静态变量赋值 c. 调用类的静态方法 d. 反射 e. 初始化一个类的子类 f. Java虚拟机启动时被标明为启动类的类 g. jdk7开始提供的动态语言支持 除了这7种情况，其他使用java类的方式都被看作是对类的被动使用，都不会到之类的初始化 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:2:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#拓展"},{"categories":["java"],"content":"\r2 自动内存管理","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:0","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#2-自动内存管理"},{"categories":["java"],"content":"\r2.1 Java内存区域与内存溢出异常","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:1","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#21-java内存区域与内存溢出异常"},{"categories":["java"],"content":"\r2.1.1 运行时数据区JVM在执行java程序的时候把其管理的内存划分为不同的数据区域。（不同区域用途不同，创建时间、销毁时间也不相同） 所有线程共享区域：method area, heap 线程隔离的数据区：vm stack, native method stack, program counter register 程序计数器它是一块较小的内存空间，很多基础功能（分支、循环、跳转、异常处理、线程恢复等）都依赖program counter register。它是指示器，字节码解释器就是通过其值来选取下一条要执行的字节码指令。 虚拟机多线程是通过线程轮流切换、分配处理器执行时间的方式来实现的。我为了保证线程切换恢复到正确的执行位置，因此，每个线程都需要一个独立的program counter register，各自的pcr独立存储互不影响。 pcr只会在线程执行java方法的时候才会记录正在执行的虚拟机字节码指令地址。如果线程执行的是本地方法，那计数器值就是空（undefined）。 pcr的内存区域是为一个在虚拟机规范中没有任何OutOfMeoryError情况的区域。 Java虚拟机栈同pcr一样，java vm stack也是线程私有的。其生命周期与线程相同。它是java方法执行的内存模型（执行native method不会被创建）。每个方法被执行时，jvm会同步创建一个 Stack Frame（方法运行期重要的基础数据结构） 用于存储局部变量表、操作数栈、动态连接、方法出口等信息。一个方法从调用到执行完毕，对应着一个stack frame 栈帧在jvm中从出栈到入栈的过程。 java内存区不止笼统的heap和stack。与对象内存分配最密切的却是这两个部分。stack通常指的就是vm stack，或更多情况下只是vm stack中局部变量表部分。 局部变量槽： 栈中局部变量表存放了编译期各种基本数据类型、对象引用（reference类型，不等同于对象本身，可能是指向对象起始地址的引用指针，或是指向代表对象的句柄或与对象有关的位置）、returnAddress类型（指向了一条字节码指令的地址）。 这些数据类型在局部变量表中的存储空间以局部变量槽（slot）来表示，其中，64位的long和double数据会占用两个变量槽，其余数据占一个。 它所需的内存空间在编译期间完成分配，当进入一个方式时，其所需要在stack frame中分配多大的局部变量空间时完全确定的，方法运行期间不会改变局部变量表的大小（指的是变量槽的数量）。 vm stack 规定了两种异常，StackOverflowError（当线程请求的栈深度大于jvm所允许的深度时）、OutOfMemoryError（当 栈容量动态扩展（hotspot不支持动态扩展，所以不会报此异常）时无法申请到足够的内存时）。 本地方法栈native method stacks与vm stack二者作用相似，前者执行 native method服务（比如c/c++），后者执行字节码。 本地方法也会在栈深度溢出或栈扩展失败时爆出StackOverflowError、OutOfMemoryError。 Java Heap堆是jvm管理内存最大的一块，也是被所有线程共享的、jvm启动时被创建。其存在的唯一目的就是存放对象实例，几乎所有的对象实例都在此分配内存。 Java堆是垃圾收集器管理的内存区，也叫CG堆（Garbage Collected Heap）。随着垃圾收集器技术的进步，是否还分为新生代、老年代、永久代等就有待商榷了。 从内存分配角度看，所有线程共享堆中可以划分为分配缓冲区（Thread Local Allocation Buffer. TLAB），以提升对象分配效率。不管如何，堆中存储的只能是对象的实例。 Java堆被细分的目的只是为了更好地回收内存、更快地分配内存。 Java堆可以处于物理上不连续但逻辑上连续的内存空间中。它的大小可以固定可以扩展，主流jvm是可扩展（通过-Xmx -Xms设置），如果堆中没有内存完成实例分配且无法扩展时，会抛出OutOfMemoryError异常。 Method Area同Java堆一样是线程共享的内存区域，但它用来存储已经被加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据。别名非堆（non-heap），目的是与java堆区分开。 方法区并不等于永久代，只是HotSpot设计初把收集器分代扩展至方法区了。这种设计有它的好处，但更容易造成内存溢出。 垃圾收集行为在这个区域比较少出现，出现了后其内存回收目标主要是针对常量池的回收和对类型的卸载。 方法区无法按满足新内存分配需求会抛出OutOfMemoryError异常。 运行时常量池Runtime Constant Pool是Method Area的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项是常量池表（Constant Pool Table），用于存放编译期生成的个种字面量与符号引用，这部分内容会在类加载后存放到Method Area的Runtime Constant Pool。 Runtime Constant Pool具备动态性，因为常量不一定只有编译期才能产生，也就是说，不只是在Class文件预置入才会进入到运行时常量池，在运行期间也可以加入新的常量到池中。常见的就是String类的intern()方法。 当常量池无法再申请到内存会抛出OutOfMemoryError。 直接内存Direct Memory不是jvm运行时数据区的一部分，但是经常被使用到。NIO引入了一种基于Channel与Buffer的IO方式，可以直接使用Native函数库直接分配堆外内存，通过一个存储在Java堆里面的 DirectByteBuffer对象作为这块内存的引用进行操作。这样会提升性能、避免在Java堆和Native堆中来回复制数据。 Direct Memory的分配不受Java堆大小的限制，但是受限于总机内存。操作不当（虚拟机运行区内存+直接内存大于总机内存）会导致动态扩展时出现OutOfMemoryError。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#211-运行时数据区"},{"categories":["java"],"content":"\r2.1.1 运行时数据区JVM在执行java程序的时候把其管理的内存划分为不同的数据区域。（不同区域用途不同，创建时间、销毁时间也不相同） 所有线程共享区域：method area, heap 线程隔离的数据区：vm stack, native method stack, program counter register 程序计数器它是一块较小的内存空间，很多基础功能（分支、循环、跳转、异常处理、线程恢复等）都依赖program counter register。它是指示器，字节码解释器就是通过其值来选取下一条要执行的字节码指令。 虚拟机多线程是通过线程轮流切换、分配处理器执行时间的方式来实现的。我为了保证线程切换恢复到正确的执行位置，因此，每个线程都需要一个独立的program counter register，各自的pcr独立存储互不影响。 pcr只会在线程执行java方法的时候才会记录正在执行的虚拟机字节码指令地址。如果线程执行的是本地方法，那计数器值就是空（undefined）。 pcr的内存区域是为一个在虚拟机规范中没有任何OutOfMeoryError情况的区域。 Java虚拟机栈同pcr一样，java vm stack也是线程私有的。其生命周期与线程相同。它是java方法执行的内存模型（执行native method不会被创建）。每个方法被执行时，jvm会同步创建一个 Stack Frame（方法运行期重要的基础数据结构） 用于存储局部变量表、操作数栈、动态连接、方法出口等信息。一个方法从调用到执行完毕，对应着一个stack frame 栈帧在jvm中从出栈到入栈的过程。 java内存区不止笼统的heap和stack。与对象内存分配最密切的却是这两个部分。stack通常指的就是vm stack，或更多情况下只是vm stack中局部变量表部分。 局部变量槽： 栈中局部变量表存放了编译期各种基本数据类型、对象引用（reference类型，不等同于对象本身，可能是指向对象起始地址的引用指针，或是指向代表对象的句柄或与对象有关的位置）、returnAddress类型（指向了一条字节码指令的地址）。 这些数据类型在局部变量表中的存储空间以局部变量槽（slot）来表示，其中，64位的long和double数据会占用两个变量槽，其余数据占一个。 它所需的内存空间在编译期间完成分配，当进入一个方式时，其所需要在stack frame中分配多大的局部变量空间时完全确定的，方法运行期间不会改变局部变量表的大小（指的是变量槽的数量）。 vm stack 规定了两种异常，StackOverflowError（当线程请求的栈深度大于jvm所允许的深度时）、OutOfMemoryError（当 栈容量动态扩展（hotspot不支持动态扩展，所以不会报此异常）时无法申请到足够的内存时）。 本地方法栈native method stacks与vm stack二者作用相似，前者执行 native method服务（比如c/c++），后者执行字节码。 本地方法也会在栈深度溢出或栈扩展失败时爆出StackOverflowError、OutOfMemoryError。 Java Heap堆是jvm管理内存最大的一块，也是被所有线程共享的、jvm启动时被创建。其存在的唯一目的就是存放对象实例，几乎所有的对象实例都在此分配内存。 Java堆是垃圾收集器管理的内存区，也叫CG堆（Garbage Collected Heap）。随着垃圾收集器技术的进步，是否还分为新生代、老年代、永久代等就有待商榷了。 从内存分配角度看，所有线程共享堆中可以划分为分配缓冲区（Thread Local Allocation Buffer. TLAB），以提升对象分配效率。不管如何，堆中存储的只能是对象的实例。 Java堆被细分的目的只是为了更好地回收内存、更快地分配内存。 Java堆可以处于物理上不连续但逻辑上连续的内存空间中。它的大小可以固定可以扩展，主流jvm是可扩展（通过-Xmx -Xms设置），如果堆中没有内存完成实例分配且无法扩展时，会抛出OutOfMemoryError异常。 Method Area同Java堆一样是线程共享的内存区域，但它用来存储已经被加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据。别名非堆（non-heap），目的是与java堆区分开。 方法区并不等于永久代，只是HotSpot设计初把收集器分代扩展至方法区了。这种设计有它的好处，但更容易造成内存溢出。 垃圾收集行为在这个区域比较少出现，出现了后其内存回收目标主要是针对常量池的回收和对类型的卸载。 方法区无法按满足新内存分配需求会抛出OutOfMemoryError异常。 运行时常量池Runtime Constant Pool是Method Area的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项是常量池表（Constant Pool Table），用于存放编译期生成的个种字面量与符号引用，这部分内容会在类加载后存放到Method Area的Runtime Constant Pool。 Runtime Constant Pool具备动态性，因为常量不一定只有编译期才能产生，也就是说，不只是在Class文件预置入才会进入到运行时常量池，在运行期间也可以加入新的常量到池中。常见的就是String类的intern()方法。 当常量池无法再申请到内存会抛出OutOfMemoryError。 直接内存Direct Memory不是jvm运行时数据区的一部分，但是经常被使用到。NIO引入了一种基于Channel与Buffer的IO方式，可以直接使用Native函数库直接分配堆外内存，通过一个存储在Java堆里面的 DirectByteBuffer对象作为这块内存的引用进行操作。这样会提升性能、避免在Java堆和Native堆中来回复制数据。 Direct Memory的分配不受Java堆大小的限制，但是受限于总机内存。操作不当（虚拟机运行区内存+直接内存大于总机内存）会导致动态扩展时出现OutOfMemoryError。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#程序计数器"},{"categories":["java"],"content":"\r2.1.1 运行时数据区JVM在执行java程序的时候把其管理的内存划分为不同的数据区域。（不同区域用途不同，创建时间、销毁时间也不相同） 所有线程共享区域：method area, heap 线程隔离的数据区：vm stack, native method stack, program counter register 程序计数器它是一块较小的内存空间，很多基础功能（分支、循环、跳转、异常处理、线程恢复等）都依赖program counter register。它是指示器，字节码解释器就是通过其值来选取下一条要执行的字节码指令。 虚拟机多线程是通过线程轮流切换、分配处理器执行时间的方式来实现的。我为了保证线程切换恢复到正确的执行位置，因此，每个线程都需要一个独立的program counter register，各自的pcr独立存储互不影响。 pcr只会在线程执行java方法的时候才会记录正在执行的虚拟机字节码指令地址。如果线程执行的是本地方法，那计数器值就是空（undefined）。 pcr的内存区域是为一个在虚拟机规范中没有任何OutOfMeoryError情况的区域。 Java虚拟机栈同pcr一样，java vm stack也是线程私有的。其生命周期与线程相同。它是java方法执行的内存模型（执行native method不会被创建）。每个方法被执行时，jvm会同步创建一个 Stack Frame（方法运行期重要的基础数据结构） 用于存储局部变量表、操作数栈、动态连接、方法出口等信息。一个方法从调用到执行完毕，对应着一个stack frame 栈帧在jvm中从出栈到入栈的过程。 java内存区不止笼统的heap和stack。与对象内存分配最密切的却是这两个部分。stack通常指的就是vm stack，或更多情况下只是vm stack中局部变量表部分。 局部变量槽： 栈中局部变量表存放了编译期各种基本数据类型、对象引用（reference类型，不等同于对象本身，可能是指向对象起始地址的引用指针，或是指向代表对象的句柄或与对象有关的位置）、returnAddress类型（指向了一条字节码指令的地址）。 这些数据类型在局部变量表中的存储空间以局部变量槽（slot）来表示，其中，64位的long和double数据会占用两个变量槽，其余数据占一个。 它所需的内存空间在编译期间完成分配，当进入一个方式时，其所需要在stack frame中分配多大的局部变量空间时完全确定的，方法运行期间不会改变局部变量表的大小（指的是变量槽的数量）。 vm stack 规定了两种异常，StackOverflowError（当线程请求的栈深度大于jvm所允许的深度时）、OutOfMemoryError（当 栈容量动态扩展（hotspot不支持动态扩展，所以不会报此异常）时无法申请到足够的内存时）。 本地方法栈native method stacks与vm stack二者作用相似，前者执行 native method服务（比如c/c++），后者执行字节码。 本地方法也会在栈深度溢出或栈扩展失败时爆出StackOverflowError、OutOfMemoryError。 Java Heap堆是jvm管理内存最大的一块，也是被所有线程共享的、jvm启动时被创建。其存在的唯一目的就是存放对象实例，几乎所有的对象实例都在此分配内存。 Java堆是垃圾收集器管理的内存区，也叫CG堆（Garbage Collected Heap）。随着垃圾收集器技术的进步，是否还分为新生代、老年代、永久代等就有待商榷了。 从内存分配角度看，所有线程共享堆中可以划分为分配缓冲区（Thread Local Allocation Buffer. TLAB），以提升对象分配效率。不管如何，堆中存储的只能是对象的实例。 Java堆被细分的目的只是为了更好地回收内存、更快地分配内存。 Java堆可以处于物理上不连续但逻辑上连续的内存空间中。它的大小可以固定可以扩展，主流jvm是可扩展（通过-Xmx -Xms设置），如果堆中没有内存完成实例分配且无法扩展时，会抛出OutOfMemoryError异常。 Method Area同Java堆一样是线程共享的内存区域，但它用来存储已经被加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据。别名非堆（non-heap），目的是与java堆区分开。 方法区并不等于永久代，只是HotSpot设计初把收集器分代扩展至方法区了。这种设计有它的好处，但更容易造成内存溢出。 垃圾收集行为在这个区域比较少出现，出现了后其内存回收目标主要是针对常量池的回收和对类型的卸载。 方法区无法按满足新内存分配需求会抛出OutOfMemoryError异常。 运行时常量池Runtime Constant Pool是Method Area的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项是常量池表（Constant Pool Table），用于存放编译期生成的个种字面量与符号引用，这部分内容会在类加载后存放到Method Area的Runtime Constant Pool。 Runtime Constant Pool具备动态性，因为常量不一定只有编译期才能产生，也就是说，不只是在Class文件预置入才会进入到运行时常量池，在运行期间也可以加入新的常量到池中。常见的就是String类的intern()方法。 当常量池无法再申请到内存会抛出OutOfMemoryError。 直接内存Direct Memory不是jvm运行时数据区的一部分，但是经常被使用到。NIO引入了一种基于Channel与Buffer的IO方式，可以直接使用Native函数库直接分配堆外内存，通过一个存储在Java堆里面的 DirectByteBuffer对象作为这块内存的引用进行操作。这样会提升性能、避免在Java堆和Native堆中来回复制数据。 Direct Memory的分配不受Java堆大小的限制，但是受限于总机内存。操作不当（虚拟机运行区内存+直接内存大于总机内存）会导致动态扩展时出现OutOfMemoryError。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#java虚拟机栈"},{"categories":["java"],"content":"\r2.1.1 运行时数据区JVM在执行java程序的时候把其管理的内存划分为不同的数据区域。（不同区域用途不同，创建时间、销毁时间也不相同） 所有线程共享区域：method area, heap 线程隔离的数据区：vm stack, native method stack, program counter register 程序计数器它是一块较小的内存空间，很多基础功能（分支、循环、跳转、异常处理、线程恢复等）都依赖program counter register。它是指示器，字节码解释器就是通过其值来选取下一条要执行的字节码指令。 虚拟机多线程是通过线程轮流切换、分配处理器执行时间的方式来实现的。我为了保证线程切换恢复到正确的执行位置，因此，每个线程都需要一个独立的program counter register，各自的pcr独立存储互不影响。 pcr只会在线程执行java方法的时候才会记录正在执行的虚拟机字节码指令地址。如果线程执行的是本地方法，那计数器值就是空（undefined）。 pcr的内存区域是为一个在虚拟机规范中没有任何OutOfMeoryError情况的区域。 Java虚拟机栈同pcr一样，java vm stack也是线程私有的。其生命周期与线程相同。它是java方法执行的内存模型（执行native method不会被创建）。每个方法被执行时，jvm会同步创建一个 Stack Frame（方法运行期重要的基础数据结构） 用于存储局部变量表、操作数栈、动态连接、方法出口等信息。一个方法从调用到执行完毕，对应着一个stack frame 栈帧在jvm中从出栈到入栈的过程。 java内存区不止笼统的heap和stack。与对象内存分配最密切的却是这两个部分。stack通常指的就是vm stack，或更多情况下只是vm stack中局部变量表部分。 局部变量槽： 栈中局部变量表存放了编译期各种基本数据类型、对象引用（reference类型，不等同于对象本身，可能是指向对象起始地址的引用指针，或是指向代表对象的句柄或与对象有关的位置）、returnAddress类型（指向了一条字节码指令的地址）。 这些数据类型在局部变量表中的存储空间以局部变量槽（slot）来表示，其中，64位的long和double数据会占用两个变量槽，其余数据占一个。 它所需的内存空间在编译期间完成分配，当进入一个方式时，其所需要在stack frame中分配多大的局部变量空间时完全确定的，方法运行期间不会改变局部变量表的大小（指的是变量槽的数量）。 vm stack 规定了两种异常，StackOverflowError（当线程请求的栈深度大于jvm所允许的深度时）、OutOfMemoryError（当 栈容量动态扩展（hotspot不支持动态扩展，所以不会报此异常）时无法申请到足够的内存时）。 本地方法栈native method stacks与vm stack二者作用相似，前者执行 native method服务（比如c/c++），后者执行字节码。 本地方法也会在栈深度溢出或栈扩展失败时爆出StackOverflowError、OutOfMemoryError。 Java Heap堆是jvm管理内存最大的一块，也是被所有线程共享的、jvm启动时被创建。其存在的唯一目的就是存放对象实例，几乎所有的对象实例都在此分配内存。 Java堆是垃圾收集器管理的内存区，也叫CG堆（Garbage Collected Heap）。随着垃圾收集器技术的进步，是否还分为新生代、老年代、永久代等就有待商榷了。 从内存分配角度看，所有线程共享堆中可以划分为分配缓冲区（Thread Local Allocation Buffer. TLAB），以提升对象分配效率。不管如何，堆中存储的只能是对象的实例。 Java堆被细分的目的只是为了更好地回收内存、更快地分配内存。 Java堆可以处于物理上不连续但逻辑上连续的内存空间中。它的大小可以固定可以扩展，主流jvm是可扩展（通过-Xmx -Xms设置），如果堆中没有内存完成实例分配且无法扩展时，会抛出OutOfMemoryError异常。 Method Area同Java堆一样是线程共享的内存区域，但它用来存储已经被加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据。别名非堆（non-heap），目的是与java堆区分开。 方法区并不等于永久代，只是HotSpot设计初把收集器分代扩展至方法区了。这种设计有它的好处，但更容易造成内存溢出。 垃圾收集行为在这个区域比较少出现，出现了后其内存回收目标主要是针对常量池的回收和对类型的卸载。 方法区无法按满足新内存分配需求会抛出OutOfMemoryError异常。 运行时常量池Runtime Constant Pool是Method Area的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项是常量池表（Constant Pool Table），用于存放编译期生成的个种字面量与符号引用，这部分内容会在类加载后存放到Method Area的Runtime Constant Pool。 Runtime Constant Pool具备动态性，因为常量不一定只有编译期才能产生，也就是说，不只是在Class文件预置入才会进入到运行时常量池，在运行期间也可以加入新的常量到池中。常见的就是String类的intern()方法。 当常量池无法再申请到内存会抛出OutOfMemoryError。 直接内存Direct Memory不是jvm运行时数据区的一部分，但是经常被使用到。NIO引入了一种基于Channel与Buffer的IO方式，可以直接使用Native函数库直接分配堆外内存，通过一个存储在Java堆里面的 DirectByteBuffer对象作为这块内存的引用进行操作。这样会提升性能、避免在Java堆和Native堆中来回复制数据。 Direct Memory的分配不受Java堆大小的限制，但是受限于总机内存。操作不当（虚拟机运行区内存+直接内存大于总机内存）会导致动态扩展时出现OutOfMemoryError。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#本地方法栈"},{"categories":["java"],"content":"\r2.1.1 运行时数据区JVM在执行java程序的时候把其管理的内存划分为不同的数据区域。（不同区域用途不同，创建时间、销毁时间也不相同） 所有线程共享区域：method area, heap 线程隔离的数据区：vm stack, native method stack, program counter register 程序计数器它是一块较小的内存空间，很多基础功能（分支、循环、跳转、异常处理、线程恢复等）都依赖program counter register。它是指示器，字节码解释器就是通过其值来选取下一条要执行的字节码指令。 虚拟机多线程是通过线程轮流切换、分配处理器执行时间的方式来实现的。我为了保证线程切换恢复到正确的执行位置，因此，每个线程都需要一个独立的program counter register，各自的pcr独立存储互不影响。 pcr只会在线程执行java方法的时候才会记录正在执行的虚拟机字节码指令地址。如果线程执行的是本地方法，那计数器值就是空（undefined）。 pcr的内存区域是为一个在虚拟机规范中没有任何OutOfMeoryError情况的区域。 Java虚拟机栈同pcr一样，java vm stack也是线程私有的。其生命周期与线程相同。它是java方法执行的内存模型（执行native method不会被创建）。每个方法被执行时，jvm会同步创建一个 Stack Frame（方法运行期重要的基础数据结构） 用于存储局部变量表、操作数栈、动态连接、方法出口等信息。一个方法从调用到执行完毕，对应着一个stack frame 栈帧在jvm中从出栈到入栈的过程。 java内存区不止笼统的heap和stack。与对象内存分配最密切的却是这两个部分。stack通常指的就是vm stack，或更多情况下只是vm stack中局部变量表部分。 局部变量槽： 栈中局部变量表存放了编译期各种基本数据类型、对象引用（reference类型，不等同于对象本身，可能是指向对象起始地址的引用指针，或是指向代表对象的句柄或与对象有关的位置）、returnAddress类型（指向了一条字节码指令的地址）。 这些数据类型在局部变量表中的存储空间以局部变量槽（slot）来表示，其中，64位的long和double数据会占用两个变量槽，其余数据占一个。 它所需的内存空间在编译期间完成分配，当进入一个方式时，其所需要在stack frame中分配多大的局部变量空间时完全确定的，方法运行期间不会改变局部变量表的大小（指的是变量槽的数量）。 vm stack 规定了两种异常，StackOverflowError（当线程请求的栈深度大于jvm所允许的深度时）、OutOfMemoryError（当 栈容量动态扩展（hotspot不支持动态扩展，所以不会报此异常）时无法申请到足够的内存时）。 本地方法栈native method stacks与vm stack二者作用相似，前者执行 native method服务（比如c/c++），后者执行字节码。 本地方法也会在栈深度溢出或栈扩展失败时爆出StackOverflowError、OutOfMemoryError。 Java Heap堆是jvm管理内存最大的一块，也是被所有线程共享的、jvm启动时被创建。其存在的唯一目的就是存放对象实例，几乎所有的对象实例都在此分配内存。 Java堆是垃圾收集器管理的内存区，也叫CG堆（Garbage Collected Heap）。随着垃圾收集器技术的进步，是否还分为新生代、老年代、永久代等就有待商榷了。 从内存分配角度看，所有线程共享堆中可以划分为分配缓冲区（Thread Local Allocation Buffer. TLAB），以提升对象分配效率。不管如何，堆中存储的只能是对象的实例。 Java堆被细分的目的只是为了更好地回收内存、更快地分配内存。 Java堆可以处于物理上不连续但逻辑上连续的内存空间中。它的大小可以固定可以扩展，主流jvm是可扩展（通过-Xmx -Xms设置），如果堆中没有内存完成实例分配且无法扩展时，会抛出OutOfMemoryError异常。 Method Area同Java堆一样是线程共享的内存区域，但它用来存储已经被加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据。别名非堆（non-heap），目的是与java堆区分开。 方法区并不等于永久代，只是HotSpot设计初把收集器分代扩展至方法区了。这种设计有它的好处，但更容易造成内存溢出。 垃圾收集行为在这个区域比较少出现，出现了后其内存回收目标主要是针对常量池的回收和对类型的卸载。 方法区无法按满足新内存分配需求会抛出OutOfMemoryError异常。 运行时常量池Runtime Constant Pool是Method Area的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项是常量池表（Constant Pool Table），用于存放编译期生成的个种字面量与符号引用，这部分内容会在类加载后存放到Method Area的Runtime Constant Pool。 Runtime Constant Pool具备动态性，因为常量不一定只有编译期才能产生，也就是说，不只是在Class文件预置入才会进入到运行时常量池，在运行期间也可以加入新的常量到池中。常见的就是String类的intern()方法。 当常量池无法再申请到内存会抛出OutOfMemoryError。 直接内存Direct Memory不是jvm运行时数据区的一部分，但是经常被使用到。NIO引入了一种基于Channel与Buffer的IO方式，可以直接使用Native函数库直接分配堆外内存，通过一个存储在Java堆里面的 DirectByteBuffer对象作为这块内存的引用进行操作。这样会提升性能、避免在Java堆和Native堆中来回复制数据。 Direct Memory的分配不受Java堆大小的限制，但是受限于总机内存。操作不当（虚拟机运行区内存+直接内存大于总机内存）会导致动态扩展时出现OutOfMemoryError。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#java-heap"},{"categories":["java"],"content":"\r2.1.1 运行时数据区JVM在执行java程序的时候把其管理的内存划分为不同的数据区域。（不同区域用途不同，创建时间、销毁时间也不相同） 所有线程共享区域：method area, heap 线程隔离的数据区：vm stack, native method stack, program counter register 程序计数器它是一块较小的内存空间，很多基础功能（分支、循环、跳转、异常处理、线程恢复等）都依赖program counter register。它是指示器，字节码解释器就是通过其值来选取下一条要执行的字节码指令。 虚拟机多线程是通过线程轮流切换、分配处理器执行时间的方式来实现的。我为了保证线程切换恢复到正确的执行位置，因此，每个线程都需要一个独立的program counter register，各自的pcr独立存储互不影响。 pcr只会在线程执行java方法的时候才会记录正在执行的虚拟机字节码指令地址。如果线程执行的是本地方法，那计数器值就是空（undefined）。 pcr的内存区域是为一个在虚拟机规范中没有任何OutOfMeoryError情况的区域。 Java虚拟机栈同pcr一样，java vm stack也是线程私有的。其生命周期与线程相同。它是java方法执行的内存模型（执行native method不会被创建）。每个方法被执行时，jvm会同步创建一个 Stack Frame（方法运行期重要的基础数据结构） 用于存储局部变量表、操作数栈、动态连接、方法出口等信息。一个方法从调用到执行完毕，对应着一个stack frame 栈帧在jvm中从出栈到入栈的过程。 java内存区不止笼统的heap和stack。与对象内存分配最密切的却是这两个部分。stack通常指的就是vm stack，或更多情况下只是vm stack中局部变量表部分。 局部变量槽： 栈中局部变量表存放了编译期各种基本数据类型、对象引用（reference类型，不等同于对象本身，可能是指向对象起始地址的引用指针，或是指向代表对象的句柄或与对象有关的位置）、returnAddress类型（指向了一条字节码指令的地址）。 这些数据类型在局部变量表中的存储空间以局部变量槽（slot）来表示，其中，64位的long和double数据会占用两个变量槽，其余数据占一个。 它所需的内存空间在编译期间完成分配，当进入一个方式时，其所需要在stack frame中分配多大的局部变量空间时完全确定的，方法运行期间不会改变局部变量表的大小（指的是变量槽的数量）。 vm stack 规定了两种异常，StackOverflowError（当线程请求的栈深度大于jvm所允许的深度时）、OutOfMemoryError（当 栈容量动态扩展（hotspot不支持动态扩展，所以不会报此异常）时无法申请到足够的内存时）。 本地方法栈native method stacks与vm stack二者作用相似，前者执行 native method服务（比如c/c++），后者执行字节码。 本地方法也会在栈深度溢出或栈扩展失败时爆出StackOverflowError、OutOfMemoryError。 Java Heap堆是jvm管理内存最大的一块，也是被所有线程共享的、jvm启动时被创建。其存在的唯一目的就是存放对象实例，几乎所有的对象实例都在此分配内存。 Java堆是垃圾收集器管理的内存区，也叫CG堆（Garbage Collected Heap）。随着垃圾收集器技术的进步，是否还分为新生代、老年代、永久代等就有待商榷了。 从内存分配角度看，所有线程共享堆中可以划分为分配缓冲区（Thread Local Allocation Buffer. TLAB），以提升对象分配效率。不管如何，堆中存储的只能是对象的实例。 Java堆被细分的目的只是为了更好地回收内存、更快地分配内存。 Java堆可以处于物理上不连续但逻辑上连续的内存空间中。它的大小可以固定可以扩展，主流jvm是可扩展（通过-Xmx -Xms设置），如果堆中没有内存完成实例分配且无法扩展时，会抛出OutOfMemoryError异常。 Method Area同Java堆一样是线程共享的内存区域，但它用来存储已经被加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据。别名非堆（non-heap），目的是与java堆区分开。 方法区并不等于永久代，只是HotSpot设计初把收集器分代扩展至方法区了。这种设计有它的好处，但更容易造成内存溢出。 垃圾收集行为在这个区域比较少出现，出现了后其内存回收目标主要是针对常量池的回收和对类型的卸载。 方法区无法按满足新内存分配需求会抛出OutOfMemoryError异常。 运行时常量池Runtime Constant Pool是Method Area的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项是常量池表（Constant Pool Table），用于存放编译期生成的个种字面量与符号引用，这部分内容会在类加载后存放到Method Area的Runtime Constant Pool。 Runtime Constant Pool具备动态性，因为常量不一定只有编译期才能产生，也就是说，不只是在Class文件预置入才会进入到运行时常量池，在运行期间也可以加入新的常量到池中。常见的就是String类的intern()方法。 当常量池无法再申请到内存会抛出OutOfMemoryError。 直接内存Direct Memory不是jvm运行时数据区的一部分，但是经常被使用到。NIO引入了一种基于Channel与Buffer的IO方式，可以直接使用Native函数库直接分配堆外内存，通过一个存储在Java堆里面的 DirectByteBuffer对象作为这块内存的引用进行操作。这样会提升性能、避免在Java堆和Native堆中来回复制数据。 Direct Memory的分配不受Java堆大小的限制，但是受限于总机内存。操作不当（虚拟机运行区内存+直接内存大于总机内存）会导致动态扩展时出现OutOfMemoryError。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#method-area"},{"categories":["java"],"content":"\r2.1.1 运行时数据区JVM在执行java程序的时候把其管理的内存划分为不同的数据区域。（不同区域用途不同，创建时间、销毁时间也不相同） 所有线程共享区域：method area, heap 线程隔离的数据区：vm stack, native method stack, program counter register 程序计数器它是一块较小的内存空间，很多基础功能（分支、循环、跳转、异常处理、线程恢复等）都依赖program counter register。它是指示器，字节码解释器就是通过其值来选取下一条要执行的字节码指令。 虚拟机多线程是通过线程轮流切换、分配处理器执行时间的方式来实现的。我为了保证线程切换恢复到正确的执行位置，因此，每个线程都需要一个独立的program counter register，各自的pcr独立存储互不影响。 pcr只会在线程执行java方法的时候才会记录正在执行的虚拟机字节码指令地址。如果线程执行的是本地方法，那计数器值就是空（undefined）。 pcr的内存区域是为一个在虚拟机规范中没有任何OutOfMeoryError情况的区域。 Java虚拟机栈同pcr一样，java vm stack也是线程私有的。其生命周期与线程相同。它是java方法执行的内存模型（执行native method不会被创建）。每个方法被执行时，jvm会同步创建一个 Stack Frame（方法运行期重要的基础数据结构） 用于存储局部变量表、操作数栈、动态连接、方法出口等信息。一个方法从调用到执行完毕，对应着一个stack frame 栈帧在jvm中从出栈到入栈的过程。 java内存区不止笼统的heap和stack。与对象内存分配最密切的却是这两个部分。stack通常指的就是vm stack，或更多情况下只是vm stack中局部变量表部分。 局部变量槽： 栈中局部变量表存放了编译期各种基本数据类型、对象引用（reference类型，不等同于对象本身，可能是指向对象起始地址的引用指针，或是指向代表对象的句柄或与对象有关的位置）、returnAddress类型（指向了一条字节码指令的地址）。 这些数据类型在局部变量表中的存储空间以局部变量槽（slot）来表示，其中，64位的long和double数据会占用两个变量槽，其余数据占一个。 它所需的内存空间在编译期间完成分配，当进入一个方式时，其所需要在stack frame中分配多大的局部变量空间时完全确定的，方法运行期间不会改变局部变量表的大小（指的是变量槽的数量）。 vm stack 规定了两种异常，StackOverflowError（当线程请求的栈深度大于jvm所允许的深度时）、OutOfMemoryError（当 栈容量动态扩展（hotspot不支持动态扩展，所以不会报此异常）时无法申请到足够的内存时）。 本地方法栈native method stacks与vm stack二者作用相似，前者执行 native method服务（比如c/c++），后者执行字节码。 本地方法也会在栈深度溢出或栈扩展失败时爆出StackOverflowError、OutOfMemoryError。 Java Heap堆是jvm管理内存最大的一块，也是被所有线程共享的、jvm启动时被创建。其存在的唯一目的就是存放对象实例，几乎所有的对象实例都在此分配内存。 Java堆是垃圾收集器管理的内存区，也叫CG堆（Garbage Collected Heap）。随着垃圾收集器技术的进步，是否还分为新生代、老年代、永久代等就有待商榷了。 从内存分配角度看，所有线程共享堆中可以划分为分配缓冲区（Thread Local Allocation Buffer. TLAB），以提升对象分配效率。不管如何，堆中存储的只能是对象的实例。 Java堆被细分的目的只是为了更好地回收内存、更快地分配内存。 Java堆可以处于物理上不连续但逻辑上连续的内存空间中。它的大小可以固定可以扩展，主流jvm是可扩展（通过-Xmx -Xms设置），如果堆中没有内存完成实例分配且无法扩展时，会抛出OutOfMemoryError异常。 Method Area同Java堆一样是线程共享的内存区域，但它用来存储已经被加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据。别名非堆（non-heap），目的是与java堆区分开。 方法区并不等于永久代，只是HotSpot设计初把收集器分代扩展至方法区了。这种设计有它的好处，但更容易造成内存溢出。 垃圾收集行为在这个区域比较少出现，出现了后其内存回收目标主要是针对常量池的回收和对类型的卸载。 方法区无法按满足新内存分配需求会抛出OutOfMemoryError异常。 运行时常量池Runtime Constant Pool是Method Area的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项是常量池表（Constant Pool Table），用于存放编译期生成的个种字面量与符号引用，这部分内容会在类加载后存放到Method Area的Runtime Constant Pool。 Runtime Constant Pool具备动态性，因为常量不一定只有编译期才能产生，也就是说，不只是在Class文件预置入才会进入到运行时常量池，在运行期间也可以加入新的常量到池中。常见的就是String类的intern()方法。 当常量池无法再申请到内存会抛出OutOfMemoryError。 直接内存Direct Memory不是jvm运行时数据区的一部分，但是经常被使用到。NIO引入了一种基于Channel与Buffer的IO方式，可以直接使用Native函数库直接分配堆外内存，通过一个存储在Java堆里面的 DirectByteBuffer对象作为这块内存的引用进行操作。这样会提升性能、避免在Java堆和Native堆中来回复制数据。 Direct Memory的分配不受Java堆大小的限制，但是受限于总机内存。操作不当（虚拟机运行区内存+直接内存大于总机内存）会导致动态扩展时出现OutOfMemoryError。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#运行时常量池"},{"categories":["java"],"content":"\r2.1.1 运行时数据区JVM在执行java程序的时候把其管理的内存划分为不同的数据区域。（不同区域用途不同，创建时间、销毁时间也不相同） 所有线程共享区域：method area, heap 线程隔离的数据区：vm stack, native method stack, program counter register 程序计数器它是一块较小的内存空间，很多基础功能（分支、循环、跳转、异常处理、线程恢复等）都依赖program counter register。它是指示器，字节码解释器就是通过其值来选取下一条要执行的字节码指令。 虚拟机多线程是通过线程轮流切换、分配处理器执行时间的方式来实现的。我为了保证线程切换恢复到正确的执行位置，因此，每个线程都需要一个独立的program counter register，各自的pcr独立存储互不影响。 pcr只会在线程执行java方法的时候才会记录正在执行的虚拟机字节码指令地址。如果线程执行的是本地方法，那计数器值就是空（undefined）。 pcr的内存区域是为一个在虚拟机规范中没有任何OutOfMeoryError情况的区域。 Java虚拟机栈同pcr一样，java vm stack也是线程私有的。其生命周期与线程相同。它是java方法执行的内存模型（执行native method不会被创建）。每个方法被执行时，jvm会同步创建一个 Stack Frame（方法运行期重要的基础数据结构） 用于存储局部变量表、操作数栈、动态连接、方法出口等信息。一个方法从调用到执行完毕，对应着一个stack frame 栈帧在jvm中从出栈到入栈的过程。 java内存区不止笼统的heap和stack。与对象内存分配最密切的却是这两个部分。stack通常指的就是vm stack，或更多情况下只是vm stack中局部变量表部分。 局部变量槽： 栈中局部变量表存放了编译期各种基本数据类型、对象引用（reference类型，不等同于对象本身，可能是指向对象起始地址的引用指针，或是指向代表对象的句柄或与对象有关的位置）、returnAddress类型（指向了一条字节码指令的地址）。 这些数据类型在局部变量表中的存储空间以局部变量槽（slot）来表示，其中，64位的long和double数据会占用两个变量槽，其余数据占一个。 它所需的内存空间在编译期间完成分配，当进入一个方式时，其所需要在stack frame中分配多大的局部变量空间时完全确定的，方法运行期间不会改变局部变量表的大小（指的是变量槽的数量）。 vm stack 规定了两种异常，StackOverflowError（当线程请求的栈深度大于jvm所允许的深度时）、OutOfMemoryError（当 栈容量动态扩展（hotspot不支持动态扩展，所以不会报此异常）时无法申请到足够的内存时）。 本地方法栈native method stacks与vm stack二者作用相似，前者执行 native method服务（比如c/c++），后者执行字节码。 本地方法也会在栈深度溢出或栈扩展失败时爆出StackOverflowError、OutOfMemoryError。 Java Heap堆是jvm管理内存最大的一块，也是被所有线程共享的、jvm启动时被创建。其存在的唯一目的就是存放对象实例，几乎所有的对象实例都在此分配内存。 Java堆是垃圾收集器管理的内存区，也叫CG堆（Garbage Collected Heap）。随着垃圾收集器技术的进步，是否还分为新生代、老年代、永久代等就有待商榷了。 从内存分配角度看，所有线程共享堆中可以划分为分配缓冲区（Thread Local Allocation Buffer. TLAB），以提升对象分配效率。不管如何，堆中存储的只能是对象的实例。 Java堆被细分的目的只是为了更好地回收内存、更快地分配内存。 Java堆可以处于物理上不连续但逻辑上连续的内存空间中。它的大小可以固定可以扩展，主流jvm是可扩展（通过-Xmx -Xms设置），如果堆中没有内存完成实例分配且无法扩展时，会抛出OutOfMemoryError异常。 Method Area同Java堆一样是线程共享的内存区域，但它用来存储已经被加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据。别名非堆（non-heap），目的是与java堆区分开。 方法区并不等于永久代，只是HotSpot设计初把收集器分代扩展至方法区了。这种设计有它的好处，但更容易造成内存溢出。 垃圾收集行为在这个区域比较少出现，出现了后其内存回收目标主要是针对常量池的回收和对类型的卸载。 方法区无法按满足新内存分配需求会抛出OutOfMemoryError异常。 运行时常量池Runtime Constant Pool是Method Area的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项是常量池表（Constant Pool Table），用于存放编译期生成的个种字面量与符号引用，这部分内容会在类加载后存放到Method Area的Runtime Constant Pool。 Runtime Constant Pool具备动态性，因为常量不一定只有编译期才能产生，也就是说，不只是在Class文件预置入才会进入到运行时常量池，在运行期间也可以加入新的常量到池中。常见的就是String类的intern()方法。 当常量池无法再申请到内存会抛出OutOfMemoryError。 直接内存Direct Memory不是jvm运行时数据区的一部分，但是经常被使用到。NIO引入了一种基于Channel与Buffer的IO方式，可以直接使用Native函数库直接分配堆外内存，通过一个存储在Java堆里面的 DirectByteBuffer对象作为这块内存的引用进行操作。这样会提升性能、避免在Java堆和Native堆中来回复制数据。 Direct Memory的分配不受Java堆大小的限制，但是受限于总机内存。操作不当（虚拟机运行区内存+直接内存大于总机内存）会导致动态扩展时出现OutOfMemoryError。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#直接内存"},{"categories":["java"],"content":"\r2.1.2 HotSpot虚拟机对象探秘\r对象的创建 Jvm接收到字节码new指令时，首先检查该指令的参数是否能在常量池中找到类的符号引用，并且检查该符号引用代表的类是否被加载、解析和初始化过。如果没有，就先执行相应的类加载过程。 类加载检查通过后，jvm给新生对象分配内存，其内存所需的大小在类加载完成后便可确定，而为对象分配空间的任务实际上就是把一块确定大小的内存从Java堆中划分出来。分配方式有Java堆是否规整决定，而Java堆的规整又由采用的垃圾收集器是否带有空间压缩整理（Compact）的能力决定。 假设Java堆中内存绝对规整，所有使用过的内存在一边，空闲内存在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间方向挪动一段与对象大小相等的距离，这种分配方式称为指针碰撞（Bump The Pointer）。 假设Java堆中内存不规整，已使用内存和未使用内存相互交错在一起，就无法简单地指针碰撞，jvm必须维护一个列表，上面记录哪些内存块可用，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式称为空闲列表（Free List） 对象在jvm的创建是复杂的行为，仅仅修改指针指向的位置，在并发下会存在线程安全问题，如给对象A分配内存，指针还未来得及修改，对象B同时使用了原来的指针来分配内存的情况。 解决方法1：对分配内存空间的动作进行同步处理，JVM采用的是CAS配上失败重试的方式保证更新操作的原子性。 解决方法2：把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在java堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer,TLAB），不同线程分配内存，就在其对应的的本地缓冲区中分配，只有本地缓冲区用完了，分配新的缓存区时才需要同步锁定。否使用了TLAB的命令：-XX: +/-UseTLAB参数。 内存分配完之后，jvm必须将分配的内存空间（不包括对象头）都初始化为零值。如果使用了TLAB，这个操作也可以提前至TLAB分配时进行。这是为了保证对象的实例字段在java代码中可以不赋初始值就直接使用，使程序能够访问到这些字段的数据类型所对应的零值。 jvm还要对对象进行必要的设置，如确定该对象是哪个类的实例、如何找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。这些信息会存在对象头（Object header）中，根据jvm当前运行状态的不同，对象头会有不用的设置。 jvm角度来看，新的对象就产生了。但从java程序视角，对象创建才刚刚开始——构造函数，即Class文件中的\u003cinit\u003e()方法还没有执行，所有字段都是默认的零值。对象所需要其他资源和状态信息也都没有构造好。 对象的内存布局HotSpot中，对象在堆内存的存储布局：对象头 Header、实例数据 Instance Data、对齐填充 Padding。 Header：一部分存储对象运行时数据，如HashCode、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，称为Mark Word。它是一个动态定义的数据结构，以便在极小的空间存储尽量多的数据。 Header：另一部分存储类型指针，即对象指向它的类型元数据的指针，jvm通过指针确定对象是哪个类的实例。并不是所有jvm的实现都必须在对象数据保留类型指针，换句话说，查找对象的元数据不一定要经过对象本身。如果对象是数组，那么对象头必须有一块记录数组长度的数据，jvm通过元数据来确定对象的大小，因此长度是要确定的。 Instance Data：存储着对象真正存储的有效信息，即代码程序中定义的各类型字段内容，无论从父类继承还是子类中定义的字段都必须记录下来。这部分存储顺序收到虚拟机分配策略参数（-XX: fieldsAllocationStyle参数）和字段在java源码中定义顺序的影响。 Padding：这不是必然存在的，也没有特别含义，仅仅起着占位符的作用。HotSpot自动内存管理系统要求对象起始地址必须是8字节的整数倍，，对象头部分是被设计过正好是8字节的倍数，而对象实例数据部分如果没对齐，就需要其来补全。 对象的访问定位Java程序通过栈上的reference数据来操作堆上的具体对象。主流对象访问方式为句柄和直接指针两种： 句柄访问，Java堆划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，句柄包含了对象实例数据（存储在堆中的实例池）与类型数据（存储在方法区）各自的地址信息。优势：稳定句柄地址，对象被移动时，只会改变句柄中的实例数据指针，而reference本身不需要修改。 直接指针，Java堆中对象的内存布局必须要考虑如何访问类型数据的相关信息，reference中存储的直接就是对象地址。优势：速度快、节省一次指针定位的开销（因为它直接存储的对象实例地址，不需要存储对象实例地址的指针），这部分时间开销是非常可观的。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:3","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#212-hotspot虚拟机对象探秘"},{"categories":["java"],"content":"\r2.1.2 HotSpot虚拟机对象探秘\r对象的创建 Jvm接收到字节码new指令时，首先检查该指令的参数是否能在常量池中找到类的符号引用，并且检查该符号引用代表的类是否被加载、解析和初始化过。如果没有，就先执行相应的类加载过程。 类加载检查通过后，jvm给新生对象分配内存，其内存所需的大小在类加载完成后便可确定，而为对象分配空间的任务实际上就是把一块确定大小的内存从Java堆中划分出来。分配方式有Java堆是否规整决定，而Java堆的规整又由采用的垃圾收集器是否带有空间压缩整理（Compact）的能力决定。 假设Java堆中内存绝对规整，所有使用过的内存在一边，空闲内存在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间方向挪动一段与对象大小相等的距离，这种分配方式称为指针碰撞（Bump The Pointer）。 假设Java堆中内存不规整，已使用内存和未使用内存相互交错在一起，就无法简单地指针碰撞，jvm必须维护一个列表，上面记录哪些内存块可用，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式称为空闲列表（Free List） 对象在jvm的创建是复杂的行为，仅仅修改指针指向的位置，在并发下会存在线程安全问题，如给对象A分配内存，指针还未来得及修改，对象B同时使用了原来的指针来分配内存的情况。 解决方法1：对分配内存空间的动作进行同步处理，JVM采用的是CAS配上失败重试的方式保证更新操作的原子性。 解决方法2：把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在java堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer,TLAB），不同线程分配内存，就在其对应的的本地缓冲区中分配，只有本地缓冲区用完了，分配新的缓存区时才需要同步锁定。否使用了TLAB的命令：-XX: +/-UseTLAB参数。 内存分配完之后，jvm必须将分配的内存空间（不包括对象头）都初始化为零值。如果使用了TLAB，这个操作也可以提前至TLAB分配时进行。这是为了保证对象的实例字段在java代码中可以不赋初始值就直接使用，使程序能够访问到这些字段的数据类型所对应的零值。 jvm还要对对象进行必要的设置，如确定该对象是哪个类的实例、如何找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。这些信息会存在对象头（Object header）中，根据jvm当前运行状态的不同，对象头会有不用的设置。 jvm角度来看，新的对象就产生了。但从java程序视角，对象创建才刚刚开始——构造函数，即Class文件中的()方法还没有执行，所有字段都是默认的零值。对象所需要其他资源和状态信息也都没有构造好。 对象的内存布局HotSpot中，对象在堆内存的存储布局：对象头 Header、实例数据 Instance Data、对齐填充 Padding。 Header：一部分存储对象运行时数据，如HashCode、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，称为Mark Word。它是一个动态定义的数据结构，以便在极小的空间存储尽量多的数据。 Header：另一部分存储类型指针，即对象指向它的类型元数据的指针，jvm通过指针确定对象是哪个类的实例。并不是所有jvm的实现都必须在对象数据保留类型指针，换句话说，查找对象的元数据不一定要经过对象本身。如果对象是数组，那么对象头必须有一块记录数组长度的数据，jvm通过元数据来确定对象的大小，因此长度是要确定的。 Instance Data：存储着对象真正存储的有效信息，即代码程序中定义的各类型字段内容，无论从父类继承还是子类中定义的字段都必须记录下来。这部分存储顺序收到虚拟机分配策略参数（-XX: fieldsAllocationStyle参数）和字段在java源码中定义顺序的影响。 Padding：这不是必然存在的，也没有特别含义，仅仅起着占位符的作用。HotSpot自动内存管理系统要求对象起始地址必须是8字节的整数倍，，对象头部分是被设计过正好是8字节的倍数，而对象实例数据部分如果没对齐，就需要其来补全。 对象的访问定位Java程序通过栈上的reference数据来操作堆上的具体对象。主流对象访问方式为句柄和直接指针两种： 句柄访问，Java堆划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，句柄包含了对象实例数据（存储在堆中的实例池）与类型数据（存储在方法区）各自的地址信息。优势：稳定句柄地址，对象被移动时，只会改变句柄中的实例数据指针，而reference本身不需要修改。 直接指针，Java堆中对象的内存布局必须要考虑如何访问类型数据的相关信息，reference中存储的直接就是对象地址。优势：速度快、节省一次指针定位的开销（因为它直接存储的对象实例地址，不需要存储对象实例地址的指针），这部分时间开销是非常可观的。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:3","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#对象的创建"},{"categories":["java"],"content":"\r2.1.2 HotSpot虚拟机对象探秘\r对象的创建 Jvm接收到字节码new指令时，首先检查该指令的参数是否能在常量池中找到类的符号引用，并且检查该符号引用代表的类是否被加载、解析和初始化过。如果没有，就先执行相应的类加载过程。 类加载检查通过后，jvm给新生对象分配内存，其内存所需的大小在类加载完成后便可确定，而为对象分配空间的任务实际上就是把一块确定大小的内存从Java堆中划分出来。分配方式有Java堆是否规整决定，而Java堆的规整又由采用的垃圾收集器是否带有空间压缩整理（Compact）的能力决定。 假设Java堆中内存绝对规整，所有使用过的内存在一边，空闲内存在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间方向挪动一段与对象大小相等的距离，这种分配方式称为指针碰撞（Bump The Pointer）。 假设Java堆中内存不规整，已使用内存和未使用内存相互交错在一起，就无法简单地指针碰撞，jvm必须维护一个列表，上面记录哪些内存块可用，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式称为空闲列表（Free List） 对象在jvm的创建是复杂的行为，仅仅修改指针指向的位置，在并发下会存在线程安全问题，如给对象A分配内存，指针还未来得及修改，对象B同时使用了原来的指针来分配内存的情况。 解决方法1：对分配内存空间的动作进行同步处理，JVM采用的是CAS配上失败重试的方式保证更新操作的原子性。 解决方法2：把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在java堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer,TLAB），不同线程分配内存，就在其对应的的本地缓冲区中分配，只有本地缓冲区用完了，分配新的缓存区时才需要同步锁定。否使用了TLAB的命令：-XX: +/-UseTLAB参数。 内存分配完之后，jvm必须将分配的内存空间（不包括对象头）都初始化为零值。如果使用了TLAB，这个操作也可以提前至TLAB分配时进行。这是为了保证对象的实例字段在java代码中可以不赋初始值就直接使用，使程序能够访问到这些字段的数据类型所对应的零值。 jvm还要对对象进行必要的设置，如确定该对象是哪个类的实例、如何找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。这些信息会存在对象头（Object header）中，根据jvm当前运行状态的不同，对象头会有不用的设置。 jvm角度来看，新的对象就产生了。但从java程序视角，对象创建才刚刚开始——构造函数，即Class文件中的()方法还没有执行，所有字段都是默认的零值。对象所需要其他资源和状态信息也都没有构造好。 对象的内存布局HotSpot中，对象在堆内存的存储布局：对象头 Header、实例数据 Instance Data、对齐填充 Padding。 Header：一部分存储对象运行时数据，如HashCode、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，称为Mark Word。它是一个动态定义的数据结构，以便在极小的空间存储尽量多的数据。 Header：另一部分存储类型指针，即对象指向它的类型元数据的指针，jvm通过指针确定对象是哪个类的实例。并不是所有jvm的实现都必须在对象数据保留类型指针，换句话说，查找对象的元数据不一定要经过对象本身。如果对象是数组，那么对象头必须有一块记录数组长度的数据，jvm通过元数据来确定对象的大小，因此长度是要确定的。 Instance Data：存储着对象真正存储的有效信息，即代码程序中定义的各类型字段内容，无论从父类继承还是子类中定义的字段都必须记录下来。这部分存储顺序收到虚拟机分配策略参数（-XX: fieldsAllocationStyle参数）和字段在java源码中定义顺序的影响。 Padding：这不是必然存在的，也没有特别含义，仅仅起着占位符的作用。HotSpot自动内存管理系统要求对象起始地址必须是8字节的整数倍，，对象头部分是被设计过正好是8字节的倍数，而对象实例数据部分如果没对齐，就需要其来补全。 对象的访问定位Java程序通过栈上的reference数据来操作堆上的具体对象。主流对象访问方式为句柄和直接指针两种： 句柄访问，Java堆划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，句柄包含了对象实例数据（存储在堆中的实例池）与类型数据（存储在方法区）各自的地址信息。优势：稳定句柄地址，对象被移动时，只会改变句柄中的实例数据指针，而reference本身不需要修改。 直接指针，Java堆中对象的内存布局必须要考虑如何访问类型数据的相关信息，reference中存储的直接就是对象地址。优势：速度快、节省一次指针定位的开销（因为它直接存储的对象实例地址，不需要存储对象实例地址的指针），这部分时间开销是非常可观的。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:3","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#对象的内存布局"},{"categories":["java"],"content":"\r2.1.2 HotSpot虚拟机对象探秘\r对象的创建 Jvm接收到字节码new指令时，首先检查该指令的参数是否能在常量池中找到类的符号引用，并且检查该符号引用代表的类是否被加载、解析和初始化过。如果没有，就先执行相应的类加载过程。 类加载检查通过后，jvm给新生对象分配内存，其内存所需的大小在类加载完成后便可确定，而为对象分配空间的任务实际上就是把一块确定大小的内存从Java堆中划分出来。分配方式有Java堆是否规整决定，而Java堆的规整又由采用的垃圾收集器是否带有空间压缩整理（Compact）的能力决定。 假设Java堆中内存绝对规整，所有使用过的内存在一边，空闲内存在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间方向挪动一段与对象大小相等的距离，这种分配方式称为指针碰撞（Bump The Pointer）。 假设Java堆中内存不规整，已使用内存和未使用内存相互交错在一起，就无法简单地指针碰撞，jvm必须维护一个列表，上面记录哪些内存块可用，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式称为空闲列表（Free List） 对象在jvm的创建是复杂的行为，仅仅修改指针指向的位置，在并发下会存在线程安全问题，如给对象A分配内存，指针还未来得及修改，对象B同时使用了原来的指针来分配内存的情况。 解决方法1：对分配内存空间的动作进行同步处理，JVM采用的是CAS配上失败重试的方式保证更新操作的原子性。 解决方法2：把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在java堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer,TLAB），不同线程分配内存，就在其对应的的本地缓冲区中分配，只有本地缓冲区用完了，分配新的缓存区时才需要同步锁定。否使用了TLAB的命令：-XX: +/-UseTLAB参数。 内存分配完之后，jvm必须将分配的内存空间（不包括对象头）都初始化为零值。如果使用了TLAB，这个操作也可以提前至TLAB分配时进行。这是为了保证对象的实例字段在java代码中可以不赋初始值就直接使用，使程序能够访问到这些字段的数据类型所对应的零值。 jvm还要对对象进行必要的设置，如确定该对象是哪个类的实例、如何找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。这些信息会存在对象头（Object header）中，根据jvm当前运行状态的不同，对象头会有不用的设置。 jvm角度来看，新的对象就产生了。但从java程序视角，对象创建才刚刚开始——构造函数，即Class文件中的()方法还没有执行，所有字段都是默认的零值。对象所需要其他资源和状态信息也都没有构造好。 对象的内存布局HotSpot中，对象在堆内存的存储布局：对象头 Header、实例数据 Instance Data、对齐填充 Padding。 Header：一部分存储对象运行时数据，如HashCode、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，称为Mark Word。它是一个动态定义的数据结构，以便在极小的空间存储尽量多的数据。 Header：另一部分存储类型指针，即对象指向它的类型元数据的指针，jvm通过指针确定对象是哪个类的实例。并不是所有jvm的实现都必须在对象数据保留类型指针，换句话说，查找对象的元数据不一定要经过对象本身。如果对象是数组，那么对象头必须有一块记录数组长度的数据，jvm通过元数据来确定对象的大小，因此长度是要确定的。 Instance Data：存储着对象真正存储的有效信息，即代码程序中定义的各类型字段内容，无论从父类继承还是子类中定义的字段都必须记录下来。这部分存储顺序收到虚拟机分配策略参数（-XX: fieldsAllocationStyle参数）和字段在java源码中定义顺序的影响。 Padding：这不是必然存在的，也没有特别含义，仅仅起着占位符的作用。HotSpot自动内存管理系统要求对象起始地址必须是8字节的整数倍，，对象头部分是被设计过正好是8字节的倍数，而对象实例数据部分如果没对齐，就需要其来补全。 对象的访问定位Java程序通过栈上的reference数据来操作堆上的具体对象。主流对象访问方式为句柄和直接指针两种： 句柄访问，Java堆划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，句柄包含了对象实例数据（存储在堆中的实例池）与类型数据（存储在方法区）各自的地址信息。优势：稳定句柄地址，对象被移动时，只会改变句柄中的实例数据指针，而reference本身不需要修改。 直接指针，Java堆中对象的内存布局必须要考虑如何访问类型数据的相关信息，reference中存储的直接就是对象地址。优势：速度快、节省一次指针定位的开销（因为它直接存储的对象实例地址，不需要存储对象实例地址的指针），这部分时间开销是非常可观的。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:3","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#对象的访问定位"},{"categories":["java"],"content":"\r2.1.3 OutOfMemoryError除了程序计数器，jvm内存的其他几个运行区域都有发生OOM的可能。 Java堆溢出随着不断地创建对象并且不清除，随着对象数量增加，总容量达到最大堆的容量限制就会OOM。 操作指令和代码测试在P90 要解决Java堆内存区域的异常，一般都是通过内存映像分析工具对Dump出来的堆转储快照进行分析，第一步要确认导致OOM的对象是哪个，然后分清是内存泄漏（memory leak）还是内存溢出（memory overflow） 内存泄漏，通过工具查看泄露对象到GC Roots的引用链，找到泄露对象通过怎样的引用路径、与哪些GC Roots相关联，才导致无法回收。根据泄露对象的类型信息以及它到GC Roots引用链的信息来定位它创建的未知，进而找到产生内存泄漏的代码的具体位置。 内存溢出，就表示内存中的对象都是存活的，就该检查jvm的堆参数设置（-Xmx-Xms）与机器内存对比，看看是否还有向上调整的空间。代码上检查是否存在生命周期过长、持有状态时间过长、存储结构设计不合理等情况，减少程序运行的内存消耗。 虚拟机栈和本地方法溢出HotSpot并不区分vm栈和native方法栈，因此，-Xoss参数是没有作用的，栈容量智能有-Xss参数来设定。 栈中有两种异常：StackOverFlowError（线程请求的栈深度超过jvm允许的最大深度时）、OutOfMemoryError（扩展栈容量时无法申请到足够的内存时）。 Java虚拟机规范中允许jvm自行实现栈的动态扩展，而HotSpot的选择是不支持，所以除非在创建线程申请内存时就因无法获得足够内存而出现OOM，否则线程运行期间是不会因为扩展而导致内存溢出的，只会因为栈容量无法容纳新的栈帧而导致StackOverflowError。 验证： 使用-Xss参数减少栈内存容量。结果，抛出StackOverflowError，异常出现时输出的堆栈深度相应缩小。 定义了大量的本地变量，增大此方法帧中本地变量表的长度。结果，抛出StackOverflowError，异常出现时输出的堆栈神父相应缩小。 方法区和运行时常量池区溢出运行时常量池是方法区的一部分，方法区溢出也是常见的异常。 本机直接内存溢出// TODO ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:4","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#213-outofmemoryerror"},{"categories":["java"],"content":"\r2.1.3 OutOfMemoryError除了程序计数器，jvm内存的其他几个运行区域都有发生OOM的可能。 Java堆溢出随着不断地创建对象并且不清除，随着对象数量增加，总容量达到最大堆的容量限制就会OOM。 操作指令和代码测试在P90 要解决Java堆内存区域的异常，一般都是通过内存映像分析工具对Dump出来的堆转储快照进行分析，第一步要确认导致OOM的对象是哪个，然后分清是内存泄漏（memory leak）还是内存溢出（memory overflow） 内存泄漏，通过工具查看泄露对象到GC Roots的引用链，找到泄露对象通过怎样的引用路径、与哪些GC Roots相关联，才导致无法回收。根据泄露对象的类型信息以及它到GC Roots引用链的信息来定位它创建的未知，进而找到产生内存泄漏的代码的具体位置。 内存溢出，就表示内存中的对象都是存活的，就该检查jvm的堆参数设置（-Xmx-Xms）与机器内存对比，看看是否还有向上调整的空间。代码上检查是否存在生命周期过长、持有状态时间过长、存储结构设计不合理等情况，减少程序运行的内存消耗。 虚拟机栈和本地方法溢出HotSpot并不区分vm栈和native方法栈，因此，-Xoss参数是没有作用的，栈容量智能有-Xss参数来设定。 栈中有两种异常：StackOverFlowError（线程请求的栈深度超过jvm允许的最大深度时）、OutOfMemoryError（扩展栈容量时无法申请到足够的内存时）。 Java虚拟机规范中允许jvm自行实现栈的动态扩展，而HotSpot的选择是不支持，所以除非在创建线程申请内存时就因无法获得足够内存而出现OOM，否则线程运行期间是不会因为扩展而导致内存溢出的，只会因为栈容量无法容纳新的栈帧而导致StackOverflowError。 验证： 使用-Xss参数减少栈内存容量。结果，抛出StackOverflowError，异常出现时输出的堆栈深度相应缩小。 定义了大量的本地变量，增大此方法帧中本地变量表的长度。结果，抛出StackOverflowError，异常出现时输出的堆栈神父相应缩小。 方法区和运行时常量池区溢出运行时常量池是方法区的一部分，方法区溢出也是常见的异常。 本机直接内存溢出// TODO ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:4","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#java堆溢出"},{"categories":["java"],"content":"\r2.1.3 OutOfMemoryError除了程序计数器，jvm内存的其他几个运行区域都有发生OOM的可能。 Java堆溢出随着不断地创建对象并且不清除，随着对象数量增加，总容量达到最大堆的容量限制就会OOM。 操作指令和代码测试在P90 要解决Java堆内存区域的异常，一般都是通过内存映像分析工具对Dump出来的堆转储快照进行分析，第一步要确认导致OOM的对象是哪个，然后分清是内存泄漏（memory leak）还是内存溢出（memory overflow） 内存泄漏，通过工具查看泄露对象到GC Roots的引用链，找到泄露对象通过怎样的引用路径、与哪些GC Roots相关联，才导致无法回收。根据泄露对象的类型信息以及它到GC Roots引用链的信息来定位它创建的未知，进而找到产生内存泄漏的代码的具体位置。 内存溢出，就表示内存中的对象都是存活的，就该检查jvm的堆参数设置（-Xmx-Xms）与机器内存对比，看看是否还有向上调整的空间。代码上检查是否存在生命周期过长、持有状态时间过长、存储结构设计不合理等情况，减少程序运行的内存消耗。 虚拟机栈和本地方法溢出HotSpot并不区分vm栈和native方法栈，因此，-Xoss参数是没有作用的，栈容量智能有-Xss参数来设定。 栈中有两种异常：StackOverFlowError（线程请求的栈深度超过jvm允许的最大深度时）、OutOfMemoryError（扩展栈容量时无法申请到足够的内存时）。 Java虚拟机规范中允许jvm自行实现栈的动态扩展，而HotSpot的选择是不支持，所以除非在创建线程申请内存时就因无法获得足够内存而出现OOM，否则线程运行期间是不会因为扩展而导致内存溢出的，只会因为栈容量无法容纳新的栈帧而导致StackOverflowError。 验证： 使用-Xss参数减少栈内存容量。结果，抛出StackOverflowError，异常出现时输出的堆栈深度相应缩小。 定义了大量的本地变量，增大此方法帧中本地变量表的长度。结果，抛出StackOverflowError，异常出现时输出的堆栈神父相应缩小。 方法区和运行时常量池区溢出运行时常量池是方法区的一部分，方法区溢出也是常见的异常。 本机直接内存溢出// TODO ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:4","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#虚拟机栈和本地方法溢出"},{"categories":["java"],"content":"\r2.1.3 OutOfMemoryError除了程序计数器，jvm内存的其他几个运行区域都有发生OOM的可能。 Java堆溢出随着不断地创建对象并且不清除，随着对象数量增加，总容量达到最大堆的容量限制就会OOM。 操作指令和代码测试在P90 要解决Java堆内存区域的异常，一般都是通过内存映像分析工具对Dump出来的堆转储快照进行分析，第一步要确认导致OOM的对象是哪个，然后分清是内存泄漏（memory leak）还是内存溢出（memory overflow） 内存泄漏，通过工具查看泄露对象到GC Roots的引用链，找到泄露对象通过怎样的引用路径、与哪些GC Roots相关联，才导致无法回收。根据泄露对象的类型信息以及它到GC Roots引用链的信息来定位它创建的未知，进而找到产生内存泄漏的代码的具体位置。 内存溢出，就表示内存中的对象都是存活的，就该检查jvm的堆参数设置（-Xmx-Xms）与机器内存对比，看看是否还有向上调整的空间。代码上检查是否存在生命周期过长、持有状态时间过长、存储结构设计不合理等情况，减少程序运行的内存消耗。 虚拟机栈和本地方法溢出HotSpot并不区分vm栈和native方法栈，因此，-Xoss参数是没有作用的，栈容量智能有-Xss参数来设定。 栈中有两种异常：StackOverFlowError（线程请求的栈深度超过jvm允许的最大深度时）、OutOfMemoryError（扩展栈容量时无法申请到足够的内存时）。 Java虚拟机规范中允许jvm自行实现栈的动态扩展，而HotSpot的选择是不支持，所以除非在创建线程申请内存时就因无法获得足够内存而出现OOM，否则线程运行期间是不会因为扩展而导致内存溢出的，只会因为栈容量无法容纳新的栈帧而导致StackOverflowError。 验证： 使用-Xss参数减少栈内存容量。结果，抛出StackOverflowError，异常出现时输出的堆栈深度相应缩小。 定义了大量的本地变量，增大此方法帧中本地变量表的长度。结果，抛出StackOverflowError，异常出现时输出的堆栈神父相应缩小。 方法区和运行时常量池区溢出运行时常量池是方法区的一部分，方法区溢出也是常见的异常。 本机直接内存溢出// TODO ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:4","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#方法区和运行时常量池区溢出"},{"categories":["java"],"content":"\r2.1.3 OutOfMemoryError除了程序计数器，jvm内存的其他几个运行区域都有发生OOM的可能。 Java堆溢出随着不断地创建对象并且不清除，随着对象数量增加，总容量达到最大堆的容量限制就会OOM。 操作指令和代码测试在P90 要解决Java堆内存区域的异常，一般都是通过内存映像分析工具对Dump出来的堆转储快照进行分析，第一步要确认导致OOM的对象是哪个，然后分清是内存泄漏（memory leak）还是内存溢出（memory overflow） 内存泄漏，通过工具查看泄露对象到GC Roots的引用链，找到泄露对象通过怎样的引用路径、与哪些GC Roots相关联，才导致无法回收。根据泄露对象的类型信息以及它到GC Roots引用链的信息来定位它创建的未知，进而找到产生内存泄漏的代码的具体位置。 内存溢出，就表示内存中的对象都是存活的，就该检查jvm的堆参数设置（-Xmx-Xms）与机器内存对比，看看是否还有向上调整的空间。代码上检查是否存在生命周期过长、持有状态时间过长、存储结构设计不合理等情况，减少程序运行的内存消耗。 虚拟机栈和本地方法溢出HotSpot并不区分vm栈和native方法栈，因此，-Xoss参数是没有作用的，栈容量智能有-Xss参数来设定。 栈中有两种异常：StackOverFlowError（线程请求的栈深度超过jvm允许的最大深度时）、OutOfMemoryError（扩展栈容量时无法申请到足够的内存时）。 Java虚拟机规范中允许jvm自行实现栈的动态扩展，而HotSpot的选择是不支持，所以除非在创建线程申请内存时就因无法获得足够内存而出现OOM，否则线程运行期间是不会因为扩展而导致内存溢出的，只会因为栈容量无法容纳新的栈帧而导致StackOverflowError。 验证： 使用-Xss参数减少栈内存容量。结果，抛出StackOverflowError，异常出现时输出的堆栈深度相应缩小。 定义了大量的本地变量，增大此方法帧中本地变量表的长度。结果，抛出StackOverflowError，异常出现时输出的堆栈神父相应缩小。 方法区和运行时常量池区溢出运行时常量池是方法区的一部分，方法区溢出也是常见的异常。 本机直接内存溢出// TODO ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:3:4","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#本机直接内存溢出"},{"categories":["java"],"content":"\r3 垃圾回收器与内存分配策略","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:0","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#3-垃圾回收器与内存分配策略"},{"categories":["java"],"content":"\r3.1 概述GC关注的是Java堆和方法区的内存管理，因为这两个区有着显著的不确定性，它们的内存分配和回收是动态的：一个接口的多个实现类需要的内存可能不一样；一个方法所执行的不同条件分支所需要的内存也可能不一样；只有处于运行期间，才知道程序究竟会创建哪些对象，多少个对象。 垃圾收集器在对堆进行回收前，就是确定对象是存活还是死亡。 引用计数算法Reference Counting原理： 每个对象中添加一个引用计数器 每当有一个地方引用它时，计数器值+1 当引用失效时，计数器值-1 任何时刻计数器为0的对象就是不可被引用 大多数情况下，reference counting是一个不错的算法（原理简单、判定效率高，虽然占用了一些额外的内存空间来进行记数）。但是在Java主流的jvm中没有选用该算法管理内存，主要原因是，该算法必须要配合大量额外处理才能保证正常工作，譬如单纯的引用计数就很难解决对象之间互相循环引用的问题。 objA和objB在互相引用，但是他们本身的属性只有一个instance并且是null，赋值令各自的instance = 对象obj。两个对象已经不可能再被引用了： 因为他们的各自的obj都是null，可以看到除了instance剩下的属性都是private，而instance是null。当值为null的时候，就表示它已经是失效的了。 但是二者又互相引用对方，导致它们的计数都不为0，所以，通过引用计数算法就无法回收他们。而通过内存回收日志可以看到，jvm并没有二者相互引用就放弃回收他们，侧面证明了，jvm不是通过引用计数算法来判断对象是否存活。 /** * testGC()方法执行后，ab会不会被GC？ */ public class ReferenceCountingGC { public Object instance = null; private static final int _1MB = 1024 * 1024; /** * 唯一充裕就是占内存，以便在GC日志中看清楚是否被回收过 */ private byte[] bigSize = new byte[2 * _1MB]; public static void testGC(){ ReferenceCountingGC objA = new ReferenceCountingGC(); ReferenceCountingGC objB = new ReferenceCountingGC(); objA.instance = objB; objB.instance = objA; // 假设在这行发生GC，a、b是都能被回收 System.gc(); } public static void main(String[] args) { testGC(); } } 可达性分析算法 Reachability Analysis算法思路：通过 GC Roots 的根对象作为起始节点集，从这些节点开始，根据引用关系向下搜索，搜索过程所走过的路径称为“引用链” Reference Chain，如果某个对象到 GC Roots 间没有任何引用链相连，或者用图论的话来说就是从 GC Roots 到这个对象不可达时，该对象是可能再被使用的。 固定可作为 GC Roots 的对象包括一下几种: 在vm stack（栈帧中的本地变量表）中引用的对象，譬如各个线程被调用的方法堆栈中使用到的参数、局部变量、临时变量等。 在method area 中类静态属性引用的对象、譬如Java类的旖旎用类型静态变量。 在method area 常量引用的对象，譬如字符串常量池（String table）里的引用。 在本地方法栈中 JNI（即 Native方法）引用的对象。 jvm内部的引用，如基本数据类型对应的Class对象，一些常驻的异常对象。（NullPointException、OutofMemoryError）等，还有系统类加载器。 所有被同步锁（synchronized关键字）持有的对象。 反应jvm内部却情况的JMXBean、JVMTI中注册的回调、本地代码缓存等。 最新的垃圾收集器欧具备了局部回收的特征，为了避免GC Roots包含过多对象而过度膨胀。 再谈引用引用如果只有未被引用和被引用就很狭隘，比如有些对象，我们希望当内存足够时就保留，内存不足就优先释放。 Strongly Reference：传统的引用定义，引用复制，类似 Object obj = new Object();，这种关系下，垃圾收集器永远不会回收被引用对象。 Soft Reference：引用有用但非必须的对象 内存溢出异常前，会把回收这些对象进行二次回收，这次回收后还没有足够内存才会抛出异常。 Weak Reference：比软引用更弱一下，弱引用关联的对象只能存活到下一次垃圾收集发生为止。 Phantom Reference：最弱的引用关系。 存在目的：对象被收集器回收时收到一个系统通知。 对象是否有虚引用，完全不会对其生存空间构成影响，也无法通过虚引用来取得一个对象实例。 生存还是死亡可达性分析算法判定对象死亡的流程： 发现对象与GC Roots没有引用链，被标记一次 二次筛选，筛选条件是对象是否有必要执行 finalize()方法 对象没有覆盖finalize()方法或该方法被jvm调用过，则视为生存 对象被判定为有必要执行finalize()方法，该对象会被防止在F-Queue的队列中 jvm随后会自动建立一个低优先级的 Finalizer线程去执行队列中对象的 finalize()方法。（注：执行表示jvm会触发方法运行，但是不承诺会等到其运行结束）如果某个对象的finalize()方法执行慢，甚至死循环，就会导致整个队列永久性等待，甚至内存回收子系统崩溃 垃圾收集器会对F-Queue中的对象进行二次标记，如果对象在调用finalize()中重新与引用链上的任何一个对象建立关联就会被视为生存，也就会被移除该队列。 没有和引用链建立关联的对象会被回收。 总结：对象被回收需要经过两次标记，自救的机会只有一次，就是在第一次和第二次标记中用过finaliza()建立与引用链上的对象的关联即可。 /** * 对象被GC时的自我拯救 */ public class FinalizeEscapeGC { public static FinalizeEscapeGC SAVE_HOOK = null; public void isAlive(){ System.out.println(\"yes, i am still alive :\u003e\"); } @Override protected void finalize() throws Throwable { super.finalize(); System.out.println(\"finalize method executed!\"); FinalizeEscapeGC.SAVE_HOOK = this; } public static void main(String[] args) throws InterruptedException { SAVE_HOOK = new FinalizeEscapeGC(); // 对象第一次拯救自己 SAVE_HOOK = null; System.gc(); // Finalizer方法优先级低，暂停0.5s等它 Thread.sleep(500); if(SAVE_HOOK != null){ SAVE_HOOK.isAlive(); }else { System.out.println(\"no, i am dead :\u003c\"); } // 代码完全相同，只为证明它只能拯救一次 // 对象第二次拯救自己 SAVE_HOOK = null; System.gc(); // Finalizer方法优先级低，暂停0.5s等它 Thread.sleep(500); if(SAVE_HOOK != null){ SAVE_HOOK.isAlive(); }else { System.out.println(\"no, i am dead :\u003c\"); } } 运行结果 finalize method executed! yes, i am still alive :\u003e no, i am dead :\u003c 并不鼓励使用这种方式来拯救对象，并且要尽量避免使用它（虽然已经使用它） 重点：finalize()已经被弃用。 finalize()的弊端，一方面无法知道对象在F-Queue中的顺序，而是没办法空值GC发生的时间。因此通过该方法释放资源是危险的。 更好地代替方式：try-with-resources 代码块 和 AutoCloseable 接口来确保资源在代码块执行完毕之后被释放。 或者使用 PhantomReference虚引用 来实现对象被回收时触发的动作。 回收方法区jvm规范中提到可以不要求jvm在method area中实现垃圾收集， 如hotspot中元空间或永久代 方法区垃圾收集的性价比低（因为java堆中，尤其新生代中，一次能回收70%-99%的内存），而方法区回收的判定条件一般比较苛刻 方法区垃圾收集主要两部分内容： 废弃常量，回收过程与对象类似。如果没有被引用就会回收。 不再使用的类型，但是判定类型是否不再被使用条件苛刻，需同时满足3个条件： 该类所有的实例已被回收 加载该类的类加载器被回收（很难达成） 该类对应的class对象没有被引用","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:1","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#31-概述"},{"categories":["java"],"content":"\r3.1 概述GC关注的是Java堆和方法区的内存管理，因为这两个区有着显著的不确定性，它们的内存分配和回收是动态的：一个接口的多个实现类需要的内存可能不一样；一个方法所执行的不同条件分支所需要的内存也可能不一样；只有处于运行期间，才知道程序究竟会创建哪些对象，多少个对象。 垃圾收集器在对堆进行回收前，就是确定对象是存活还是死亡。 引用计数算法Reference Counting原理： 每个对象中添加一个引用计数器 每当有一个地方引用它时，计数器值+1 当引用失效时，计数器值-1 任何时刻计数器为0的对象就是不可被引用 大多数情况下，reference counting是一个不错的算法（原理简单、判定效率高，虽然占用了一些额外的内存空间来进行记数）。但是在Java主流的jvm中没有选用该算法管理内存，主要原因是，该算法必须要配合大量额外处理才能保证正常工作，譬如单纯的引用计数就很难解决对象之间互相循环引用的问题。 objA和objB在互相引用，但是他们本身的属性只有一个instance并且是null，赋值令各自的instance = 对象obj。两个对象已经不可能再被引用了： 因为他们的各自的obj都是null，可以看到除了instance剩下的属性都是private，而instance是null。当值为null的时候，就表示它已经是失效的了。 但是二者又互相引用对方，导致它们的计数都不为0，所以，通过引用计数算法就无法回收他们。而通过内存回收日志可以看到，jvm并没有二者相互引用就放弃回收他们，侧面证明了，jvm不是通过引用计数算法来判断对象是否存活。 /** * testGC()方法执行后，ab会不会被GC？ */ public class ReferenceCountingGC { public Object instance = null; private static final int _1MB = 1024 * 1024; /** * 唯一充裕就是占内存，以便在GC日志中看清楚是否被回收过 */ private byte[] bigSize = new byte[2 * _1MB]; public static void testGC(){ ReferenceCountingGC objA = new ReferenceCountingGC(); ReferenceCountingGC objB = new ReferenceCountingGC(); objA.instance = objB; objB.instance = objA; // 假设在这行发生GC，a、b是都能被回收 System.gc(); } public static void main(String[] args) { testGC(); } } 可达性分析算法 Reachability Analysis算法思路：通过 GC Roots 的根对象作为起始节点集，从这些节点开始，根据引用关系向下搜索，搜索过程所走过的路径称为“引用链” Reference Chain，如果某个对象到 GC Roots 间没有任何引用链相连，或者用图论的话来说就是从 GC Roots 到这个对象不可达时，该对象是可能再被使用的。 固定可作为 GC Roots 的对象包括一下几种: 在vm stack（栈帧中的本地变量表）中引用的对象，譬如各个线程被调用的方法堆栈中使用到的参数、局部变量、临时变量等。 在method area 中类静态属性引用的对象、譬如Java类的旖旎用类型静态变量。 在method area 常量引用的对象，譬如字符串常量池（String table）里的引用。 在本地方法栈中 JNI（即 Native方法）引用的对象。 jvm内部的引用，如基本数据类型对应的Class对象，一些常驻的异常对象。（NullPointException、OutofMemoryError）等，还有系统类加载器。 所有被同步锁（synchronized关键字）持有的对象。 反应jvm内部却情况的JMXBean、JVMTI中注册的回调、本地代码缓存等。 最新的垃圾收集器欧具备了局部回收的特征，为了避免GC Roots包含过多对象而过度膨胀。 再谈引用引用如果只有未被引用和被引用就很狭隘，比如有些对象，我们希望当内存足够时就保留，内存不足就优先释放。 Strongly Reference：传统的引用定义，引用复制，类似 Object obj = new Object();，这种关系下，垃圾收集器永远不会回收被引用对象。 Soft Reference：引用有用但非必须的对象 内存溢出异常前，会把回收这些对象进行二次回收，这次回收后还没有足够内存才会抛出异常。 Weak Reference：比软引用更弱一下，弱引用关联的对象只能存活到下一次垃圾收集发生为止。 Phantom Reference：最弱的引用关系。 存在目的：对象被收集器回收时收到一个系统通知。 对象是否有虚引用，完全不会对其生存空间构成影响，也无法通过虚引用来取得一个对象实例。 生存还是死亡可达性分析算法判定对象死亡的流程： 发现对象与GC Roots没有引用链，被标记一次 二次筛选，筛选条件是对象是否有必要执行 finalize()方法 对象没有覆盖finalize()方法或该方法被jvm调用过，则视为生存 对象被判定为有必要执行finalize()方法，该对象会被防止在F-Queue的队列中 jvm随后会自动建立一个低优先级的 Finalizer线程去执行队列中对象的 finalize()方法。（注：执行表示jvm会触发方法运行，但是不承诺会等到其运行结束）如果某个对象的finalize()方法执行慢，甚至死循环，就会导致整个队列永久性等待，甚至内存回收子系统崩溃 垃圾收集器会对F-Queue中的对象进行二次标记，如果对象在调用finalize()中重新与引用链上的任何一个对象建立关联就会被视为生存，也就会被移除该队列。 没有和引用链建立关联的对象会被回收。 总结：对象被回收需要经过两次标记，自救的机会只有一次，就是在第一次和第二次标记中用过finaliza()建立与引用链上的对象的关联即可。 /** * 对象被GC时的自我拯救 */ public class FinalizeEscapeGC { public static FinalizeEscapeGC SAVE_HOOK = null; public void isAlive(){ System.out.println(\"yes, i am still alive :\u003e\"); } @Override protected void finalize() throws Throwable { super.finalize(); System.out.println(\"finalize method executed!\"); FinalizeEscapeGC.SAVE_HOOK = this; } public static void main(String[] args) throws InterruptedException { SAVE_HOOK = new FinalizeEscapeGC(); // 对象第一次拯救自己 SAVE_HOOK = null; System.gc(); // Finalizer方法优先级低，暂停0.5s等它 Thread.sleep(500); if(SAVE_HOOK != null){ SAVE_HOOK.isAlive(); }else { System.out.println(\"no, i am dead :\u003c\"); } // 代码完全相同，只为证明它只能拯救一次 // 对象第二次拯救自己 SAVE_HOOK = null; System.gc(); // Finalizer方法优先级低，暂停0.5s等它 Thread.sleep(500); if(SAVE_HOOK != null){ SAVE_HOOK.isAlive(); }else { System.out.println(\"no, i am dead :\u003c\"); } } 运行结果 finalize method executed! yes, i am still alive :\u003e no, i am dead :\u003c 并不鼓励使用这种方式来拯救对象，并且要尽量避免使用它（虽然已经使用它） 重点：finalize()已经被弃用。 finalize()的弊端，一方面无法知道对象在F-Queue中的顺序，而是没办法空值GC发生的时间。因此通过该方法释放资源是危险的。 更好地代替方式：try-with-resources 代码块 和 AutoCloseable 接口来确保资源在代码块执行完毕之后被释放。 或者使用 PhantomReference虚引用 来实现对象被回收时触发的动作。 回收方法区jvm规范中提到可以不要求jvm在method area中实现垃圾收集， 如hotspot中元空间或永久代 方法区垃圾收集的性价比低（因为java堆中，尤其新生代中，一次能回收70%-99%的内存），而方法区回收的判定条件一般比较苛刻 方法区垃圾收集主要两部分内容： 废弃常量，回收过程与对象类似。如果没有被引用就会回收。 不再使用的类型，但是判定类型是否不再被使用条件苛刻，需同时满足3个条件： 该类所有的实例已被回收 加载该类的类加载器被回收（很难达成） 该类对应的class对象没有被引用","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:1","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#引用计数算法"},{"categories":["java"],"content":"\r3.1 概述GC关注的是Java堆和方法区的内存管理，因为这两个区有着显著的不确定性，它们的内存分配和回收是动态的：一个接口的多个实现类需要的内存可能不一样；一个方法所执行的不同条件分支所需要的内存也可能不一样；只有处于运行期间，才知道程序究竟会创建哪些对象，多少个对象。 垃圾收集器在对堆进行回收前，就是确定对象是存活还是死亡。 引用计数算法Reference Counting原理： 每个对象中添加一个引用计数器 每当有一个地方引用它时，计数器值+1 当引用失效时，计数器值-1 任何时刻计数器为0的对象就是不可被引用 大多数情况下，reference counting是一个不错的算法（原理简单、判定效率高，虽然占用了一些额外的内存空间来进行记数）。但是在Java主流的jvm中没有选用该算法管理内存，主要原因是，该算法必须要配合大量额外处理才能保证正常工作，譬如单纯的引用计数就很难解决对象之间互相循环引用的问题。 objA和objB在互相引用，但是他们本身的属性只有一个instance并且是null，赋值令各自的instance = 对象obj。两个对象已经不可能再被引用了： 因为他们的各自的obj都是null，可以看到除了instance剩下的属性都是private，而instance是null。当值为null的时候，就表示它已经是失效的了。 但是二者又互相引用对方，导致它们的计数都不为0，所以，通过引用计数算法就无法回收他们。而通过内存回收日志可以看到，jvm并没有二者相互引用就放弃回收他们，侧面证明了，jvm不是通过引用计数算法来判断对象是否存活。 /** * testGC()方法执行后，ab会不会被GC？ */ public class ReferenceCountingGC { public Object instance = null; private static final int _1MB = 1024 * 1024; /** * 唯一充裕就是占内存，以便在GC日志中看清楚是否被回收过 */ private byte[] bigSize = new byte[2 * _1MB]; public static void testGC(){ ReferenceCountingGC objA = new ReferenceCountingGC(); ReferenceCountingGC objB = new ReferenceCountingGC(); objA.instance = objB; objB.instance = objA; // 假设在这行发生GC，a、b是都能被回收 System.gc(); } public static void main(String[] args) { testGC(); } } 可达性分析算法 Reachability Analysis算法思路：通过 GC Roots 的根对象作为起始节点集，从这些节点开始，根据引用关系向下搜索，搜索过程所走过的路径称为“引用链” Reference Chain，如果某个对象到 GC Roots 间没有任何引用链相连，或者用图论的话来说就是从 GC Roots 到这个对象不可达时，该对象是可能再被使用的。 固定可作为 GC Roots 的对象包括一下几种: 在vm stack（栈帧中的本地变量表）中引用的对象，譬如各个线程被调用的方法堆栈中使用到的参数、局部变量、临时变量等。 在method area 中类静态属性引用的对象、譬如Java类的旖旎用类型静态变量。 在method area 常量引用的对象，譬如字符串常量池（String table）里的引用。 在本地方法栈中 JNI（即 Native方法）引用的对象。 jvm内部的引用，如基本数据类型对应的Class对象，一些常驻的异常对象。（NullPointException、OutofMemoryError）等，还有系统类加载器。 所有被同步锁（synchronized关键字）持有的对象。 反应jvm内部却情况的JMXBean、JVMTI中注册的回调、本地代码缓存等。 最新的垃圾收集器欧具备了局部回收的特征，为了避免GC Roots包含过多对象而过度膨胀。 再谈引用引用如果只有未被引用和被引用就很狭隘，比如有些对象，我们希望当内存足够时就保留，内存不足就优先释放。 Strongly Reference：传统的引用定义，引用复制，类似 Object obj = new Object();，这种关系下，垃圾收集器永远不会回收被引用对象。 Soft Reference：引用有用但非必须的对象 内存溢出异常前，会把回收这些对象进行二次回收，这次回收后还没有足够内存才会抛出异常。 Weak Reference：比软引用更弱一下，弱引用关联的对象只能存活到下一次垃圾收集发生为止。 Phantom Reference：最弱的引用关系。 存在目的：对象被收集器回收时收到一个系统通知。 对象是否有虚引用，完全不会对其生存空间构成影响，也无法通过虚引用来取得一个对象实例。 生存还是死亡可达性分析算法判定对象死亡的流程： 发现对象与GC Roots没有引用链，被标记一次 二次筛选，筛选条件是对象是否有必要执行 finalize()方法 对象没有覆盖finalize()方法或该方法被jvm调用过，则视为生存 对象被判定为有必要执行finalize()方法，该对象会被防止在F-Queue的队列中 jvm随后会自动建立一个低优先级的 Finalizer线程去执行队列中对象的 finalize()方法。（注：执行表示jvm会触发方法运行，但是不承诺会等到其运行结束）如果某个对象的finalize()方法执行慢，甚至死循环，就会导致整个队列永久性等待，甚至内存回收子系统崩溃 垃圾收集器会对F-Queue中的对象进行二次标记，如果对象在调用finalize()中重新与引用链上的任何一个对象建立关联就会被视为生存，也就会被移除该队列。 没有和引用链建立关联的对象会被回收。 总结：对象被回收需要经过两次标记，自救的机会只有一次，就是在第一次和第二次标记中用过finaliza()建立与引用链上的对象的关联即可。 /** * 对象被GC时的自我拯救 */ public class FinalizeEscapeGC { public static FinalizeEscapeGC SAVE_HOOK = null; public void isAlive(){ System.out.println(\"yes, i am still alive :\u003e\"); } @Override protected void finalize() throws Throwable { super.finalize(); System.out.println(\"finalize method executed!\"); FinalizeEscapeGC.SAVE_HOOK = this; } public static void main(String[] args) throws InterruptedException { SAVE_HOOK = new FinalizeEscapeGC(); // 对象第一次拯救自己 SAVE_HOOK = null; System.gc(); // Finalizer方法优先级低，暂停0.5s等它 Thread.sleep(500); if(SAVE_HOOK != null){ SAVE_HOOK.isAlive(); }else { System.out.println(\"no, i am dead :\u003c\"); } // 代码完全相同，只为证明它只能拯救一次 // 对象第二次拯救自己 SAVE_HOOK = null; System.gc(); // Finalizer方法优先级低，暂停0.5s等它 Thread.sleep(500); if(SAVE_HOOK != null){ SAVE_HOOK.isAlive(); }else { System.out.println(\"no, i am dead :\u003c\"); } } 运行结果 finalize method executed! yes, i am still alive :\u003e no, i am dead :\u003c 并不鼓励使用这种方式来拯救对象，并且要尽量避免使用它（虽然已经使用它） 重点：finalize()已经被弃用。 finalize()的弊端，一方面无法知道对象在F-Queue中的顺序，而是没办法空值GC发生的时间。因此通过该方法释放资源是危险的。 更好地代替方式：try-with-resources 代码块 和 AutoCloseable 接口来确保资源在代码块执行完毕之后被释放。 或者使用 PhantomReference虚引用 来实现对象被回收时触发的动作。 回收方法区jvm规范中提到可以不要求jvm在method area中实现垃圾收集， 如hotspot中元空间或永久代 方法区垃圾收集的性价比低（因为java堆中，尤其新生代中，一次能回收70%-99%的内存），而方法区回收的判定条件一般比较苛刻 方法区垃圾收集主要两部分内容： 废弃常量，回收过程与对象类似。如果没有被引用就会回收。 不再使用的类型，但是判定类型是否不再被使用条件苛刻，需同时满足3个条件： 该类所有的实例已被回收 加载该类的类加载器被回收（很难达成） 该类对应的class对象没有被引用","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:1","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#可达性分析算法-reachability-analysis"},{"categories":["java"],"content":"\r3.1 概述GC关注的是Java堆和方法区的内存管理，因为这两个区有着显著的不确定性，它们的内存分配和回收是动态的：一个接口的多个实现类需要的内存可能不一样；一个方法所执行的不同条件分支所需要的内存也可能不一样；只有处于运行期间，才知道程序究竟会创建哪些对象，多少个对象。 垃圾收集器在对堆进行回收前，就是确定对象是存活还是死亡。 引用计数算法Reference Counting原理： 每个对象中添加一个引用计数器 每当有一个地方引用它时，计数器值+1 当引用失效时，计数器值-1 任何时刻计数器为0的对象就是不可被引用 大多数情况下，reference counting是一个不错的算法（原理简单、判定效率高，虽然占用了一些额外的内存空间来进行记数）。但是在Java主流的jvm中没有选用该算法管理内存，主要原因是，该算法必须要配合大量额外处理才能保证正常工作，譬如单纯的引用计数就很难解决对象之间互相循环引用的问题。 objA和objB在互相引用，但是他们本身的属性只有一个instance并且是null，赋值令各自的instance = 对象obj。两个对象已经不可能再被引用了： 因为他们的各自的obj都是null，可以看到除了instance剩下的属性都是private，而instance是null。当值为null的时候，就表示它已经是失效的了。 但是二者又互相引用对方，导致它们的计数都不为0，所以，通过引用计数算法就无法回收他们。而通过内存回收日志可以看到，jvm并没有二者相互引用就放弃回收他们，侧面证明了，jvm不是通过引用计数算法来判断对象是否存活。 /** * testGC()方法执行后，ab会不会被GC？ */ public class ReferenceCountingGC { public Object instance = null; private static final int _1MB = 1024 * 1024; /** * 唯一充裕就是占内存，以便在GC日志中看清楚是否被回收过 */ private byte[] bigSize = new byte[2 * _1MB]; public static void testGC(){ ReferenceCountingGC objA = new ReferenceCountingGC(); ReferenceCountingGC objB = new ReferenceCountingGC(); objA.instance = objB; objB.instance = objA; // 假设在这行发生GC，a、b是都能被回收 System.gc(); } public static void main(String[] args) { testGC(); } } 可达性分析算法 Reachability Analysis算法思路：通过 GC Roots 的根对象作为起始节点集，从这些节点开始，根据引用关系向下搜索，搜索过程所走过的路径称为“引用链” Reference Chain，如果某个对象到 GC Roots 间没有任何引用链相连，或者用图论的话来说就是从 GC Roots 到这个对象不可达时，该对象是可能再被使用的。 固定可作为 GC Roots 的对象包括一下几种: 在vm stack（栈帧中的本地变量表）中引用的对象，譬如各个线程被调用的方法堆栈中使用到的参数、局部变量、临时变量等。 在method area 中类静态属性引用的对象、譬如Java类的旖旎用类型静态变量。 在method area 常量引用的对象，譬如字符串常量池（String table）里的引用。 在本地方法栈中 JNI（即 Native方法）引用的对象。 jvm内部的引用，如基本数据类型对应的Class对象，一些常驻的异常对象。（NullPointException、OutofMemoryError）等，还有系统类加载器。 所有被同步锁（synchronized关键字）持有的对象。 反应jvm内部却情况的JMXBean、JVMTI中注册的回调、本地代码缓存等。 最新的垃圾收集器欧具备了局部回收的特征，为了避免GC Roots包含过多对象而过度膨胀。 再谈引用引用如果只有未被引用和被引用就很狭隘，比如有些对象，我们希望当内存足够时就保留，内存不足就优先释放。 Strongly Reference：传统的引用定义，引用复制，类似 Object obj = new Object();，这种关系下，垃圾收集器永远不会回收被引用对象。 Soft Reference：引用有用但非必须的对象 内存溢出异常前，会把回收这些对象进行二次回收，这次回收后还没有足够内存才会抛出异常。 Weak Reference：比软引用更弱一下，弱引用关联的对象只能存活到下一次垃圾收集发生为止。 Phantom Reference：最弱的引用关系。 存在目的：对象被收集器回收时收到一个系统通知。 对象是否有虚引用，完全不会对其生存空间构成影响，也无法通过虚引用来取得一个对象实例。 生存还是死亡可达性分析算法判定对象死亡的流程： 发现对象与GC Roots没有引用链，被标记一次 二次筛选，筛选条件是对象是否有必要执行 finalize()方法 对象没有覆盖finalize()方法或该方法被jvm调用过，则视为生存 对象被判定为有必要执行finalize()方法，该对象会被防止在F-Queue的队列中 jvm随后会自动建立一个低优先级的 Finalizer线程去执行队列中对象的 finalize()方法。（注：执行表示jvm会触发方法运行，但是不承诺会等到其运行结束）如果某个对象的finalize()方法执行慢，甚至死循环，就会导致整个队列永久性等待，甚至内存回收子系统崩溃 垃圾收集器会对F-Queue中的对象进行二次标记，如果对象在调用finalize()中重新与引用链上的任何一个对象建立关联就会被视为生存，也就会被移除该队列。 没有和引用链建立关联的对象会被回收。 总结：对象被回收需要经过两次标记，自救的机会只有一次，就是在第一次和第二次标记中用过finaliza()建立与引用链上的对象的关联即可。 /** * 对象被GC时的自我拯救 */ public class FinalizeEscapeGC { public static FinalizeEscapeGC SAVE_HOOK = null; public void isAlive(){ System.out.println(\"yes, i am still alive :\u003e\"); } @Override protected void finalize() throws Throwable { super.finalize(); System.out.println(\"finalize method executed!\"); FinalizeEscapeGC.SAVE_HOOK = this; } public static void main(String[] args) throws InterruptedException { SAVE_HOOK = new FinalizeEscapeGC(); // 对象第一次拯救自己 SAVE_HOOK = null; System.gc(); // Finalizer方法优先级低，暂停0.5s等它 Thread.sleep(500); if(SAVE_HOOK != null){ SAVE_HOOK.isAlive(); }else { System.out.println(\"no, i am dead :\u003c\"); } // 代码完全相同，只为证明它只能拯救一次 // 对象第二次拯救自己 SAVE_HOOK = null; System.gc(); // Finalizer方法优先级低，暂停0.5s等它 Thread.sleep(500); if(SAVE_HOOK != null){ SAVE_HOOK.isAlive(); }else { System.out.println(\"no, i am dead :\u003c\"); } } 运行结果 finalize method executed! yes, i am still alive :\u003e no, i am dead :\u003c 并不鼓励使用这种方式来拯救对象，并且要尽量避免使用它（虽然已经使用它） 重点：finalize()已经被弃用。 finalize()的弊端，一方面无法知道对象在F-Queue中的顺序，而是没办法空值GC发生的时间。因此通过该方法释放资源是危险的。 更好地代替方式：try-with-resources 代码块 和 AutoCloseable 接口来确保资源在代码块执行完毕之后被释放。 或者使用 PhantomReference虚引用 来实现对象被回收时触发的动作。 回收方法区jvm规范中提到可以不要求jvm在method area中实现垃圾收集， 如hotspot中元空间或永久代 方法区垃圾收集的性价比低（因为java堆中，尤其新生代中，一次能回收70%-99%的内存），而方法区回收的判定条件一般比较苛刻 方法区垃圾收集主要两部分内容： 废弃常量，回收过程与对象类似。如果没有被引用就会回收。 不再使用的类型，但是判定类型是否不再被使用条件苛刻，需同时满足3个条件： 该类所有的实例已被回收 加载该类的类加载器被回收（很难达成） 该类对应的class对象没有被引用","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:1","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#再谈引用"},{"categories":["java"],"content":"\r3.1 概述GC关注的是Java堆和方法区的内存管理，因为这两个区有着显著的不确定性，它们的内存分配和回收是动态的：一个接口的多个实现类需要的内存可能不一样；一个方法所执行的不同条件分支所需要的内存也可能不一样；只有处于运行期间，才知道程序究竟会创建哪些对象，多少个对象。 垃圾收集器在对堆进行回收前，就是确定对象是存活还是死亡。 引用计数算法Reference Counting原理： 每个对象中添加一个引用计数器 每当有一个地方引用它时，计数器值+1 当引用失效时，计数器值-1 任何时刻计数器为0的对象就是不可被引用 大多数情况下，reference counting是一个不错的算法（原理简单、判定效率高，虽然占用了一些额外的内存空间来进行记数）。但是在Java主流的jvm中没有选用该算法管理内存，主要原因是，该算法必须要配合大量额外处理才能保证正常工作，譬如单纯的引用计数就很难解决对象之间互相循环引用的问题。 objA和objB在互相引用，但是他们本身的属性只有一个instance并且是null，赋值令各自的instance = 对象obj。两个对象已经不可能再被引用了： 因为他们的各自的obj都是null，可以看到除了instance剩下的属性都是private，而instance是null。当值为null的时候，就表示它已经是失效的了。 但是二者又互相引用对方，导致它们的计数都不为0，所以，通过引用计数算法就无法回收他们。而通过内存回收日志可以看到，jvm并没有二者相互引用就放弃回收他们，侧面证明了，jvm不是通过引用计数算法来判断对象是否存活。 /** * testGC()方法执行后，ab会不会被GC？ */ public class ReferenceCountingGC { public Object instance = null; private static final int _1MB = 1024 * 1024; /** * 唯一充裕就是占内存，以便在GC日志中看清楚是否被回收过 */ private byte[] bigSize = new byte[2 * _1MB]; public static void testGC(){ ReferenceCountingGC objA = new ReferenceCountingGC(); ReferenceCountingGC objB = new ReferenceCountingGC(); objA.instance = objB; objB.instance = objA; // 假设在这行发生GC，a、b是都能被回收 System.gc(); } public static void main(String[] args) { testGC(); } } 可达性分析算法 Reachability Analysis算法思路：通过 GC Roots 的根对象作为起始节点集，从这些节点开始，根据引用关系向下搜索，搜索过程所走过的路径称为“引用链” Reference Chain，如果某个对象到 GC Roots 间没有任何引用链相连，或者用图论的话来说就是从 GC Roots 到这个对象不可达时，该对象是可能再被使用的。 固定可作为 GC Roots 的对象包括一下几种: 在vm stack（栈帧中的本地变量表）中引用的对象，譬如各个线程被调用的方法堆栈中使用到的参数、局部变量、临时变量等。 在method area 中类静态属性引用的对象、譬如Java类的旖旎用类型静态变量。 在method area 常量引用的对象，譬如字符串常量池（String table）里的引用。 在本地方法栈中 JNI（即 Native方法）引用的对象。 jvm内部的引用，如基本数据类型对应的Class对象，一些常驻的异常对象。（NullPointException、OutofMemoryError）等，还有系统类加载器。 所有被同步锁（synchronized关键字）持有的对象。 反应jvm内部却情况的JMXBean、JVMTI中注册的回调、本地代码缓存等。 最新的垃圾收集器欧具备了局部回收的特征，为了避免GC Roots包含过多对象而过度膨胀。 再谈引用引用如果只有未被引用和被引用就很狭隘，比如有些对象，我们希望当内存足够时就保留，内存不足就优先释放。 Strongly Reference：传统的引用定义，引用复制，类似 Object obj = new Object();，这种关系下，垃圾收集器永远不会回收被引用对象。 Soft Reference：引用有用但非必须的对象 内存溢出异常前，会把回收这些对象进行二次回收，这次回收后还没有足够内存才会抛出异常。 Weak Reference：比软引用更弱一下，弱引用关联的对象只能存活到下一次垃圾收集发生为止。 Phantom Reference：最弱的引用关系。 存在目的：对象被收集器回收时收到一个系统通知。 对象是否有虚引用，完全不会对其生存空间构成影响，也无法通过虚引用来取得一个对象实例。 生存还是死亡可达性分析算法判定对象死亡的流程： 发现对象与GC Roots没有引用链，被标记一次 二次筛选，筛选条件是对象是否有必要执行 finalize()方法 对象没有覆盖finalize()方法或该方法被jvm调用过，则视为生存 对象被判定为有必要执行finalize()方法，该对象会被防止在F-Queue的队列中 jvm随后会自动建立一个低优先级的 Finalizer线程去执行队列中对象的 finalize()方法。（注：执行表示jvm会触发方法运行，但是不承诺会等到其运行结束）如果某个对象的finalize()方法执行慢，甚至死循环，就会导致整个队列永久性等待，甚至内存回收子系统崩溃 垃圾收集器会对F-Queue中的对象进行二次标记，如果对象在调用finalize()中重新与引用链上的任何一个对象建立关联就会被视为生存，也就会被移除该队列。 没有和引用链建立关联的对象会被回收。 总结：对象被回收需要经过两次标记，自救的机会只有一次，就是在第一次和第二次标记中用过finaliza()建立与引用链上的对象的关联即可。 /** * 对象被GC时的自我拯救 */ public class FinalizeEscapeGC { public static FinalizeEscapeGC SAVE_HOOK = null; public void isAlive(){ System.out.println(\"yes, i am still alive :\u003e\"); } @Override protected void finalize() throws Throwable { super.finalize(); System.out.println(\"finalize method executed!\"); FinalizeEscapeGC.SAVE_HOOK = this; } public static void main(String[] args) throws InterruptedException { SAVE_HOOK = new FinalizeEscapeGC(); // 对象第一次拯救自己 SAVE_HOOK = null; System.gc(); // Finalizer方法优先级低，暂停0.5s等它 Thread.sleep(500); if(SAVE_HOOK != null){ SAVE_HOOK.isAlive(); }else { System.out.println(\"no, i am dead :\u003c\"); } // 代码完全相同，只为证明它只能拯救一次 // 对象第二次拯救自己 SAVE_HOOK = null; System.gc(); // Finalizer方法优先级低，暂停0.5s等它 Thread.sleep(500); if(SAVE_HOOK != null){ SAVE_HOOK.isAlive(); }else { System.out.println(\"no, i am dead :\u003c\"); } } 运行结果 finalize method executed! yes, i am still alive :\u003e no, i am dead :\u003c 并不鼓励使用这种方式来拯救对象，并且要尽量避免使用它（虽然已经使用它） 重点：finalize()已经被弃用。 finalize()的弊端，一方面无法知道对象在F-Queue中的顺序，而是没办法空值GC发生的时间。因此通过该方法释放资源是危险的。 更好地代替方式：try-with-resources 代码块 和 AutoCloseable 接口来确保资源在代码块执行完毕之后被释放。 或者使用 PhantomReference虚引用 来实现对象被回收时触发的动作。 回收方法区jvm规范中提到可以不要求jvm在method area中实现垃圾收集， 如hotspot中元空间或永久代 方法区垃圾收集的性价比低（因为java堆中，尤其新生代中，一次能回收70%-99%的内存），而方法区回收的判定条件一般比较苛刻 方法区垃圾收集主要两部分内容： 废弃常量，回收过程与对象类似。如果没有被引用就会回收。 不再使用的类型，但是判定类型是否不再被使用条件苛刻，需同时满足3个条件： 该类所有的实例已被回收 加载该类的类加载器被回收（很难达成） 该类对应的class对象没有被引用","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:1","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#生存还是死亡"},{"categories":["java"],"content":"\r3.1 概述GC关注的是Java堆和方法区的内存管理，因为这两个区有着显著的不确定性，它们的内存分配和回收是动态的：一个接口的多个实现类需要的内存可能不一样；一个方法所执行的不同条件分支所需要的内存也可能不一样；只有处于运行期间，才知道程序究竟会创建哪些对象，多少个对象。 垃圾收集器在对堆进行回收前，就是确定对象是存活还是死亡。 引用计数算法Reference Counting原理： 每个对象中添加一个引用计数器 每当有一个地方引用它时，计数器值+1 当引用失效时，计数器值-1 任何时刻计数器为0的对象就是不可被引用 大多数情况下，reference counting是一个不错的算法（原理简单、判定效率高，虽然占用了一些额外的内存空间来进行记数）。但是在Java主流的jvm中没有选用该算法管理内存，主要原因是，该算法必须要配合大量额外处理才能保证正常工作，譬如单纯的引用计数就很难解决对象之间互相循环引用的问题。 objA和objB在互相引用，但是他们本身的属性只有一个instance并且是null，赋值令各自的instance = 对象obj。两个对象已经不可能再被引用了： 因为他们的各自的obj都是null，可以看到除了instance剩下的属性都是private，而instance是null。当值为null的时候，就表示它已经是失效的了。 但是二者又互相引用对方，导致它们的计数都不为0，所以，通过引用计数算法就无法回收他们。而通过内存回收日志可以看到，jvm并没有二者相互引用就放弃回收他们，侧面证明了，jvm不是通过引用计数算法来判断对象是否存活。 /** * testGC()方法执行后，ab会不会被GC？ */ public class ReferenceCountingGC { public Object instance = null; private static final int _1MB = 1024 * 1024; /** * 唯一充裕就是占内存，以便在GC日志中看清楚是否被回收过 */ private byte[] bigSize = new byte[2 * _1MB]; public static void testGC(){ ReferenceCountingGC objA = new ReferenceCountingGC(); ReferenceCountingGC objB = new ReferenceCountingGC(); objA.instance = objB; objB.instance = objA; // 假设在这行发生GC，a、b是都能被回收 System.gc(); } public static void main(String[] args) { testGC(); } } 可达性分析算法 Reachability Analysis算法思路：通过 GC Roots 的根对象作为起始节点集，从这些节点开始，根据引用关系向下搜索，搜索过程所走过的路径称为“引用链” Reference Chain，如果某个对象到 GC Roots 间没有任何引用链相连，或者用图论的话来说就是从 GC Roots 到这个对象不可达时，该对象是可能再被使用的。 固定可作为 GC Roots 的对象包括一下几种: 在vm stack（栈帧中的本地变量表）中引用的对象，譬如各个线程被调用的方法堆栈中使用到的参数、局部变量、临时变量等。 在method area 中类静态属性引用的对象、譬如Java类的旖旎用类型静态变量。 在method area 常量引用的对象，譬如字符串常量池（String table）里的引用。 在本地方法栈中 JNI（即 Native方法）引用的对象。 jvm内部的引用，如基本数据类型对应的Class对象，一些常驻的异常对象。（NullPointException、OutofMemoryError）等，还有系统类加载器。 所有被同步锁（synchronized关键字）持有的对象。 反应jvm内部却情况的JMXBean、JVMTI中注册的回调、本地代码缓存等。 最新的垃圾收集器欧具备了局部回收的特征，为了避免GC Roots包含过多对象而过度膨胀。 再谈引用引用如果只有未被引用和被引用就很狭隘，比如有些对象，我们希望当内存足够时就保留，内存不足就优先释放。 Strongly Reference：传统的引用定义，引用复制，类似 Object obj = new Object();，这种关系下，垃圾收集器永远不会回收被引用对象。 Soft Reference：引用有用但非必须的对象 内存溢出异常前，会把回收这些对象进行二次回收，这次回收后还没有足够内存才会抛出异常。 Weak Reference：比软引用更弱一下，弱引用关联的对象只能存活到下一次垃圾收集发生为止。 Phantom Reference：最弱的引用关系。 存在目的：对象被收集器回收时收到一个系统通知。 对象是否有虚引用，完全不会对其生存空间构成影响，也无法通过虚引用来取得一个对象实例。 生存还是死亡可达性分析算法判定对象死亡的流程： 发现对象与GC Roots没有引用链，被标记一次 二次筛选，筛选条件是对象是否有必要执行 finalize()方法 对象没有覆盖finalize()方法或该方法被jvm调用过，则视为生存 对象被判定为有必要执行finalize()方法，该对象会被防止在F-Queue的队列中 jvm随后会自动建立一个低优先级的 Finalizer线程去执行队列中对象的 finalize()方法。（注：执行表示jvm会触发方法运行，但是不承诺会等到其运行结束）如果某个对象的finalize()方法执行慢，甚至死循环，就会导致整个队列永久性等待，甚至内存回收子系统崩溃 垃圾收集器会对F-Queue中的对象进行二次标记，如果对象在调用finalize()中重新与引用链上的任何一个对象建立关联就会被视为生存，也就会被移除该队列。 没有和引用链建立关联的对象会被回收。 总结：对象被回收需要经过两次标记，自救的机会只有一次，就是在第一次和第二次标记中用过finaliza()建立与引用链上的对象的关联即可。 /** * 对象被GC时的自我拯救 */ public class FinalizeEscapeGC { public static FinalizeEscapeGC SAVE_HOOK = null; public void isAlive(){ System.out.println(\"yes, i am still alive :\u003e\"); } @Override protected void finalize() throws Throwable { super.finalize(); System.out.println(\"finalize method executed!\"); FinalizeEscapeGC.SAVE_HOOK = this; } public static void main(String[] args) throws InterruptedException { SAVE_HOOK = new FinalizeEscapeGC(); // 对象第一次拯救自己 SAVE_HOOK = null; System.gc(); // Finalizer方法优先级低，暂停0.5s等它 Thread.sleep(500); if(SAVE_HOOK != null){ SAVE_HOOK.isAlive(); }else { System.out.println(\"no, i am dead :\u003c\"); } // 代码完全相同，只为证明它只能拯救一次 // 对象第二次拯救自己 SAVE_HOOK = null; System.gc(); // Finalizer方法优先级低，暂停0.5s等它 Thread.sleep(500); if(SAVE_HOOK != null){ SAVE_HOOK.isAlive(); }else { System.out.println(\"no, i am dead :\u003c\"); } } 运行结果 finalize method executed! yes, i am still alive :\u003e no, i am dead :\u003c 并不鼓励使用这种方式来拯救对象，并且要尽量避免使用它（虽然已经使用它） 重点：finalize()已经被弃用。 finalize()的弊端，一方面无法知道对象在F-Queue中的顺序，而是没办法空值GC发生的时间。因此通过该方法释放资源是危险的。 更好地代替方式：try-with-resources 代码块 和 AutoCloseable 接口来确保资源在代码块执行完毕之后被释放。 或者使用 PhantomReference虚引用 来实现对象被回收时触发的动作。 回收方法区jvm规范中提到可以不要求jvm在method area中实现垃圾收集， 如hotspot中元空间或永久代 方法区垃圾收集的性价比低（因为java堆中，尤其新生代中，一次能回收70%-99%的内存），而方法区回收的判定条件一般比较苛刻 方法区垃圾收集主要两部分内容： 废弃常量，回收过程与对象类似。如果没有被引用就会回收。 不再使用的类型，但是判定类型是否不再被使用条件苛刻，需同时满足3个条件： 该类所有的实例已被回收 加载该类的类加载器被回收（很难达成） 该类对应的class对象没有被引用","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:1","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#回收方法区"},{"categories":["java"],"content":"\r3.2 垃圾收集算法\r分代收集理论主流垃圾收集器大多遵循 分代收集 Generational Collection的理论，它联立在两个假说之上： 弱分代假说 weak generatiinal hypothesis：绝大多数对象都是朝生夕灭 强分代假说 strong generational hypothesis：熬过越多次垃圾收集的对象就越难消亡 根据假说得到的设计原则：将java堆分出不同的区域并根据对象年龄分配到不同区存储。在jvm中至少会分为： 新生代 young generation，大批对象死去，每次回收后存活的少量对象，将晋升到老年代中存放。 老年代 old generation 第三条假说：跨代引用假说 intergeneraational reference hypothesis（相比于同代引用占比极小），引入该假说的原因是：在新生代中存在一些不被新生代引用但却被老年代引用的对象，内存回收中需要考虑这种情况。 名词解释： 部分收集 partial GC：目标不是完整收集整个java stack的垃圾回收，其中又分为： 新生代收集 minor GC / young GC 老年代收集 major GC / old GC：目前只有 CMS收集器会有单独的老年代 混合收集 mixed GC：收集整个新生代以及部分老年代的垃圾收集，目前只有G1收集器有 整堆收集 full GC：收集整个java stack 和 method area的垃圾收集 标记-清除算法 mark-sweep最早也是最基础的垃圾收集算法，先标记后清除。 缺点：执行效率不稳定，效率随着对象数量的增加成反比；内存空间碎片化，标记、清除都会产生不连续的内存碎片 标记-复制算法（基于半区复制算法 semispace copying）为解决标记清除算法效率低而创造，原理是，将可用内存划分出相等两块，每次只使用其中一个。如果内存用完了，就将上面存活的对象复制到另一个内存上，然后把该内存空间都清理掉。 优点，完美解决标记清除算法面对大量清除对象时的低效问题 缺点，内存空间浪费。 但实际使用中，新生代98%的对象都逃不脱第一轮收集，因此，并不需要 1:1 来划分新生代内存空间。 更优化的半区复制分代策略： Appel式回收： 思路：把新生代分为一大（eden）二小（survivor）的空间，每次分配内存只是有eden和其中一个survivor。垃圾回收时将这两个存活的对象一次性复制到另一个survivor上，然后清除这两个空间。HotSpot上默认分配的二者空间比例是 8:1。 空间利用率：每次新生代占整内存空间的90%，剩下10%是一个survivor，等着备用。 逃生门安全设计：当survivor内存空间不足以容纳eden和另一个survivor上的对象时，就依赖其他内存空间进行分额担保 handel promotion。 标记-整理算法 mark compact解决标记复制算法对对象存活率较高而进行较多的复制导致效率低的问题（针对老年代对象的死亡特征）。 思路：标记和前面一样，后续步骤不是直接对回收对象清理，而是让所有存活的对象都向内存空间一端移动，然后直接清理掉边界以外的内存。 标记清除和标记整理的本质差异：后者是前者的移动式回收。 移动式的风险决策： 在有大量对象存活的老年代区，移动它们并更新所有引用是一个极为负重的操作，移动过程中必须全程暂停用户应用程序才可进行，被称为 stop the world。 完全不考虑移动和整理存活对象，会造成空间碎片化问题，只能依赖复杂的内存分配器和内存访问器来解决。如果负担太大，会影响应用程序的吞吐量。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#32-垃圾收集算法"},{"categories":["java"],"content":"\r3.2 垃圾收集算法\r分代收集理论主流垃圾收集器大多遵循 分代收集 Generational Collection的理论，它联立在两个假说之上： 弱分代假说 weak generatiinal hypothesis：绝大多数对象都是朝生夕灭 强分代假说 strong generational hypothesis：熬过越多次垃圾收集的对象就越难消亡 根据假说得到的设计原则：将java堆分出不同的区域并根据对象年龄分配到不同区存储。在jvm中至少会分为： 新生代 young generation，大批对象死去，每次回收后存活的少量对象，将晋升到老年代中存放。 老年代 old generation 第三条假说：跨代引用假说 intergeneraational reference hypothesis（相比于同代引用占比极小），引入该假说的原因是：在新生代中存在一些不被新生代引用但却被老年代引用的对象，内存回收中需要考虑这种情况。 名词解释： 部分收集 partial GC：目标不是完整收集整个java stack的垃圾回收，其中又分为： 新生代收集 minor GC / young GC 老年代收集 major GC / old GC：目前只有 CMS收集器会有单独的老年代 混合收集 mixed GC：收集整个新生代以及部分老年代的垃圾收集，目前只有G1收集器有 整堆收集 full GC：收集整个java stack 和 method area的垃圾收集 标记-清除算法 mark-sweep最早也是最基础的垃圾收集算法，先标记后清除。 缺点：执行效率不稳定，效率随着对象数量的增加成反比；内存空间碎片化，标记、清除都会产生不连续的内存碎片 标记-复制算法（基于半区复制算法 semispace copying）为解决标记清除算法效率低而创造，原理是，将可用内存划分出相等两块，每次只使用其中一个。如果内存用完了，就将上面存活的对象复制到另一个内存上，然后把该内存空间都清理掉。 优点，完美解决标记清除算法面对大量清除对象时的低效问题 缺点，内存空间浪费。 但实际使用中，新生代98%的对象都逃不脱第一轮收集，因此，并不需要 1:1 来划分新生代内存空间。 更优化的半区复制分代策略： Appel式回收： 思路：把新生代分为一大（eden）二小（survivor）的空间，每次分配内存只是有eden和其中一个survivor。垃圾回收时将这两个存活的对象一次性复制到另一个survivor上，然后清除这两个空间。HotSpot上默认分配的二者空间比例是 8:1。 空间利用率：每次新生代占整内存空间的90%，剩下10%是一个survivor，等着备用。 逃生门安全设计：当survivor内存空间不足以容纳eden和另一个survivor上的对象时，就依赖其他内存空间进行分额担保 handel promotion。 标记-整理算法 mark compact解决标记复制算法对对象存活率较高而进行较多的复制导致效率低的问题（针对老年代对象的死亡特征）。 思路：标记和前面一样，后续步骤不是直接对回收对象清理，而是让所有存活的对象都向内存空间一端移动，然后直接清理掉边界以外的内存。 标记清除和标记整理的本质差异：后者是前者的移动式回收。 移动式的风险决策： 在有大量对象存活的老年代区，移动它们并更新所有引用是一个极为负重的操作，移动过程中必须全程暂停用户应用程序才可进行，被称为 stop the world。 完全不考虑移动和整理存活对象，会造成空间碎片化问题，只能依赖复杂的内存分配器和内存访问器来解决。如果负担太大，会影响应用程序的吞吐量。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#分代收集理论"},{"categories":["java"],"content":"\r3.2 垃圾收集算法\r分代收集理论主流垃圾收集器大多遵循 分代收集 Generational Collection的理论，它联立在两个假说之上： 弱分代假说 weak generatiinal hypothesis：绝大多数对象都是朝生夕灭 强分代假说 strong generational hypothesis：熬过越多次垃圾收集的对象就越难消亡 根据假说得到的设计原则：将java堆分出不同的区域并根据对象年龄分配到不同区存储。在jvm中至少会分为： 新生代 young generation，大批对象死去，每次回收后存活的少量对象，将晋升到老年代中存放。 老年代 old generation 第三条假说：跨代引用假说 intergeneraational reference hypothesis（相比于同代引用占比极小），引入该假说的原因是：在新生代中存在一些不被新生代引用但却被老年代引用的对象，内存回收中需要考虑这种情况。 名词解释： 部分收集 partial GC：目标不是完整收集整个java stack的垃圾回收，其中又分为： 新生代收集 minor GC / young GC 老年代收集 major GC / old GC：目前只有 CMS收集器会有单独的老年代 混合收集 mixed GC：收集整个新生代以及部分老年代的垃圾收集，目前只有G1收集器有 整堆收集 full GC：收集整个java stack 和 method area的垃圾收集 标记-清除算法 mark-sweep最早也是最基础的垃圾收集算法，先标记后清除。 缺点：执行效率不稳定，效率随着对象数量的增加成反比；内存空间碎片化，标记、清除都会产生不连续的内存碎片 标记-复制算法（基于半区复制算法 semispace copying）为解决标记清除算法效率低而创造，原理是，将可用内存划分出相等两块，每次只使用其中一个。如果内存用完了，就将上面存活的对象复制到另一个内存上，然后把该内存空间都清理掉。 优点，完美解决标记清除算法面对大量清除对象时的低效问题 缺点，内存空间浪费。 但实际使用中，新生代98%的对象都逃不脱第一轮收集，因此，并不需要 1:1 来划分新生代内存空间。 更优化的半区复制分代策略： Appel式回收： 思路：把新生代分为一大（eden）二小（survivor）的空间，每次分配内存只是有eden和其中一个survivor。垃圾回收时将这两个存活的对象一次性复制到另一个survivor上，然后清除这两个空间。HotSpot上默认分配的二者空间比例是 8:1。 空间利用率：每次新生代占整内存空间的90%，剩下10%是一个survivor，等着备用。 逃生门安全设计：当survivor内存空间不足以容纳eden和另一个survivor上的对象时，就依赖其他内存空间进行分额担保 handel promotion。 标记-整理算法 mark compact解决标记复制算法对对象存活率较高而进行较多的复制导致效率低的问题（针对老年代对象的死亡特征）。 思路：标记和前面一样，后续步骤不是直接对回收对象清理，而是让所有存活的对象都向内存空间一端移动，然后直接清理掉边界以外的内存。 标记清除和标记整理的本质差异：后者是前者的移动式回收。 移动式的风险决策： 在有大量对象存活的老年代区，移动它们并更新所有引用是一个极为负重的操作，移动过程中必须全程暂停用户应用程序才可进行，被称为 stop the world。 完全不考虑移动和整理存活对象，会造成空间碎片化问题，只能依赖复杂的内存分配器和内存访问器来解决。如果负担太大，会影响应用程序的吞吐量。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#标记-清除算法-mark-sweep"},{"categories":["java"],"content":"\r3.2 垃圾收集算法\r分代收集理论主流垃圾收集器大多遵循 分代收集 Generational Collection的理论，它联立在两个假说之上： 弱分代假说 weak generatiinal hypothesis：绝大多数对象都是朝生夕灭 强分代假说 strong generational hypothesis：熬过越多次垃圾收集的对象就越难消亡 根据假说得到的设计原则：将java堆分出不同的区域并根据对象年龄分配到不同区存储。在jvm中至少会分为： 新生代 young generation，大批对象死去，每次回收后存活的少量对象，将晋升到老年代中存放。 老年代 old generation 第三条假说：跨代引用假说 intergeneraational reference hypothesis（相比于同代引用占比极小），引入该假说的原因是：在新生代中存在一些不被新生代引用但却被老年代引用的对象，内存回收中需要考虑这种情况。 名词解释： 部分收集 partial GC：目标不是完整收集整个java stack的垃圾回收，其中又分为： 新生代收集 minor GC / young GC 老年代收集 major GC / old GC：目前只有 CMS收集器会有单独的老年代 混合收集 mixed GC：收集整个新生代以及部分老年代的垃圾收集，目前只有G1收集器有 整堆收集 full GC：收集整个java stack 和 method area的垃圾收集 标记-清除算法 mark-sweep最早也是最基础的垃圾收集算法，先标记后清除。 缺点：执行效率不稳定，效率随着对象数量的增加成反比；内存空间碎片化，标记、清除都会产生不连续的内存碎片 标记-复制算法（基于半区复制算法 semispace copying）为解决标记清除算法效率低而创造，原理是，将可用内存划分出相等两块，每次只使用其中一个。如果内存用完了，就将上面存活的对象复制到另一个内存上，然后把该内存空间都清理掉。 优点，完美解决标记清除算法面对大量清除对象时的低效问题 缺点，内存空间浪费。 但实际使用中，新生代98%的对象都逃不脱第一轮收集，因此，并不需要 1:1 来划分新生代内存空间。 更优化的半区复制分代策略： Appel式回收： 思路：把新生代分为一大（eden）二小（survivor）的空间，每次分配内存只是有eden和其中一个survivor。垃圾回收时将这两个存活的对象一次性复制到另一个survivor上，然后清除这两个空间。HotSpot上默认分配的二者空间比例是 8:1。 空间利用率：每次新生代占整内存空间的90%，剩下10%是一个survivor，等着备用。 逃生门安全设计：当survivor内存空间不足以容纳eden和另一个survivor上的对象时，就依赖其他内存空间进行分额担保 handel promotion。 标记-整理算法 mark compact解决标记复制算法对对象存活率较高而进行较多的复制导致效率低的问题（针对老年代对象的死亡特征）。 思路：标记和前面一样，后续步骤不是直接对回收对象清理，而是让所有存活的对象都向内存空间一端移动，然后直接清理掉边界以外的内存。 标记清除和标记整理的本质差异：后者是前者的移动式回收。 移动式的风险决策： 在有大量对象存活的老年代区，移动它们并更新所有引用是一个极为负重的操作，移动过程中必须全程暂停用户应用程序才可进行，被称为 stop the world。 完全不考虑移动和整理存活对象，会造成空间碎片化问题，只能依赖复杂的内存分配器和内存访问器来解决。如果负担太大，会影响应用程序的吞吐量。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#标记-复制算法基于半区复制算法-semispace-copying"},{"categories":["java"],"content":"\r3.2 垃圾收集算法\r分代收集理论主流垃圾收集器大多遵循 分代收集 Generational Collection的理论，它联立在两个假说之上： 弱分代假说 weak generatiinal hypothesis：绝大多数对象都是朝生夕灭 强分代假说 strong generational hypothesis：熬过越多次垃圾收集的对象就越难消亡 根据假说得到的设计原则：将java堆分出不同的区域并根据对象年龄分配到不同区存储。在jvm中至少会分为： 新生代 young generation，大批对象死去，每次回收后存活的少量对象，将晋升到老年代中存放。 老年代 old generation 第三条假说：跨代引用假说 intergeneraational reference hypothesis（相比于同代引用占比极小），引入该假说的原因是：在新生代中存在一些不被新生代引用但却被老年代引用的对象，内存回收中需要考虑这种情况。 名词解释： 部分收集 partial GC：目标不是完整收集整个java stack的垃圾回收，其中又分为： 新生代收集 minor GC / young GC 老年代收集 major GC / old GC：目前只有 CMS收集器会有单独的老年代 混合收集 mixed GC：收集整个新生代以及部分老年代的垃圾收集，目前只有G1收集器有 整堆收集 full GC：收集整个java stack 和 method area的垃圾收集 标记-清除算法 mark-sweep最早也是最基础的垃圾收集算法，先标记后清除。 缺点：执行效率不稳定，效率随着对象数量的增加成反比；内存空间碎片化，标记、清除都会产生不连续的内存碎片 标记-复制算法（基于半区复制算法 semispace copying）为解决标记清除算法效率低而创造，原理是，将可用内存划分出相等两块，每次只使用其中一个。如果内存用完了，就将上面存活的对象复制到另一个内存上，然后把该内存空间都清理掉。 优点，完美解决标记清除算法面对大量清除对象时的低效问题 缺点，内存空间浪费。 但实际使用中，新生代98%的对象都逃不脱第一轮收集，因此，并不需要 1:1 来划分新生代内存空间。 更优化的半区复制分代策略： Appel式回收： 思路：把新生代分为一大（eden）二小（survivor）的空间，每次分配内存只是有eden和其中一个survivor。垃圾回收时将这两个存活的对象一次性复制到另一个survivor上，然后清除这两个空间。HotSpot上默认分配的二者空间比例是 8:1。 空间利用率：每次新生代占整内存空间的90%，剩下10%是一个survivor，等着备用。 逃生门安全设计：当survivor内存空间不足以容纳eden和另一个survivor上的对象时，就依赖其他内存空间进行分额担保 handel promotion。 标记-整理算法 mark compact解决标记复制算法对对象存活率较高而进行较多的复制导致效率低的问题（针对老年代对象的死亡特征）。 思路：标记和前面一样，后续步骤不是直接对回收对象清理，而是让所有存活的对象都向内存空间一端移动，然后直接清理掉边界以外的内存。 标记清除和标记整理的本质差异：后者是前者的移动式回收。 移动式的风险决策： 在有大量对象存活的老年代区，移动它们并更新所有引用是一个极为负重的操作，移动过程中必须全程暂停用户应用程序才可进行，被称为 stop the world。 完全不考虑移动和整理存活对象，会造成空间碎片化问题，只能依赖复杂的内存分配器和内存访问器来解决。如果负担太大，会影响应用程序的吞吐量。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:2","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#标记-整理算法-mark-compact"},{"categories":["java"],"content":"\r3.3 HotSopt算法细节的实现该章节主要是为了介绍各款垃圾收集器做前置知识铺垫。 根节点枚举根枚举节点：找出所有的 GC Roots（主要在全局性的引用，如常量或类静态属性，与执行上下文，如栈帧中的本地变量表） 弊端：根节点枚举是必须暂停用户线程的，stop the world困扰。它必须在一个能够保障一致性的快照中才能进行，一致性指的是： 整个枚举期间，不会出现根节点集合的对象引用关系还在不断变化，这是分析准确性的标准。这也是导致垃圾回收过程中必须停顿所有的用户线程的重要原因。 实际中，HotSpot在用户线程停顿下来后，也并不会全都检查完，它使用了OopMap的数据结构用于枚举 GC Roots，将外部记录下类型信息，存成映射表。生成的方式就是遍历原始映射表/对象，得到一个个偏移量，取偏移量上的类型数据存到oppmap上。 OopMap的协助下，hotspot可以快速准确地完成 GC Roots枚举。 安全点理论上，每一条指令都生成对应的OopMap，就会需要大量额外存储空间，空间成本增加。 实际上，hotspot并没有每条指令生成一个OopMap，只是在特定位置记录这些信息，这个位置就被称为 安全点 Safepoint。 限制条件：程序执行时，不是在任意代码指令流都能停顿下来进行垃圾回收，而是强制必须在安全点才能够暂停。 安全点的标准：选定点不能太少，因为选的太少就会导致每次收集等待时间过长；也不能频繁，以至于增加运行时内存负荷。遵守以“是否具有让程序长时间执行的特征”来选定。长时间执行的特征就是指令序列的复用，如方法调用、循环跳转、异常跳转等，这种指令才会产生安全点。 如何在发生垃圾收集时让所有线程（除JNI调用的线程）都跑到最近的安全点并停顿下来。这里有两个选择： 抢先式中断 preemptive suspension（几乎弃用）：执行代码不用关注中断问题，系统会在GC时，把线程中断，如果有不在安全点的线程，就恢复其线程执行，然后过一会再重新中断，知道全都在安全点上。 主动式中断 voluntary suspension：中断线程时，不对线程操作，只设置一个标志位，各线程执行中会不停地主动轮询该标志，如果是中断标记为真，线程就在最近的安全点主动中断挂起。轮询标志点和安全点是重合的。 安全区域 Safe Region安全点机制保证了程序执行时，如何停顿用户线程，让jvm进去垃圾回收状态的问题。 程序不执行（线程无法响应jvm中断的请求）时，怎么办？ 安全区域可以解决，它是能够确保在某一段代码中，引用关系不会变化，因此，该区域中任意地方开始垃圾回收都是安全的。通俗理解，安全区域就是被扩展拉伸了的安全点。 实现：线程执行到安全区，首先会表示进入到安全区，jvm在垃圾收集时会忽略已经声明在安全区的线程。这些线程要离开安全区域时，就要检查jvm是否完成了 根节点枚举，如果完成就继续执行，否则就一直等待直到收到信号。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:3","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#33-hotsopt算法细节的实现"},{"categories":["java"],"content":"\r3.3 HotSopt算法细节的实现该章节主要是为了介绍各款垃圾收集器做前置知识铺垫。 根节点枚举根枚举节点：找出所有的 GC Roots（主要在全局性的引用，如常量或类静态属性，与执行上下文，如栈帧中的本地变量表） 弊端：根节点枚举是必须暂停用户线程的，stop the world困扰。它必须在一个能够保障一致性的快照中才能进行，一致性指的是： 整个枚举期间，不会出现根节点集合的对象引用关系还在不断变化，这是分析准确性的标准。这也是导致垃圾回收过程中必须停顿所有的用户线程的重要原因。 实际中，HotSpot在用户线程停顿下来后，也并不会全都检查完，它使用了OopMap的数据结构用于枚举 GC Roots，将外部记录下类型信息，存成映射表。生成的方式就是遍历原始映射表/对象，得到一个个偏移量，取偏移量上的类型数据存到oppmap上。 OopMap的协助下，hotspot可以快速准确地完成 GC Roots枚举。 安全点理论上，每一条指令都生成对应的OopMap，就会需要大量额外存储空间，空间成本增加。 实际上，hotspot并没有每条指令生成一个OopMap，只是在特定位置记录这些信息，这个位置就被称为 安全点 Safepoint。 限制条件：程序执行时，不是在任意代码指令流都能停顿下来进行垃圾回收，而是强制必须在安全点才能够暂停。 安全点的标准：选定点不能太少，因为选的太少就会导致每次收集等待时间过长；也不能频繁，以至于增加运行时内存负荷。遵守以“是否具有让程序长时间执行的特征”来选定。长时间执行的特征就是指令序列的复用，如方法调用、循环跳转、异常跳转等，这种指令才会产生安全点。 如何在发生垃圾收集时让所有线程（除JNI调用的线程）都跑到最近的安全点并停顿下来。这里有两个选择： 抢先式中断 preemptive suspension（几乎弃用）：执行代码不用关注中断问题，系统会在GC时，把线程中断，如果有不在安全点的线程，就恢复其线程执行，然后过一会再重新中断，知道全都在安全点上。 主动式中断 voluntary suspension：中断线程时，不对线程操作，只设置一个标志位，各线程执行中会不停地主动轮询该标志，如果是中断标记为真，线程就在最近的安全点主动中断挂起。轮询标志点和安全点是重合的。 安全区域 Safe Region安全点机制保证了程序执行时，如何停顿用户线程，让jvm进去垃圾回收状态的问题。 程序不执行（线程无法响应jvm中断的请求）时，怎么办？ 安全区域可以解决，它是能够确保在某一段代码中，引用关系不会变化，因此，该区域中任意地方开始垃圾回收都是安全的。通俗理解，安全区域就是被扩展拉伸了的安全点。 实现：线程执行到安全区，首先会表示进入到安全区，jvm在垃圾收集时会忽略已经声明在安全区的线程。这些线程要离开安全区域时，就要检查jvm是否完成了 根节点枚举，如果完成就继续执行，否则就一直等待直到收到信号。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:3","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#根节点枚举"},{"categories":["java"],"content":"\r3.3 HotSopt算法细节的实现该章节主要是为了介绍各款垃圾收集器做前置知识铺垫。 根节点枚举根枚举节点：找出所有的 GC Roots（主要在全局性的引用，如常量或类静态属性，与执行上下文，如栈帧中的本地变量表） 弊端：根节点枚举是必须暂停用户线程的，stop the world困扰。它必须在一个能够保障一致性的快照中才能进行，一致性指的是： 整个枚举期间，不会出现根节点集合的对象引用关系还在不断变化，这是分析准确性的标准。这也是导致垃圾回收过程中必须停顿所有的用户线程的重要原因。 实际中，HotSpot在用户线程停顿下来后，也并不会全都检查完，它使用了OopMap的数据结构用于枚举 GC Roots，将外部记录下类型信息，存成映射表。生成的方式就是遍历原始映射表/对象，得到一个个偏移量，取偏移量上的类型数据存到oppmap上。 OopMap的协助下，hotspot可以快速准确地完成 GC Roots枚举。 安全点理论上，每一条指令都生成对应的OopMap，就会需要大量额外存储空间，空间成本增加。 实际上，hotspot并没有每条指令生成一个OopMap，只是在特定位置记录这些信息，这个位置就被称为 安全点 Safepoint。 限制条件：程序执行时，不是在任意代码指令流都能停顿下来进行垃圾回收，而是强制必须在安全点才能够暂停。 安全点的标准：选定点不能太少，因为选的太少就会导致每次收集等待时间过长；也不能频繁，以至于增加运行时内存负荷。遵守以“是否具有让程序长时间执行的特征”来选定。长时间执行的特征就是指令序列的复用，如方法调用、循环跳转、异常跳转等，这种指令才会产生安全点。 如何在发生垃圾收集时让所有线程（除JNI调用的线程）都跑到最近的安全点并停顿下来。这里有两个选择： 抢先式中断 preemptive suspension（几乎弃用）：执行代码不用关注中断问题，系统会在GC时，把线程中断，如果有不在安全点的线程，就恢复其线程执行，然后过一会再重新中断，知道全都在安全点上。 主动式中断 voluntary suspension：中断线程时，不对线程操作，只设置一个标志位，各线程执行中会不停地主动轮询该标志，如果是中断标记为真，线程就在最近的安全点主动中断挂起。轮询标志点和安全点是重合的。 安全区域 Safe Region安全点机制保证了程序执行时，如何停顿用户线程，让jvm进去垃圾回收状态的问题。 程序不执行（线程无法响应jvm中断的请求）时，怎么办？ 安全区域可以解决，它是能够确保在某一段代码中，引用关系不会变化，因此，该区域中任意地方开始垃圾回收都是安全的。通俗理解，安全区域就是被扩展拉伸了的安全点。 实现：线程执行到安全区，首先会表示进入到安全区，jvm在垃圾收集时会忽略已经声明在安全区的线程。这些线程要离开安全区域时，就要检查jvm是否完成了 根节点枚举，如果完成就继续执行，否则就一直等待直到收到信号。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:3","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#安全点"},{"categories":["java"],"content":"\r3.3 HotSopt算法细节的实现该章节主要是为了介绍各款垃圾收集器做前置知识铺垫。 根节点枚举根枚举节点：找出所有的 GC Roots（主要在全局性的引用，如常量或类静态属性，与执行上下文，如栈帧中的本地变量表） 弊端：根节点枚举是必须暂停用户线程的，stop the world困扰。它必须在一个能够保障一致性的快照中才能进行，一致性指的是： 整个枚举期间，不会出现根节点集合的对象引用关系还在不断变化，这是分析准确性的标准。这也是导致垃圾回收过程中必须停顿所有的用户线程的重要原因。 实际中，HotSpot在用户线程停顿下来后，也并不会全都检查完，它使用了OopMap的数据结构用于枚举 GC Roots，将外部记录下类型信息，存成映射表。生成的方式就是遍历原始映射表/对象，得到一个个偏移量，取偏移量上的类型数据存到oppmap上。 OopMap的协助下，hotspot可以快速准确地完成 GC Roots枚举。 安全点理论上，每一条指令都生成对应的OopMap，就会需要大量额外存储空间，空间成本增加。 实际上，hotspot并没有每条指令生成一个OopMap，只是在特定位置记录这些信息，这个位置就被称为 安全点 Safepoint。 限制条件：程序执行时，不是在任意代码指令流都能停顿下来进行垃圾回收，而是强制必须在安全点才能够暂停。 安全点的标准：选定点不能太少，因为选的太少就会导致每次收集等待时间过长；也不能频繁，以至于增加运行时内存负荷。遵守以“是否具有让程序长时间执行的特征”来选定。长时间执行的特征就是指令序列的复用，如方法调用、循环跳转、异常跳转等，这种指令才会产生安全点。 如何在发生垃圾收集时让所有线程（除JNI调用的线程）都跑到最近的安全点并停顿下来。这里有两个选择： 抢先式中断 preemptive suspension（几乎弃用）：执行代码不用关注中断问题，系统会在GC时，把线程中断，如果有不在安全点的线程，就恢复其线程执行，然后过一会再重新中断，知道全都在安全点上。 主动式中断 voluntary suspension：中断线程时，不对线程操作，只设置一个标志位，各线程执行中会不停地主动轮询该标志，如果是中断标记为真，线程就在最近的安全点主动中断挂起。轮询标志点和安全点是重合的。 安全区域 Safe Region安全点机制保证了程序执行时，如何停顿用户线程，让jvm进去垃圾回收状态的问题。 程序不执行（线程无法响应jvm中断的请求）时，怎么办？ 安全区域可以解决，它是能够确保在某一段代码中，引用关系不会变化，因此，该区域中任意地方开始垃圾回收都是安全的。通俗理解，安全区域就是被扩展拉伸了的安全点。 实现：线程执行到安全区，首先会表示进入到安全区，jvm在垃圾收集时会忽略已经声明在安全区的线程。这些线程要离开安全区域时，就要检查jvm是否完成了 根节点枚举，如果完成就继续执行，否则就一直等待直到收到信号。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:3","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#安全区域-safe-region"},{"categories":["java"],"content":"\r3.4.4 记忆集与卡表 Remembered Set \u0026 Card TableRemembered Set是为了解决对象跨代引用所带来的问题而在新生代中建立的数据结构（跨域不局限于新生代和老年代之间）。 作用：避免把整个老年代加进 GC Roots 扫描范围（缩减GC Roots扫描范围） 原理：一种记录从非收集区指向收集区的指针集合的抽象数据结构。（不考虑效率和成本的实现方式是，一个用非收集区中所有含跨代引用的对象数组） 实现方案： 记录全部含跨代饮引用对象的空间引用和维护成本高昂。 在垃圾收集场景中国，只需要通过记忆集判断某一块非收集区域是否存在有指向了收集区域的指针即可。 实现记忆集因此选择更为粗犷的记录粒度来节省记忆集的存储和维护成本，列举一些记录精度： 字长精度：每个记录精确到一个机器字长（处理器的寻址位数，如32位或64位，此精度决定了机器访问物理内存地址的指针长度），该字包含跨代指针。 对象精度：每个记录精确到对象，该对象里有字段含有跨代指针。 卡精度：每个记录精确到一个块内存区域，该区域内有对象含有跨代指针。 其中，卡精度是最常用的记忆集实现形式，它由卡表（Card Table）的方式来实现。（注：卡表 不等于 记忆集，记忆集只是一种抽象的数据结构，抽象指的是只定义了记忆集的行为意图，并没有定义其行为的具体实现）卡表就是记忆集的以中国具体实现，它定义了记忆集的记录精度、与堆内存的映射关系等。 卡表和记忆集的关系就好比HashMap和Map的关系。 卡表最简单的形式：一个字节数组；选用 byte[]而不是bit[]，主要是因为速度，计算机硬件都是按照最小按字节寻址，没有直接存储bit指令。 解析字节数组card table：其中的每个元素都对应其标识的内存区域中一块特定大小的内存块，这个内存卡块被称为卡页（card page）。通常，卡页大小都是2的N次幂的字节数，比如 2^9，就是512字节。 一个卡页的内存通常包含不止一个对象，只要卡页内有对象的字段存在着跨代指针，那就将对应的卡表的数组元素标识为1，称这个元素变脏（dirty），没有则标识为0。在GC时，只要筛选出标识为1的元素，就能得到哪些卡页内存块中包含跨代指针，把他们加入到GC Roots中一并扫描。 写屏障 Write Barrier卡表存在的问题： 问题1，何时变脏：前置条件，其他分代区域中对象引用了本区域对象时，卡表元素变脏；变脏时间点，原则上发生在引用类型字段赋值的那一刻。 问题2，如何变脏（即如何在对象赋值的那一刻更新维护卡表）： 场景1（解释执行的字节码）：相对好处理，jvm负责每天字节码指令的执行，有充分的介入空间。 场景2（编译执行的场景）：即时编译后的代码是纯粹的机器指令流，必须通过机器码层面的手段，把维护卡表的动作放到每一个赋值操作之中。 写屏障的作用：解决卡表元素如何维护的问题，例如，何时变脏、谁来把它们变脏等。（维护卡表状态） 对写屏障的理解：在JVM层面对\"引用类型字段赋值\"这个动作的AOP切面，在引用对象赋值时会产生一个环形（Around）通知，供程序执行额外的动作，即写前屏障（pre-write barrier）、写后屏障（post-write barrier） 写屏障的效果：jvm会为所有的赋值操作生成相应的指令，一旦收集器在写屏障中增加了更新卡表的操作，无论更新的是否为老年代对新生代对象的引用，每次只要对引用进行更新，就会额外产生额外的开销，但是这个开销还是远远低于扫描整个老年代的。 弊端： 额外开销 伪共享（False Sharing）：卡表在高并发场景下会面临的问题，是处理并发底层细节时需要考虑的。处理器的缓存系统多数是以缓存行（Cache Line）为单位存储的，当多线程修改互相独立的变量时，这些变量恰好共享同一个缓存行，就会彼此影响（写回、无效化、同步）而导致性能降低。 假设，处理器缓存行大小64字节，一个卡表元素占1字节，64个卡表元素共享同一个缓存行。它们对应的卡页总的内存就是32kb（64-512字节）。如果不同线程更新的对象正好出于这32kb的内存区域内，就会导致更新卡表时正好写入同一个缓存行而影响性能。 解决伪共享：不采用无条件的写屏障；先检查卡表标记，只有当该卡表元素未被标记过时才将其标记为变脏。即先判断 if(card_table != 0){card_table=0} ；参数 -XX: +UseCondCardMark，来决定是否开启卡表更新的条件判断，开启能够避免伪共享问题，但是会增加一次额外判断的开销，二者都有性能损失。 并发的可达性分析 可达性分析算法理论的前提，全过程都基于一个保障一致性的快照中，才可以进行。也就是说必须全程冻结用户线程的运行。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:4","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#344-记忆集与卡表-remembered-set--card-table"},{"categories":["java"],"content":"\r3.4.4 记忆集与卡表 Remembered Set \u0026 Card TableRemembered Set是为了解决对象跨代引用所带来的问题而在新生代中建立的数据结构（跨域不局限于新生代和老年代之间）。 作用：避免把整个老年代加进 GC Roots 扫描范围（缩减GC Roots扫描范围） 原理：一种记录从非收集区指向收集区的指针集合的抽象数据结构。（不考虑效率和成本的实现方式是，一个用非收集区中所有含跨代引用的对象数组） 实现方案： 记录全部含跨代饮引用对象的空间引用和维护成本高昂。 在垃圾收集场景中国，只需要通过记忆集判断某一块非收集区域是否存在有指向了收集区域的指针即可。 实现记忆集因此选择更为粗犷的记录粒度来节省记忆集的存储和维护成本，列举一些记录精度： 字长精度：每个记录精确到一个机器字长（处理器的寻址位数，如32位或64位，此精度决定了机器访问物理内存地址的指针长度），该字包含跨代指针。 对象精度：每个记录精确到对象，该对象里有字段含有跨代指针。 卡精度：每个记录精确到一个块内存区域，该区域内有对象含有跨代指针。 其中，卡精度是最常用的记忆集实现形式，它由卡表（Card Table）的方式来实现。（注：卡表 不等于 记忆集，记忆集只是一种抽象的数据结构，抽象指的是只定义了记忆集的行为意图，并没有定义其行为的具体实现）卡表就是记忆集的以中国具体实现，它定义了记忆集的记录精度、与堆内存的映射关系等。 卡表和记忆集的关系就好比HashMap和Map的关系。 卡表最简单的形式：一个字节数组；选用 byte[]而不是bit[]，主要是因为速度，计算机硬件都是按照最小按字节寻址，没有直接存储bit指令。 解析字节数组card table：其中的每个元素都对应其标识的内存区域中一块特定大小的内存块，这个内存卡块被称为卡页（card page）。通常，卡页大小都是2的N次幂的字节数，比如 2^9，就是512字节。 一个卡页的内存通常包含不止一个对象，只要卡页内有对象的字段存在着跨代指针，那就将对应的卡表的数组元素标识为1，称这个元素变脏（dirty），没有则标识为0。在GC时，只要筛选出标识为1的元素，就能得到哪些卡页内存块中包含跨代指针，把他们加入到GC Roots中一并扫描。 写屏障 Write Barrier卡表存在的问题： 问题1，何时变脏：前置条件，其他分代区域中对象引用了本区域对象时，卡表元素变脏；变脏时间点，原则上发生在引用类型字段赋值的那一刻。 问题2，如何变脏（即如何在对象赋值的那一刻更新维护卡表）： 场景1（解释执行的字节码）：相对好处理，jvm负责每天字节码指令的执行，有充分的介入空间。 场景2（编译执行的场景）：即时编译后的代码是纯粹的机器指令流，必须通过机器码层面的手段，把维护卡表的动作放到每一个赋值操作之中。 写屏障的作用：解决卡表元素如何维护的问题，例如，何时变脏、谁来把它们变脏等。（维护卡表状态） 对写屏障的理解：在JVM层面对\"引用类型字段赋值\"这个动作的AOP切面，在引用对象赋值时会产生一个环形（Around）通知，供程序执行额外的动作，即写前屏障（pre-write barrier）、写后屏障（post-write barrier） 写屏障的效果：jvm会为所有的赋值操作生成相应的指令，一旦收集器在写屏障中增加了更新卡表的操作，无论更新的是否为老年代对新生代对象的引用，每次只要对引用进行更新，就会额外产生额外的开销，但是这个开销还是远远低于扫描整个老年代的。 弊端： 额外开销 伪共享（False Sharing）：卡表在高并发场景下会面临的问题，是处理并发底层细节时需要考虑的。处理器的缓存系统多数是以缓存行（Cache Line）为单位存储的，当多线程修改互相独立的变量时，这些变量恰好共享同一个缓存行，就会彼此影响（写回、无效化、同步）而导致性能降低。 假设，处理器缓存行大小64字节，一个卡表元素占1字节，64个卡表元素共享同一个缓存行。它们对应的卡页总的内存就是32kb（64-512字节）。如果不同线程更新的对象正好出于这32kb的内存区域内，就会导致更新卡表时正好写入同一个缓存行而影响性能。 解决伪共享：不采用无条件的写屏障；先检查卡表标记，只有当该卡表元素未被标记过时才将其标记为变脏。即先判断 if(card_table != 0){card_table=0} ；参数 -XX: +UseCondCardMark，来决定是否开启卡表更新的条件判断，开启能够避免伪共享问题，但是会增加一次额外判断的开销，二者都有性能损失。 并发的可达性分析 可达性分析算法理论的前提，全过程都基于一个保障一致性的快照中，才可以进行。也就是说必须全程冻结用户线程的运行。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:4","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#写屏障-write-barrier"},{"categories":["java"],"content":"\r3.4.4 记忆集与卡表 Remembered Set \u0026 Card TableRemembered Set是为了解决对象跨代引用所带来的问题而在新生代中建立的数据结构（跨域不局限于新生代和老年代之间）。 作用：避免把整个老年代加进 GC Roots 扫描范围（缩减GC Roots扫描范围） 原理：一种记录从非收集区指向收集区的指针集合的抽象数据结构。（不考虑效率和成本的实现方式是，一个用非收集区中所有含跨代引用的对象数组） 实现方案： 记录全部含跨代饮引用对象的空间引用和维护成本高昂。 在垃圾收集场景中国，只需要通过记忆集判断某一块非收集区域是否存在有指向了收集区域的指针即可。 实现记忆集因此选择更为粗犷的记录粒度来节省记忆集的存储和维护成本，列举一些记录精度： 字长精度：每个记录精确到一个机器字长（处理器的寻址位数，如32位或64位，此精度决定了机器访问物理内存地址的指针长度），该字包含跨代指针。 对象精度：每个记录精确到对象，该对象里有字段含有跨代指针。 卡精度：每个记录精确到一个块内存区域，该区域内有对象含有跨代指针。 其中，卡精度是最常用的记忆集实现形式，它由卡表（Card Table）的方式来实现。（注：卡表 不等于 记忆集，记忆集只是一种抽象的数据结构，抽象指的是只定义了记忆集的行为意图，并没有定义其行为的具体实现）卡表就是记忆集的以中国具体实现，它定义了记忆集的记录精度、与堆内存的映射关系等。 卡表和记忆集的关系就好比HashMap和Map的关系。 卡表最简单的形式：一个字节数组；选用 byte[]而不是bit[]，主要是因为速度，计算机硬件都是按照最小按字节寻址，没有直接存储bit指令。 解析字节数组card table：其中的每个元素都对应其标识的内存区域中一块特定大小的内存块，这个内存卡块被称为卡页（card page）。通常，卡页大小都是2的N次幂的字节数，比如 2^9，就是512字节。 一个卡页的内存通常包含不止一个对象，只要卡页内有对象的字段存在着跨代指针，那就将对应的卡表的数组元素标识为1，称这个元素变脏（dirty），没有则标识为0。在GC时，只要筛选出标识为1的元素，就能得到哪些卡页内存块中包含跨代指针，把他们加入到GC Roots中一并扫描。 写屏障 Write Barrier卡表存在的问题： 问题1，何时变脏：前置条件，其他分代区域中对象引用了本区域对象时，卡表元素变脏；变脏时间点，原则上发生在引用类型字段赋值的那一刻。 问题2，如何变脏（即如何在对象赋值的那一刻更新维护卡表）： 场景1（解释执行的字节码）：相对好处理，jvm负责每天字节码指令的执行，有充分的介入空间。 场景2（编译执行的场景）：即时编译后的代码是纯粹的机器指令流，必须通过机器码层面的手段，把维护卡表的动作放到每一个赋值操作之中。 写屏障的作用：解决卡表元素如何维护的问题，例如，何时变脏、谁来把它们变脏等。（维护卡表状态） 对写屏障的理解：在JVM层面对\"引用类型字段赋值\"这个动作的AOP切面，在引用对象赋值时会产生一个环形（Around）通知，供程序执行额外的动作，即写前屏障（pre-write barrier）、写后屏障（post-write barrier） 写屏障的效果：jvm会为所有的赋值操作生成相应的指令，一旦收集器在写屏障中增加了更新卡表的操作，无论更新的是否为老年代对新生代对象的引用，每次只要对引用进行更新，就会额外产生额外的开销，但是这个开销还是远远低于扫描整个老年代的。 弊端： 额外开销 伪共享（False Sharing）：卡表在高并发场景下会面临的问题，是处理并发底层细节时需要考虑的。处理器的缓存系统多数是以缓存行（Cache Line）为单位存储的，当多线程修改互相独立的变量时，这些变量恰好共享同一个缓存行，就会彼此影响（写回、无效化、同步）而导致性能降低。 假设，处理器缓存行大小64字节，一个卡表元素占1字节，64个卡表元素共享同一个缓存行。它们对应的卡页总的内存就是32kb（64-512字节）。如果不同线程更新的对象正好出于这32kb的内存区域内，就会导致更新卡表时正好写入同一个缓存行而影响性能。 解决伪共享：不采用无条件的写屏障；先检查卡表标记，只有当该卡表元素未被标记过时才将其标记为变脏。即先判断 if(card_table != 0){card_table=0} ；参数 -XX: +UseCondCardMark，来决定是否开启卡表更新的条件判断，开启能够避免伪共享问题，但是会增加一次额外判断的开销，二者都有性能损失。 并发的可达性分析 可达性分析算法理论的前提，全过程都基于一个保障一致性的快照中，才可以进行。也就是说必须全程冻结用户线程的运行。 ","date":"2022-12-21","objectID":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/:4:4","series":["jvm-notes"],"tags":["java","jvm"],"title":"深入理解JVM","uri":"/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3jvm/#并发的可达性分析"},{"categories":null,"content":"\r关于本站博主一直写文章的习惯，但它们都躺在了我的磁盘中，在和一些大佬前辈们交流过后甚至博客网站重要性。因此这个博客也就此诞生。。。 本站域名为 hongspell，hong是本人的名，而spell有两层含义。spell第一次层含义，拼写，意味着代码和程序都是有字符拼接起来的，这种行为更像是一种创造。spell的另一层含义，咒语，代码更像是魔法世界的咒语，赋予了人们无限的可能性。 这个域名满足了我中二又童趣的幻想。博主敲击学习的不是代码，而是沟通魔法世界的符文。 ","date":"2022-10-11","objectID":"/about/:1:0","series":null,"tags":null,"title":"About","uri":"/about/#关于本站"},{"categories":null,"content":"\r关于博主博主是个死宅，曾在University of Nottingham 就读 MSc Computer Science。 大环境下，让博主不得不钻研技术的深度和广度，这何尝不是一种驱动力，因此博客也是对个人成长的记录和见证。另外，闲暇之余，博主也爱好摄影、徒步、 旅行、音乐和美食。 如果您愿意说一声 索尼大法好 或者 湖人总冠军，那我们就是一辈子刚交的朋友。 ","date":"2022-10-11","objectID":"/about/:2:0","series":null,"tags":null,"title":"About","uri":"/about/#关于博主"},{"categories":null,"content":"\r特别说明本站所有内容仅代表个人观点，和任何组织或个他人无关。 本站内容仅供学习交流，请勿用于任何形式商业行为。 本站如无意中侵犯了某些组织或个人的知识产权，请速告之，本站定及时处理。 ","date":"2022-10-11","objectID":"/about/:3:0","series":null,"tags":null,"title":"About","uri":"/about/#特别说明"},{"categories":null,"content":"\rFeedback Prize | Kaggle\rkaggle竞赛自然语言处理方向的一个解决思路\rRead more...\r","date":"2021-11-01","objectID":"/projects/:0:0","series":null,"tags":null,"title":"Projects","uri":"/projects/#"},{"categories":null,"content":"There is nothing. ","date":"2021-11-01","objectID":"/sketch/:0:0","series":null,"tags":null,"title":"Sketches","uri":"/sketch/#"},{"categories":["machine learning"],"content":"\r1 模型评估与选择 过拟合：就是不具备普遍性，在训练集中表现好，测试集和实际使用中一般 欠拟合：模型对于训练集的表现都不好 评估方法：将数据集分为训练集和测试集的方法 留出法：简单因此常用 交叉验证：应用于同算法异参数或异算法之间的比较 自助法：应用于集成学习 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:1:0","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#1-模型评估与选择"},{"categories":["machine learning"],"content":"\r2 线性模型 基本形式：$f(x) = w^T x + b$，其中，w是1*d维的向量，w和b确定后，模型就确定了。 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:2:0","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#2-线性模型"},{"categories":["machine learning"],"content":"\r2.1 线性回归 目的：使f(x)接近y值 如何达到目的，在x和y是已知的情况下，确定w和b的值即可 如何确定w和b？ 最小二乘法： $(w,b) = arg \\ \\ min_(w,b) \\ \\ \\sum(f(x)-y)^2$ 即 $(w,b) = arg \\ \\ min_(w,b) \\ \\ \\sum(y - wx - b)^2$ 最小二乘法几何意义：试图找到一个直线，使得样本到直线上的欧氏距离之和最小 求最小值问题：就是分别对w、b求解其导数=0的过程 在多元线性回归中，同理，只不过x从向量变成了矩阵 令 $w^* = (w;b) = (w_1; w_2; …; w_d; b)$，从一元线性回归方程中可得： $w^=arg \\ \\ min_{w^} \\ \\ (y-Xw^)^T (y-Xw^)$，其中 X表示m*d维的x 为什么一元线性回归的平方形式在多元中可以写成其乘以其转置的形式？ 这样成立的前提是X为满秩矩阵或者正定矩阵（full-rank or positive definite matrix） 对$w^*$求导并让其导数式=0即可求得最优解，因此 $w^* = (X^TX)^{-1} X^T y$，其中，$(X^TX)^{-1}$为$(X^TX)$的逆矩阵 逆矩阵： 只有方阵才有逆矩阵 矩阵和其逆矩阵相乘等于单位矩阵（从左上到右下的对角线上的元素为1，其余元素为0），效果等于实数中一个数乘以其倒数就等于1 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:2:1","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#21-线性回归"},{"categories":["machine learning"],"content":"\r2.2 正则化 Regularization理想情况，$X^TX$是满秩的，但是实际情况中多数不是满秩，此时可解出多个w，且都能使均方误差最小化，如果选择一个w作为输出？—正则化项 什么是正则化？ 让w的个数最小化就是正则化 形式：$\\frac{\\lambda}{2} ||W||^2_2, \\lambda \u003e= 0$，其中$||W||^2_2$ 表示二范数 整体结构：$min{\\sum(y-W^Tx) + \\frac{\\lambda}{2}||W||^2_2}, \\lambda\u003e=0$ ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:2:2","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#22-正则化-regularization"},{"categories":["machine learning"],"content":"\r2.3 对数线性回归 log-linear regression 对数线性回归解决的了，线性模型输出值之间可能存在跨度太大的问题，即将输出标记控制在指数尺度上。 通过 $ln y = w^Tx +b$，试图让 $e^{w^Tx+b}$ 逼近y。 单位阶跃函数\r对数线性回归适合与回归问题 对数几率回归则适用于分类问题 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:2:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#23-对数线性回归-log-linear-regression"},{"categories":["machine learning"],"content":"\r2.4 对数几率回归 二分类问题中，$y \\in {0,1}$，线性回归的预测值是实值，不好用与分类中，而 “单位阶跃函数” unit-step，可以将值转换为0/1. 但是单位阶跃函数不连续，所以不能直接作用在线性回归上，而对数几率函数可以替代单位阶跃函数，且它是连续的。 单位阶跃函数与对数几率函数对比： 线性回归\r对数几率函数：$y=\\frac{1}{1+e^{-(w^Tx+b)}}$ 它是一种 Sigmoid函数，将 $(w^Tx+b)$，设为 z，该函数将z转化为接近0或1的值 其中几率为，得到正例的可能性 和 1 - 得到正例的可能性（即反例）的比值，对这个几率取对数就是叫对数几率（log odds, logit) $ln\\frac{y}{1-y}=w^Tx+b$ 对数几率回归优点： 不仅可以预测类别，还可以得到近似的概率预测 对数几率函数是任意阶可导的凸函数（凸函数是可以直接用于求最优解的函数） 求解对数几率函数中的w和b：极大似然估计 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:2:4","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#24-对数几率回归"},{"categories":["machine learning"],"content":"\r2.5 数学基础：极大似然估计 MLE 什么时候用极大似然估计：模型已定，参数未知 采样要满足的假设：所有采样都要独立同分布 正态分布下的极大似然估计的公式： $f(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma} exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})$ $P(x|\\theta)$ 输入有两个，一个x，一个是模型的参数 $\\theta$ 概率函数：$\\theta$ 已知，x是变量；描述对于不同样本点x，其出现概率是多少 似然函数：x已知，$\\theta$是变量；描述对于不同的模型参数，出现x这个样本点的概率是多少。 极大似然估计的数学例子 如何求极大似然估计？ 令其导数=0，理解起立就是，函数有切线，函数中不同的位置对应着不同的切线，如何找到极值点？就是当它切线与x轴平行的时候，即切线斜率=0的时候，而这个切线斜率就是函数对应的导数。 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:2:5","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#25-数学基础极大似然估计-mle"},{"categories":["machine learning"],"content":"\r2.6 数学基础：贝叶斯公式 Bayes 区分概率和统计：前者已知模型和参数，推数据；后者已知数据，推模型和参数。 贝叶斯公式: $P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$，核心就是条件概率和联合概率 把B展开： $P(A|B)=\\frac{P(B|A)P(A)}{P(B|A)|(A)+P(B|\\sim A)P(\\sim A)}$，其中 ~A表示非A 贝叶斯公式就是在描述，How much you can trust evidence 理解公式和思想的例子：一辆车的警报响了； 事件A表示车被砸 事件B表示警报响 P(A|B)表示警报响了并且车被砸了概率 P(B|A)表示车被砸引发了警报的概率 P(B|~A)表示警报响了，但是车并没有被砸 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:2:6","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#26-数学基础贝叶斯公式-bayes"},{"categories":["machine learning"],"content":"\r2.7 线性判别分析 (Linear Discriminant Analysis LDA) 线性判别分析主要用于分类问题，也叫 Fisher判别分析，是一种监督降维方法 LDA思想：设法将样本投影到一条直线上，使样本在这条线上最容易分类；要求，同类近，异类远 线性判别模型：采用直线或超平面将样本直接切开，表示为 y = f(w^T x + b)，划分平面表示为 w^T x + b = 0。常见模型，逻辑回归（sigmod函数）、感知机（激活函数） 其中 w就是我们要找的投影线的向量，我们只关心向量方向，而不关心模长 为了让异类样本相隔远，就需要让异类均值的差大，让同类间离散小，就需要同类的协方差小 最大化目标：$J=\\frac{||w^T \\mu_0 - w^T \\mu_1||^2_2}{w^T \\Sigma_0 w + w^T \\Sigma_1 w}$ 展开得 $\\frac{w^T(\\mu_0 - \\mu_1)(\\mu_0 - \\mu_1)^Tw}{w^T(\\Sigma_0+\\Sigma_1)w}$ 类内方差小 min，类间均值大 max within-class scatter matrix: $S_w = \\Sigma_0 + \\Sigma_1$ between-class scatter matrix: $S_b = (\\mu_0-\\mu_1)(\\mu0-\\mu_1)^T$ 改写后，$J = \\frac{w^T S_b w}{w^T S_w w}$，这就是LDA的最大化目标，即 Sb和Sw的广义瑞利商（generalized rayleigh quotient），要求的就是 max J 此时，max J 不可解，因为w会被约掉；但由于w的大小并不会影响最终结果，因为只需要确定w方向。因此，解决方案就是 固定w的大小，固定的方式有很多种，书中是将分母大小固定，$w^T S_w w = 1$： 已知 Sw的大小是固定的，为什么？因为给定样本后，Sw是个固定的矩阵，是个常量；这么做就等价于固定w的模长（不管固定分子还是分母都是可以的，因为样本给定后，均值和方差都是固定的） 所以 max J = $w^T S_b w$，s.t. $w^T S_w w=1$ 而通常优化问题都是转化成最小化问题，那么此时 max J -\u003e min J: $min_w J = - w^T S_b w$，s.t. $w^T S_w w=1$ 求解带约束的优化问题的常用方法：拉格朗日乘子法 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:2:7","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#27-线性判别分析-linear-discriminant-analysis-lda"},{"categories":["machine learning"],"content":"\r2.8 数学基础：拉格朗日乘子法 对于仅含约束的优化问题： $min_x f(x)$，s.t. $h_i(x)=0$，i = 1,2,…,n 其中自变量x属于实数，f(x)和hi(x)均有连续的一阶偏导数。首先推出其拉格朗日函数： $L(x, \\lambda) = f(x) + \\sum_{i=1}^n \\lambda_i h_i(x)$ 其中 $\\lambda = (\\lambda_1, \\lambda_2,…,\\lambda_n)^T$ 为拉格朗日乘子。然后对拉格朗日函数关于x求偏导。并令导数=0，再搭配约束条件 $h_i(x)=0$解出x，求解出所有x即为上述优化问题的所有可能（极值点）（只能求出1个或者多个局部极值点，不能确定哪个是想要的） 求解 w $min_w J = - w^T S_b w$，s.t. $w^T S_w w-1=0$，其中 $w^T S_w w-1=0$ 就是 h(x) 由拉格朗日乘子法可得拉格朗日函数：$L(w, \\lambda) = -w^TS_b w + \\lambda(w^TS_w w - 1)$，也就是 f(x)+h(x) 对 w 求偏导： $\\frac{\\partial L(w, \\lambda)}{\\partial w} = - \\frac{\\partial (w^T S_b w)}{\\partial w} + \\lambda \\frac{\\partial (w^T S_w w - 1)}{\\partial w}$ $= -(S_b + S_b^T)w + \\lambda(S_w + S^T_w)w$ 由于 Sb=Sb^T，Sw=Sw^T（因为Sb和Sw都是对称矩阵），所以： $L(w, \\lambda) = -2 S_b w + 2 \\lambda S_w w$ 令上式 = 0： $S_b w = \\lambda S_w w$ (广义特征值) 特征值：$Ax = \\lambda x$ 广义特征值：$Ax = \\lambda B x$ 将Sb和Sw展开：$(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^T w = \\lambda S_w w$ 其中，$\\mu_0$ 和 $\\mu_1$ 是列向量，它们相减还是列向量，$(\\mu_0-\\mu_1)^T$是行向量，w是列向量；行向量*列向量 = 实数，所以： 令 $(\\mu_0 - \\mu_1)^T w = \\gamma$，则： $\\gamma (\\mu_1 - \\mu_2) = \\lambda S_w w$ $w = \\frac{\\gamma}{\\lambda} S^{-1}_w (\\mu_0 - \\mu_1)$ 由于最终求解的w不关心其大小，只关心方向，所以令常数项=1，即$\\frac{\\gamma}{\\lambda}=1$，此时有： $w = S^{-1}_w (\\mu_0 - \\mu_1)$ ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:2:8","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#28-数学基础拉格朗日乘子法"},{"categories":["machine learning"],"content":"\r2.9 数学基础：广义特征值 定义：设A, B为 n阶方阵，若存在 $\\lambda$，使得方程 $Ax=\\lambda Bx$ 存在非零解，则称 $\\lambda$ 为A相对于 B的特征广义指，x为A相对于B的属于广义特征值 $\\lambda$ 的特征向量。 特别地，当 B = I（单位矩阵）时，广义特征值问题退化为标准特征值问题。 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:2:9","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#29-数学基础广义特征值"},{"categories":["machine learning"],"content":"\r2.10 数学基础：广义瑞利商 定义：设A, B为 n阶厄米（Hermitian）矩阵，且B正定，称 $R(x)=\\frac{x^H Ax}{x^H Bx}$ (x != 0) 为A相对于B的广义瑞利商。 特别地，当 B = I（单位矩阵）时，广义瑞利商退化为瑞利商。 其中，当矩阵是实数矩阵时，厄米矩阵就等于转置，即 $A^H=A^T$，此时 A和A^H对称；而当元素为复数时，会有不同。 性质：假设，$\\lambda_i, x_i (i=1,2,…,n)$ 为A相对于B的广义特征值和特征向量，且 $\\lambda_1 \u003c= \\lambda_2 \u003c= … \u003c= \\lambda_n$，则有： $min_{x != 0} \\ \\ R(X) = \\frac{x^H Ax}{x^H bx} = \\lambda_1, \\ \\ x^* = x_1$ $max_{x!=0} \\ \\ R(x) = \\frac{x^H Ax}{x^H Bx} = \\lambda_n, \\ \\ x^* = x_n$ ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:2:10","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#210-数学基础广义瑞利商"},{"categories":["machine learning"],"content":"\r2.11 多分类问题 学习：一般是利用二分类学习器解决多分类问题，通过拆解法，将多分类任务拆为若干个二分类任务求解。 测试：对这些分类器的预测结果集成得到多分类结果。 拆分策略： One vs One 将N个类别两两配对，产生N(N-1)/2个二分类任务（也就是训练N(N-1)/2个分类器） 测试阶段，新样本提交给所有的分类器，得到N(N-1)/2个分类结果，把预测最多的类别作为最终结果 存储开销和测试开销会大 One vs Rest 将一个类的样例作为正例，其余全部作为反例，来训练N个分类器 测试阶段，仅有一个分类器预测为正类，则对应的类别标记作为最终分类结果；若有多个分类器预测为正类，则根据置信度最大的分类器的类别标记作为分类结果 存储开销和测试时间开销通常小于上一个；如果类别很多的情况下，测试时间开销就大于上一个，因为她需要用到全部样例，而上一个朱需要用到两个类的样例 预测性能，二者差不多 Many vs Many 每次将若干个类作为正类，若干个其他类作为反类。正反类构造必须有特殊的设计，不能随意选取，常用技术：纠错输出码（Error Correction Output Codes, ECOC） ECOC是将编码的思想引入类别拆分，尽可能在解码过程中具有容错性 工作过程： 编码：对N个类别做M次划分，每次划分将两部分类别分别划分为正反类，从而形成一个二分类训练集，这样产生M个训练集，可以训练出M个分类器 解码：M个分类器分别对测试样本进行预测，这些预测标记组成一个编码。将编码与每个类别的各自编码进行比较，返回其中距离最小的类别作为最终预测结果 类别划分通过编码矩阵（coding matrix） 指定，常见形式 二元码、三元码 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:2:11","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#211-多分类问题"},{"categories":["machine learning"],"content":"\r2.12 类别不平衡问题 class-imbalance 前提：分类任务中，不同类别的训练样例数母差别很大，比如，正类样例少，反类样例特别多 类别不平衡处理的基本方法： 基本策略：再缩放（rescaling） 几率y/(1-y)反映了正例可能性和反例可能性之比，若其 \u003e 1，则预测为正例； 然而，训练集中正反例数目不同时，令m^+表示正例数目，m^-表示反例数目，则，目测几率是 m^+/m^- 由于，通常假设训练集是真实样本总体的无偏采样，因此观测几率就代表了真实几率。于是，只要分类器的预测几率高于观测几率就判定为正例： y/(1-y) \u003e m^+/m^-，则 预测为正例 而 分类器基于 y/(1-y)进行决策，因此，需要对预测值进行调整，即再缩放： $\\frac{y’}{1-y’} = \\frac{y}{1-y} \\frac{m^-}{m^+}$ 弊端：假设往往不成立，即，训练集是真实样本总体的无偏采样 主流做法1：欠采样（undersampling） 直接对训练集里的反类样例进行欠采样，即去除一些反例，使得正反数目接近 优势：开销小 弊端：不能随意丢弃反例，会丢失重要信息，需要通过特定算法来处理 主流做法2：过采样（oversampling） 对训练集里的正类样例进行过采样，即增加一些正例使得正反数目接近 弊端：不能随便对样例进行重复采样，否则会过拟合 主流做法3：阈值移动（threshold-moving） 直接基于原始训练集进行学习，但是在训练好的分类器进行预测时，将$\\frac{y’}{1-y’} = \\frac{y}{1-y} \\frac{m^-}{m^+}$嵌入到其决策过程中 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:2:12","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#212-类别不平衡问题-class-imbalance"},{"categories":["machine learning"],"content":"\r3 决策树 Decision Tree","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:3:0","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#3-决策树-decision-tree"},{"categories":["machine learning"],"content":"\r3.1 基本流程一般的，决策树包含根节点、若干内部节点、若干叶节点 叶节点：一个决策结果 除叶节点：特征测试（属性测试） 父节点传递给子节点什么？ 将其包含的样本集合，根据属性测试的结果，划分到子节点中（根节点包含的是样本全集） 从 根节点 到 每个叶子节点 的路径，对应了一个判定测试序列 决策树学习的目的： 生成一颗泛化能力强的树（其基本流程遵循分而治之策略 divide-and conquer） 决策树的生成：递归过程 三种情况会导致递归： 当前节点包含的样本 全属于同一类别，即，无需划分 即，将当前节点标记为某一类别叶节点 当前特征（属性）为null，或，所有样本在 所有属性上 取值相同，即，无法划分 即，将其标记为叶节点，其类别是训练集中样本数量最多的类 当前节点包含的 样本集合为null，即，不能划分 将其父节点标记为叶节点，类别为训练集中最多样本最多的类 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:3:1","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#31-基本流程"},{"categories":["machine learning"],"content":"\r3.2 划分选择决策树的流程中最重要的是：如何从属性集A中 选择最优划分属性 a？ 随着划分的进行，让节点 纯度（purity）越来越高，即，分支节点所包含的样本属性尽可能地属于同一类别 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:3:2","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#32-划分选择"},{"categories":["machine learning"],"content":"\r3.3 信息增益 information entropy一种度量样本纯度的最长用的指标 规定样本集合D中，第k类样本所占比例为pk (k = 1, 2, …, |y|),那么信息熵为： $$Ent(D) = - \\sum_{k=1}^{|y|} p_k log_{2} p_k$$ Ent(D)越小，样本集合D纯度越高，其中，$0 \u003c= Ent(D) \u003c= log_2 |y|$，|y|表示样本类总数，$0 \u003c= p_k \u003c= 1$，$\\sum_{k=1}^n p_k = 1$。 若令 |y| = n，即集合D就是样本全集，那么，pk = xk，（x指的是样本，不是特征） 信息熵公式推导 假定存在一个离散属性a，它有个v个取值，使用a对D互粉，就会产生v个分支节点，每个分支节点包含了对应属性a的样本，记D^v。再求出D^v的信息熵后，考虑不同分支节点包含样本数不同，给分支节点赋予权重 $\\frac{|D^v|}{|D|}$，（这个权重就是a1在a中的正例占比，D^v即1-正例占比），样本数越多的分支节点，影响越大。由此计算出用属性a对于D进行划分所获得的信息增益（information gain） $$Gain(D, a) = Ent(D) - \\sum_{v=1}^V \\frac{|D^v|}{|D|} Ent(D^v)$$ 由属性a划分所获得的纯度提升，和信息增益成正比。 因此，可以在生成分支前，先进行决策树的划分属性选择（max) ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:3:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#33-信息增益-information-entropy"},{"categories":["machine learning"],"content":"\r3.4 增益率 Gain Ratio解决的问题：信息增益准则由于一些偏好（比如，对取值数目多的属性偏好）对预测带来不利影响 $$Grain_ratio(D, a) = \\frac{Gain(D, a)}{IV(a)}$$ 其中，属性a是固有值（intrinsic value)，属性a取值数目越多，V越大，IV通常越大。 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:3:4","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#34-增益率-gain-ratio"},{"categories":["machine learning"],"content":"\r3.5 基尼指数 CART Index (Classification and Regression Tree)CART决策树使用基尼指数来划分属性，数据集D的纯度通过基尼值来度量： $$Gini(D)=\\sum_{k=1}^{|y|} \\sum_{k’ !=k} p_k p_{k’}$$ $$= 1- \\sum_{k=1}^{|y|} p^2_k$$ 基尼值反映了D随机抽取两个样本，其类别标记不一致的概率。 基尼值越小，数据集纯度越高。 基尼指数定义： $$Gini_index(D, a) = \\sum_{v=1}^V \\frac{|D^v|}{|D|} Gini(D^v)$$ ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:3:5","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#35-基尼指数-cart-index-classification-and-regression-tree"},{"categories":["machine learning"],"content":"\r3.6 剪枝处理 pruning 用途：对付过拟合（避免分支过多） 剪枝策略： 预剪枝（prepruning) 场景：决策树生成中 过程：在节点划分前，进行估计；如果当前节点不能提升泛化性能，则终止划分并将当前节点标为叶节点 后剪枝（post-pruing） 场景：生成完整的决策树后 过程：自底向上地对非叶子节点测试，若该节点的子树替换成叶子节点能够提升泛化型，则将该子替换成叶子节点 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:3:6","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#36-剪枝处理-pruning"},{"categories":["machine learning"],"content":"\r3.7 连续与缺失值连续值处理： 由于连续值的数目不是有限的，所以不能按照离散属性生成决策树那样处理连续值 通过连续属性离散化来处理 比如最简单的策略：二分法（bi-partition)' $$T_a = (\\frac{a^i+a^{i+1}}{2} | 1\u003c= i \u003c= n-1)$$ ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:3:7","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#37-连续与缺失值"},{"categories":["machine learning"],"content":"\r4 神经网络神经元模型 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:4:0","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#4-神经网络"},{"categories":["machine learning"],"content":"\r4.1 激活函数 activation function把值范围压缩到 (0,1)之间，典型的Sigmoid函数 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:4:1","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#41-激活函数-activation-function"},{"categories":["machine learning"],"content":"\r4.2 感知机（Perceptron）与多层网络M-P神经元： 单个M-P神经元：感知机（sgn激活函数），对数几率回归（sigmoid激活函数） 多个M-P神经元：神经网络 激活函数：模拟抑制和激活，对值做一个压缩处理 感知机： 两层神经元，输入层 + 输出层，其中输出层是M-P神经元，称为threhold logic unit 把激活函数替换成阶跃函数（符号）的神经元 分类模型 感知机公式： $$y = f(\\sum_{i=1}^n w_i x_i - \\theta) = f(w^t x - \\theta)$$ 其中，theta是阈值，f是符号函数 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:5:0","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#42-感知机perceptron与多层网络"},{"categories":["machine learning"],"content":"\r4.3 感知机的学习策略对于权重w和阈值theta可以通过学习得到， Processing: step 1: 假设，有一个误分类样本集合M，找到所有的误分类样本（x,y）（分错类的），对于误分类样本来说 当$w^Tx- \\theta \u003e= 0$时，它的分类结果是 y’ = 1，然后真实值 y=0；反之，$w^Tx- \\theta \u003c 0$时，y’= 0, y = 1；结合两种情形，可得： $$(\\hat{y} - y) (w^t x - \\theta) \u003e= 0$$ 所以，对于给定数据集，其损失函数为： $$L(w,\\theta) = \\sum_{x \\in M} (\\hat{y} - y) (w^t x - \\theta)$$ 显然，L始终是非负的，且： 无误分类点，损失函数为0 误分类点越少，L就越小 误分类点离超平面越近，L也越小 因此，L是一个关于w、theta连续可导的函数 step 2: 将感知机模型的学习问题转化为求解损失函数的最优化问题 求解 w、theta: $$min_{w,\\theta} L(w,\\theta) = min_{w,\\theta} \\sum_{x_i \\in{M}} (\\hat{y_i}-{y_i})(w^T x_i - \\theta)$$ 将阈值 theta看作一个固定输入为-1的\"哑节点\"，所以有: $$-\\theta = -1*w_{n+1}=x_{n+1}*w_{n+1}$$ 解释，把损失函数中的 -theta整体看作是 -1 * theta，把theta看作 w_n+1，本身w只能取到n，人为的在扩充了一个；n被+1，x也受到n的影响，x也被扩充到n+1，而x_n+1恒等于-1，此时，函数变为: $$min_{w,\\theta} L(w,\\theta) = min_{w,\\theta} \\sum_{x_i \\in{M}} (\\hat{y_i}-{y_i})(w^T x_i)$$ 注，此式的w和x的维度是n+1维，原式子的维度是n维 求新损失函数L(w)的梯度: $$\\bigtriangledown_wL(w)=\\sum_{x_i \\in M} (\\hat{y_i} - y_i)x_i$$ 感知机使用的是随机梯度下降，普通的梯度下降是一次使M中所有的误分类点梯度下降，而随机梯度是一次取某一个误分类点使其梯度下降，然后更新权重w，而w的更新公式: $$w \\leftarrow w + \\triangle w$$ $$\\triangle w = - \\eta(\\hat{y_i} - y_i)x_i = \\eta( y_i - \\hat{y_i})x_i$$ 其中 eta是学习率 learning rate； 迭代n多次后求出w（梯度=0），最后求解出来的w通常不唯一，就是会有1\u003c个梯度为0的点（很多局部最优） ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:5:1","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#43-感知机的学习策略"},{"categories":["machine learning"],"content":"\r4.4 多层网络 定义：在输入输出中加入 hidden layer 多层前馈神经网络（multi-layer feedforward neural networks)：每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接 通用近似定理：只需要一个包含足够多神经元的隐层，多层前馈网络就可以以任意精度逼近任意复杂度的连续函数。因此，神经网络即可以做分类又可以做回归任务。 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:5:2","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#44-多层网络"},{"categories":["machine learning"],"content":"\r4.5 误差逆传播算法 error BackPropagation BP该算法用来训练多层神经网络，是一个迭代学习算法，每一轮迭代都采用广义的感知机学习规则对参数估计。 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:6:0","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#45-误差逆传播算法-error-backpropagation-bp"},{"categories":["machine learning"],"content":"\r5 数据处理","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:7:0","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#5-数据处理"},{"categories":["machine learning"],"content":"\r5.1 数据预处理-简介","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:7:1","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#51-数据预处理-简介"},{"categories":["machine learning"],"content":"\r5.2 数据无量纲化 概念：将不同规格的数据转换成同一个规格，或不同分布的数据转成特定分布。 优点： 加快求解速度：在核心为梯度或矩阵的算法中，比如逻辑回归、支持向量机、神经网络 提升模型精度：在距离类模型中，比如K近邻、K-Means聚类中；避免某一取值范围特别大的特征对距离计算造成影响 特例：决策树不需要无量纲化，决策树本身就可以把任意数据处理得很好 类型： 线性无量纲化：中心化处理（Zero-centered / Mean-subtraction）和 缩放处理（Scale） 中心化本质：让所有记录减去一个固定值，即让样本数据平移到某个位置 缩放本质：通过除以一个固定值，将数据固定在某个范围内（比如，取对数） 非线性无量纲化 数据归一化（Normalization 、 min-max scaling）：数据按照最小值中心化后，再按极差（最大-最小）缩放，数据移动了最小值个单位，并会收敛到 [0,1]之间 归一化后的数据服从正态分布 feature_range：MinMaxScaler的重要参数，控制数据压缩到的范围，默认[0,1] 公式：$x^*=\\frac{x-min(x)}{max(x)-min(x)}$，就是 中心化 / 极差 .fit()本质是生成最大值和最小值 数据标准化（Standardization、Z-score normalization）：数据按均值中心化后，再按标准化缩放，数据就会服从期望为0，方差为1的正态分布 公式：$x^*=\\frac{x-\\mu}{\\sigma}$ .fit()本质是生成均值和方差 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:7:2","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#52-数据无量纲化"},{"categories":["machine learning"],"content":"\r5.3 数据预处理 - 处理缺失值\r5.3.1 数据清洗从数据集中纠正或消除不准确、损坏、格式错误、重复或不完整的数据的做法称为数据清理。 5.3.2 填补缺失值的重要性如果没有正确处理缺失的数值，可能会对数据得出错误的结论，这将对建模阶段产生重大影响，会影响最后的结果。 5.3.3 缺失值导致的问题 在缺乏证据的情况下，统计能力，即检验在零假设错误时拒绝该零假设的几率会降低。 数据的丢失可能导致参数估计出现偏差。 具有降低样本代表性的能力。 5.3.4 缺失数据类型根据数据集或数据中不存在的模式或数据，可以将其分类。 完全随机缺失(MCAR)： 当丢失数据的概率与要获得的精确值或观察到的答案的集合无关时 solution: deleting rows or columns 随机缺失(MAR)： 当丢失响应的概率由观察到的响应的集合而不是预期达到的精确缺失值决定时。 solution: imputation of data 非随机缺失(MNAR)： MNAR 是缺失数据，这类数据案例很难处理，可以对缺失数据进行建模是获得参数的公平近似值的唯一方法 solution improve dataset find data 5.3.5 缺失值的类别 连续变量或特征 — — 数值数据集，即数字可以是任何类型 分类变量或特征 — — 它可以是数值的或客观的类型 5.3.6 缺失值插补类型插补有多种大小和形式。这是在为我们的应用程序建模以提高精度之前解决数据集中缺失数据问题的方法之一。 单变量插补或均值插补是指仅使用目标变量对值进行插补。 多元插补： 根据其他因素插补值，例如使用线性回归根据其他变量估计缺失值。 单一插补： 要构建单个插补数据集，只需在数据集中插补一次缺失值。 大量插补： 在数据集中多次插补相同的缺失值。这本质上需要重复单个插补以获得大量插补数据集。 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:7:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#53-数据预处理---处理缺失值"},{"categories":["machine learning"],"content":"\r5.3 数据预处理 - 处理缺失值\r5.3.1 数据清洗从数据集中纠正或消除不准确、损坏、格式错误、重复或不完整的数据的做法称为数据清理。 5.3.2 填补缺失值的重要性如果没有正确处理缺失的数值，可能会对数据得出错误的结论，这将对建模阶段产生重大影响，会影响最后的结果。 5.3.3 缺失值导致的问题 在缺乏证据的情况下，统计能力，即检验在零假设错误时拒绝该零假设的几率会降低。 数据的丢失可能导致参数估计出现偏差。 具有降低样本代表性的能力。 5.3.4 缺失数据类型根据数据集或数据中不存在的模式或数据，可以将其分类。 完全随机缺失(MCAR)： 当丢失数据的概率与要获得的精确值或观察到的答案的集合无关时 solution: deleting rows or columns 随机缺失(MAR)： 当丢失响应的概率由观察到的响应的集合而不是预期达到的精确缺失值决定时。 solution: imputation of data 非随机缺失(MNAR)： MNAR 是缺失数据，这类数据案例很难处理，可以对缺失数据进行建模是获得参数的公平近似值的唯一方法 solution improve dataset find data 5.3.5 缺失值的类别 连续变量或特征 — — 数值数据集，即数字可以是任何类型 分类变量或特征 — — 它可以是数值的或客观的类型 5.3.6 缺失值插补类型插补有多种大小和形式。这是在为我们的应用程序建模以提高精度之前解决数据集中缺失数据问题的方法之一。 单变量插补或均值插补是指仅使用目标变量对值进行插补。 多元插补： 根据其他因素插补值，例如使用线性回归根据其他变量估计缺失值。 单一插补： 要构建单个插补数据集，只需在数据集中插补一次缺失值。 大量插补： 在数据集中多次插补相同的缺失值。这本质上需要重复单个插补以获得大量插补数据集。 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:7:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#531-数据清洗"},{"categories":["machine learning"],"content":"\r5.3 数据预处理 - 处理缺失值\r5.3.1 数据清洗从数据集中纠正或消除不准确、损坏、格式错误、重复或不完整的数据的做法称为数据清理。 5.3.2 填补缺失值的重要性如果没有正确处理缺失的数值，可能会对数据得出错误的结论，这将对建模阶段产生重大影响，会影响最后的结果。 5.3.3 缺失值导致的问题 在缺乏证据的情况下，统计能力，即检验在零假设错误时拒绝该零假设的几率会降低。 数据的丢失可能导致参数估计出现偏差。 具有降低样本代表性的能力。 5.3.4 缺失数据类型根据数据集或数据中不存在的模式或数据，可以将其分类。 完全随机缺失(MCAR)： 当丢失数据的概率与要获得的精确值或观察到的答案的集合无关时 solution: deleting rows or columns 随机缺失(MAR)： 当丢失响应的概率由观察到的响应的集合而不是预期达到的精确缺失值决定时。 solution: imputation of data 非随机缺失(MNAR)： MNAR 是缺失数据，这类数据案例很难处理，可以对缺失数据进行建模是获得参数的公平近似值的唯一方法 solution improve dataset find data 5.3.5 缺失值的类别 连续变量或特征 — — 数值数据集，即数字可以是任何类型 分类变量或特征 — — 它可以是数值的或客观的类型 5.3.6 缺失值插补类型插补有多种大小和形式。这是在为我们的应用程序建模以提高精度之前解决数据集中缺失数据问题的方法之一。 单变量插补或均值插补是指仅使用目标变量对值进行插补。 多元插补： 根据其他因素插补值，例如使用线性回归根据其他变量估计缺失值。 单一插补： 要构建单个插补数据集，只需在数据集中插补一次缺失值。 大量插补： 在数据集中多次插补相同的缺失值。这本质上需要重复单个插补以获得大量插补数据集。 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:7:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#532-填补缺失值的重要性"},{"categories":["machine learning"],"content":"\r5.3 数据预处理 - 处理缺失值\r5.3.1 数据清洗从数据集中纠正或消除不准确、损坏、格式错误、重复或不完整的数据的做法称为数据清理。 5.3.2 填补缺失值的重要性如果没有正确处理缺失的数值，可能会对数据得出错误的结论，这将对建模阶段产生重大影响，会影响最后的结果。 5.3.3 缺失值导致的问题 在缺乏证据的情况下，统计能力，即检验在零假设错误时拒绝该零假设的几率会降低。 数据的丢失可能导致参数估计出现偏差。 具有降低样本代表性的能力。 5.3.4 缺失数据类型根据数据集或数据中不存在的模式或数据，可以将其分类。 完全随机缺失(MCAR)： 当丢失数据的概率与要获得的精确值或观察到的答案的集合无关时 solution: deleting rows or columns 随机缺失(MAR)： 当丢失响应的概率由观察到的响应的集合而不是预期达到的精确缺失值决定时。 solution: imputation of data 非随机缺失(MNAR)： MNAR 是缺失数据，这类数据案例很难处理，可以对缺失数据进行建模是获得参数的公平近似值的唯一方法 solution improve dataset find data 5.3.5 缺失值的类别 连续变量或特征 — — 数值数据集，即数字可以是任何类型 分类变量或特征 — — 它可以是数值的或客观的类型 5.3.6 缺失值插补类型插补有多种大小和形式。这是在为我们的应用程序建模以提高精度之前解决数据集中缺失数据问题的方法之一。 单变量插补或均值插补是指仅使用目标变量对值进行插补。 多元插补： 根据其他因素插补值，例如使用线性回归根据其他变量估计缺失值。 单一插补： 要构建单个插补数据集，只需在数据集中插补一次缺失值。 大量插补： 在数据集中多次插补相同的缺失值。这本质上需要重复单个插补以获得大量插补数据集。 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:7:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#533-缺失值导致的问题"},{"categories":["machine learning"],"content":"\r5.3 数据预处理 - 处理缺失值\r5.3.1 数据清洗从数据集中纠正或消除不准确、损坏、格式错误、重复或不完整的数据的做法称为数据清理。 5.3.2 填补缺失值的重要性如果没有正确处理缺失的数值，可能会对数据得出错误的结论，这将对建模阶段产生重大影响，会影响最后的结果。 5.3.3 缺失值导致的问题 在缺乏证据的情况下，统计能力，即检验在零假设错误时拒绝该零假设的几率会降低。 数据的丢失可能导致参数估计出现偏差。 具有降低样本代表性的能力。 5.3.4 缺失数据类型根据数据集或数据中不存在的模式或数据，可以将其分类。 完全随机缺失(MCAR)： 当丢失数据的概率与要获得的精确值或观察到的答案的集合无关时 solution: deleting rows or columns 随机缺失(MAR)： 当丢失响应的概率由观察到的响应的集合而不是预期达到的精确缺失值决定时。 solution: imputation of data 非随机缺失(MNAR)： MNAR 是缺失数据，这类数据案例很难处理，可以对缺失数据进行建模是获得参数的公平近似值的唯一方法 solution improve dataset find data 5.3.5 缺失值的类别 连续变量或特征 — — 数值数据集，即数字可以是任何类型 分类变量或特征 — — 它可以是数值的或客观的类型 5.3.6 缺失值插补类型插补有多种大小和形式。这是在为我们的应用程序建模以提高精度之前解决数据集中缺失数据问题的方法之一。 单变量插补或均值插补是指仅使用目标变量对值进行插补。 多元插补： 根据其他因素插补值，例如使用线性回归根据其他变量估计缺失值。 单一插补： 要构建单个插补数据集，只需在数据集中插补一次缺失值。 大量插补： 在数据集中多次插补相同的缺失值。这本质上需要重复单个插补以获得大量插补数据集。 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:7:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#534-缺失数据类型"},{"categories":["machine learning"],"content":"\r5.3 数据预处理 - 处理缺失值\r5.3.1 数据清洗从数据集中纠正或消除不准确、损坏、格式错误、重复或不完整的数据的做法称为数据清理。 5.3.2 填补缺失值的重要性如果没有正确处理缺失的数值，可能会对数据得出错误的结论，这将对建模阶段产生重大影响，会影响最后的结果。 5.3.3 缺失值导致的问题 在缺乏证据的情况下，统计能力，即检验在零假设错误时拒绝该零假设的几率会降低。 数据的丢失可能导致参数估计出现偏差。 具有降低样本代表性的能力。 5.3.4 缺失数据类型根据数据集或数据中不存在的模式或数据，可以将其分类。 完全随机缺失(MCAR)： 当丢失数据的概率与要获得的精确值或观察到的答案的集合无关时 solution: deleting rows or columns 随机缺失(MAR)： 当丢失响应的概率由观察到的响应的集合而不是预期达到的精确缺失值决定时。 solution: imputation of data 非随机缺失(MNAR)： MNAR 是缺失数据，这类数据案例很难处理，可以对缺失数据进行建模是获得参数的公平近似值的唯一方法 solution improve dataset find data 5.3.5 缺失值的类别 连续变量或特征 — — 数值数据集，即数字可以是任何类型 分类变量或特征 — — 它可以是数值的或客观的类型 5.3.6 缺失值插补类型插补有多种大小和形式。这是在为我们的应用程序建模以提高精度之前解决数据集中缺失数据问题的方法之一。 单变量插补或均值插补是指仅使用目标变量对值进行插补。 多元插补： 根据其他因素插补值，例如使用线性回归根据其他变量估计缺失值。 单一插补： 要构建单个插补数据集，只需在数据集中插补一次缺失值。 大量插补： 在数据集中多次插补相同的缺失值。这本质上需要重复单个插补以获得大量插补数据集。 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:7:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#535-缺失值的类别"},{"categories":["machine learning"],"content":"\r5.3 数据预处理 - 处理缺失值\r5.3.1 数据清洗从数据集中纠正或消除不准确、损坏、格式错误、重复或不完整的数据的做法称为数据清理。 5.3.2 填补缺失值的重要性如果没有正确处理缺失的数值，可能会对数据得出错误的结论，这将对建模阶段产生重大影响，会影响最后的结果。 5.3.3 缺失值导致的问题 在缺乏证据的情况下，统计能力，即检验在零假设错误时拒绝该零假设的几率会降低。 数据的丢失可能导致参数估计出现偏差。 具有降低样本代表性的能力。 5.3.4 缺失数据类型根据数据集或数据中不存在的模式或数据，可以将其分类。 完全随机缺失(MCAR)： 当丢失数据的概率与要获得的精确值或观察到的答案的集合无关时 solution: deleting rows or columns 随机缺失(MAR)： 当丢失响应的概率由观察到的响应的集合而不是预期达到的精确缺失值决定时。 solution: imputation of data 非随机缺失(MNAR)： MNAR 是缺失数据，这类数据案例很难处理，可以对缺失数据进行建模是获得参数的公平近似值的唯一方法 solution improve dataset find data 5.3.5 缺失值的类别 连续变量或特征 — — 数值数据集，即数字可以是任何类型 分类变量或特征 — — 它可以是数值的或客观的类型 5.3.6 缺失值插补类型插补有多种大小和形式。这是在为我们的应用程序建模以提高精度之前解决数据集中缺失数据问题的方法之一。 单变量插补或均值插补是指仅使用目标变量对值进行插补。 多元插补： 根据其他因素插补值，例如使用线性回归根据其他变量估计缺失值。 单一插补： 要构建单个插补数据集，只需在数据集中插补一次缺失值。 大量插补： 在数据集中多次插补相同的缺失值。这本质上需要重复单个插补以获得大量插补数据集。 ","date":"2021-10-29","objectID":"/pumpkin-book-notes06/:7:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 06","uri":"/pumpkin-book-notes06/#536-缺失值插补类型"},{"categories":["machine learning"],"content":"\r6. 支持向量机它是一个几何角度建模模型，对于线性可分数据集，支持向量机就是找距离正负样本都最远的超平面。相比于感知机（它就是找到一个超平面能将正负分开就行，解不唯一），其解是唯一的，且不偏不倚，泛化性能更好。 ","date":"2021-10-20","objectID":"/pumpkin-book-notes05/:1:0","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 05","uri":"/pumpkin-book-notes05/#6-支持向量机"},{"categories":["machine learning"],"content":"\r6.0 预备知识\r超平面\r一维超平面是点，三位超平面是个面，n维空间的超平面（$\\omega^Tx+b=0$，其中$\\omega, \\ x \\in{R^n}$）\r* 超平面不唯一\r* 法向量$\\omega$和位移项b确定唯一一个超平面\r* 法向量$\\omega$垂直于超平面（缩放$\\omega,b$时，若缩放倍数为负数会改变法向量方向）\r* 法向量$\\omega$指向的那一半空间为正空间（正空间的点代入方程是大于0），另一半为负空间（负空间点代入小于0）\r* 任意点x到超平面的距离公式：\r$$r=\\frac{|\\omega^Tx+b|}{||\\omega||}$$\r证明点到超平面的距离公式\r证明：对于任意一点$x_0=(x^0_1,x^0_2,...,x^0_n)^T$，设其在超平面$\\omega^Tx+b=0$上的投影点$x_1=(x^1_1,x^1_2,...,x^1_n)^T$，则$\\omega^Tx_1+b=0$，且向量$\\overrightarrow{x_1x_0}$与法向量$\\omega$平行，因此\r向量点乘：$a \\cdot b = ||a|| \\cdot cos\\theta \\cdot ||b||$ (向量a模长乘上cos两个的夹角乘上b的模长)\r$$|\\omega \\overrightarrow{x_1x_0}| = ||\\omega|| \\ cos\\pi \\ ||\\overrightarrow{x_1x_0}|| = ||\\omega|| \\ ||\\overrightarrow{x_1x_0}|| = ||\\omega|| r$$\r$\\overrightarrow{x_1x_0}$ 就是点到超平面的距离，记作r\r$$\\omega \\overrightarrow{x_1x_0} = \\omega_1(x^0_1-x^1_1)+\\omega_2(x^0_2-x^1_2)+...+\\omega_n(x^0_n-x^1_n)$$\r$$=\\omega_1x^0_1+\\omega_2x^0_2+...+\\omega_nx^0_n\\omega_1x^1_1+\\omega_2x^1_2+...+\\omega_nx^1_n)$$\r$$=\\omega^Tx_0-x^Tx_1$$\r其中，$\\omega^Tx_0$等于证明假设中 $\\omega^Tx_1$，$-x^Tx_1$ 等于 $\\omega^Tx_1+b=0$ 的 b\r$$=\\omega^Tx_0+b$$\r$$|\\omega^Tx_0+b|=||\\omega|| r \\Rightarrow r = \\frac{|\\omega^Tx+b|}{||\\omega||}$$\r几何间隔\r对于给定的数据集X和超平面 $\\omega^Tx+b=0$，定义数据集X中任意一个**样本点**$(x_i,y_i), \\ y_i \\in{\\begin{Bmatrix} -1,1 \\end{Bmatrix}}, \\ i=1,2,...,m$ 关于超平面的几何间隔为（其中y取值为1，-1方便计算，也可以取值1，0都行）\r$$\\gamma_i=\\frac{y_i(\\omega^Tx_i+b)}{||\\omega||}$$\r正确分类时：$\\gamma_i \u003e0$，几何间隔此时等价于点到超平面距离。\r没正确分类时，$\\gamma_i \u003c0$。\r对于给定的数据集X和超平面，定义**数据集**关于超平面的几何间隔为：数据集中所有样本点的几何间隔最小值：\r$$\\gamma=min_{i=1,2,...,m}\\gamma_i$$\r","date":"2021-10-20","objectID":"/pumpkin-book-notes05/:1:1","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 05","uri":"/pumpkin-book-notes05/#60-预备知识"},{"categories":["machine learning"],"content":"\r6.0 预备知识\r超平面\r一维超平面是点，三位超平面是个面，n维空间的超平面（$\\omega^Tx+b=0$，其中$\\omega, \\ x \\in{R^n}$）\r* 超平面不唯一\r* 法向量$\\omega$和位移项b确定唯一一个超平面\r* 法向量$\\omega$垂直于超平面（缩放$\\omega,b$时，若缩放倍数为负数会改变法向量方向）\r* 法向量$\\omega$指向的那一半空间为正空间（正空间的点代入方程是大于0），另一半为负空间（负空间点代入小于0）\r* 任意点x到超平面的距离公式：\r$$r=\\frac{|\\omega^Tx+b|}{||\\omega||}$$\r证明点到超平面的距离公式\r证明：对于任意一点$x_0=(x^0_1,x^0_2,...,x^0_n)^T$，设其在超平面$\\omega^Tx+b=0$上的投影点$x_1=(x^1_1,x^1_2,...,x^1_n)^T$，则$\\omega^Tx_1+b=0$，且向量$\\overrightarrow{x_1x_0}$与法向量$\\omega$平行，因此\r向量点乘：$a \\cdot b = ||a|| \\cdot cos\\theta \\cdot ||b||$ (向量a模长乘上cos两个的夹角乘上b的模长)\r$$|\\omega \\overrightarrow{x_1x_0}| = ||\\omega|| \\ cos\\pi \\ ||\\overrightarrow{x_1x_0}|| = ||\\omega|| \\ ||\\overrightarrow{x_1x_0}|| = ||\\omega|| r$$\r$\\overrightarrow{x_1x_0}$ 就是点到超平面的距离，记作r\r$$\\omega \\overrightarrow{x_1x_0} = \\omega_1(x^0_1-x^1_1)+\\omega_2(x^0_2-x^1_2)+...+\\omega_n(x^0_n-x^1_n)$$\r$$=\\omega_1x^0_1+\\omega_2x^0_2+...+\\omega_nx^0_n\\omega_1x^1_1+\\omega_2x^1_2+...+\\omega_nx^1_n)$$\r$$=\\omega^Tx_0-x^Tx_1$$\r其中，$\\omega^Tx_0$等于证明假设中 $\\omega^Tx_1$，$-x^Tx_1$ 等于 $\\omega^Tx_1+b=0$ 的 b\r$$=\\omega^Tx_0+b$$\r$$|\\omega^Tx_0+b|=||\\omega|| r \\Rightarrow r = \\frac{|\\omega^Tx+b|}{||\\omega||}$$\r几何间隔\r对于给定的数据集X和超平面 $\\omega^Tx+b=0$，定义数据集X中任意一个**样本点**$(x_i,y_i), \\ y_i \\in{\\begin{Bmatrix} -1,1 \\end{Bmatrix}}, \\ i=1,2,...,m$ 关于超平面的几何间隔为（其中y取值为1，-1方便计算，也可以取值1，0都行）\r$$\\gamma_i=\\frac{y_i(\\omega^Tx_i+b)}{||\\omega||}$$\r正确分类时：$\\gamma_i \u003e0$，几何间隔此时等价于点到超平面距离。\r没正确分类时，$\\gamma_i \u003c0$。\r对于给定的数据集X和超平面，定义**数据集**关于超平面的几何间隔为：数据集中所有样本点的几何间隔最小值：\r$$\\gamma=min_{i=1,2,...,m}\\gamma_i$$\r","date":"2021-10-20","objectID":"/pumpkin-book-notes05/:1:1","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 05","uri":"/pumpkin-book-notes05/#超平面"},{"categories":["machine learning"],"content":"\r6.0 预备知识\r超平面\r一维超平面是点，三位超平面是个面，n维空间的超平面（$\\omega^Tx+b=0$，其中$\\omega, \\ x \\in{R^n}$）\r* 超平面不唯一\r* 法向量$\\omega$和位移项b确定唯一一个超平面\r* 法向量$\\omega$垂直于超平面（缩放$\\omega,b$时，若缩放倍数为负数会改变法向量方向）\r* 法向量$\\omega$指向的那一半空间为正空间（正空间的点代入方程是大于0），另一半为负空间（负空间点代入小于0）\r* 任意点x到超平面的距离公式：\r$$r=\\frac{|\\omega^Tx+b|}{||\\omega||}$$\r证明点到超平面的距离公式\r证明：对于任意一点$x_0=(x^0_1,x^0_2,...,x^0_n)^T$，设其在超平面$\\omega^Tx+b=0$上的投影点$x_1=(x^1_1,x^1_2,...,x^1_n)^T$，则$\\omega^Tx_1+b=0$，且向量$\\overrightarrow{x_1x_0}$与法向量$\\omega$平行，因此\r向量点乘：$a \\cdot b = ||a|| \\cdot cos\\theta \\cdot ||b||$ (向量a模长乘上cos两个的夹角乘上b的模长)\r$$|\\omega \\overrightarrow{x_1x_0}| = ||\\omega|| \\ cos\\pi \\ ||\\overrightarrow{x_1x_0}|| = ||\\omega|| \\ ||\\overrightarrow{x_1x_0}|| = ||\\omega|| r$$\r$\\overrightarrow{x_1x_0}$ 就是点到超平面的距离，记作r\r$$\\omega \\overrightarrow{x_1x_0} = \\omega_1(x^0_1-x^1_1)+\\omega_2(x^0_2-x^1_2)+...+\\omega_n(x^0_n-x^1_n)$$\r$$=\\omega_1x^0_1+\\omega_2x^0_2+...+\\omega_nx^0_n\\omega_1x^1_1+\\omega_2x^1_2+...+\\omega_nx^1_n)$$\r$$=\\omega^Tx_0-x^Tx_1$$\r其中，$\\omega^Tx_0$等于证明假设中 $\\omega^Tx_1$，$-x^Tx_1$ 等于 $\\omega^Tx_1+b=0$ 的 b\r$$=\\omega^Tx_0+b$$\r$$|\\omega^Tx_0+b|=||\\omega|| r \\Rightarrow r = \\frac{|\\omega^Tx+b|}{||\\omega||}$$\r几何间隔\r对于给定的数据集X和超平面 $\\omega^Tx+b=0$，定义数据集X中任意一个**样本点**$(x_i,y_i), \\ y_i \\in{\\begin{Bmatrix} -1,1 \\end{Bmatrix}}, \\ i=1,2,...,m$ 关于超平面的几何间隔为（其中y取值为1，-1方便计算，也可以取值1，0都行）\r$$\\gamma_i=\\frac{y_i(\\omega^Tx_i+b)}{||\\omega||}$$\r正确分类时：$\\gamma_i \u003e0$，几何间隔此时等价于点到超平面距离。\r没正确分类时，$\\gamma_i \u003c0$。\r对于给定的数据集X和超平面，定义**数据集**关于超平面的几何间隔为：数据集中所有样本点的几何间隔最小值：\r$$\\gamma=min_{i=1,2,...,m}\\gamma_i$$\r","date":"2021-10-20","objectID":"/pumpkin-book-notes05/:1:1","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 05","uri":"/pumpkin-book-notes05/#几何间隔"},{"categories":["machine learning"],"content":"\r6.1 支持向量机模型角度：给定线性可分数据集X，支持向量机模型希望求得数据集X关于超平面的几何间隔 $\\gamma$ 达到最大的那那个超平面，然后套上一个sign函数实现分类功能（简单理解，该函数就是值大于0取1，小于0取-1） $$y=sign(\\omega^Tx+b)=\r\\begin{cases} 1, \\ \\omega^Tx+b\u003e0 \\\\ -1, \\ \\omega^Tx+b \u003c0 \\end{cases}$$\r所起其本质和感知机一样，仍然是在求一个超平面。那么几何间隔最大的超平面一定是前面所说的那个距离正负样本点都最远的超平面。因为：\r* 当超平面没有正确划分正负样本时：几何间隔最小的为误分类点，因此 $\\gamma \u003c 0$\r* 当超平面正确划分超平面时：$\\gamma \\eqslantgtr 0$，且越靠近中央其值越大。\r策略角度：给定线性可分数据集X，设X中集合间隔最小的样本为$(x_min, y_min)$，那么支**持向量机找超平面的过程**可以转化为以下带约束条件的优化问题\r首先我们要找到数据集关于超平面的最大几何间隔\r$$max \\ \\ \\ \\gamma$$\r对于每个样本关于超平面的几何间隔中要找到最小的\r$$s.t. \\ \\ \\ \\gamma_i \\eqslantgtr \\gamma, \\ \\ i = 1,2,...,m$$\r最小的几何间隔\r$$max_{\\omega,b} \\ \\ \\frac{y_{min}(\\omega^Tx_{min}+b)}{||\\omega||}$$\r$$s.t. \\ \\ \\ \\frac{y_i(\\omega^Tx_i+b)}{||\\omega||} \\eqslantgtr \\frac{y_{min}(\\omega^Tx_{min}+b)}{||\\omega||}, \\ \\ i=1,2,...,m$$\r在约束中将分母$\\omega$约掉\r$$max_{\\omega,b} \\ \\ \\ \\frac{y_{min}(\\omega^Tx_{min}+b)}{||\\omega||}$$\r$$s.t. \\ \\ \\ y_i(\\omega^Tx_i+b) \\eqslantgtr y_{min}(\\omega^Tx_{min}+b), \\ \\ i=1,2,...,m$$\r假设该问题的最优解为($\\omega^*,b^*$)，那么$(\\alpha \\omega^*, \\alpha b^*), \\alpha \\in R^+$ 也是最优解，且超平面也不变，因此还需要对$\\omega, b$做一定限制才能使得上述优化问题有可能的唯一解。\r不防令$y_{min}(\\omega^Tx_{min}+b)=1$（这里是固定分子，等于1这个值可以随便取，2、100都行，只要能解出来就行），因为对于特定的(x_{min},y_{min})来说，能使得$y_{min}(\\omega^Tx_{min})=1$的$\\alpha$有且只有一个（ymin、xmin是固定的且唯一的，如果再将取值固定了，那么w、b就也是固定的了）。因此上述优化问题进一步转化为：\r$$max_{\\omega,b} \\ \\ \\ \\ \\frac{1}{||\\omega||}$$\r$$s.t. \\ \\ \\ \\ y_i(\\omega^Tx_i+b) \\eqslantgtr 1, \\ \\ i=1,2,...,m$$\r为了方便后续计算，将最大化问题转成最小化，其中平方是为了方便计算，二分之一是为了在求导的时候方便约掉，即\r$$max_{\\omega,b} \\ \\ \\ \\ \\frac{1}{2} ||\\omega||^2$$\r$$s.t. \\ \\ \\ \\ 1-y_i(\\omega^Tx_i+b) \\eqslantless 0, \\ \\ i=1,2,...,m$$\r此优化问题含不等式约束的优化问题，且为凸优化问题，因此可以直接用很多专门求解凸优化问题的方法求解。在这里支持向量机通常采用拉格朗日对偶来求解。 ","date":"2021-10-20","objectID":"/pumpkin-book-notes05/:1:2","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 05","uri":"/pumpkin-book-notes05/#61-支持向量机"},{"categories":["machine learning"],"content":"\r6.2 对偶问题对最大化间隔可以用拉格朗日乘子法得对偶问题 dual problem 6.2.1 基础知识：凸优化问题对于一般地约束优化问题： $$min \\ \\ \\ f(x)$$\r$$s.t. \\ \\ \\ \\begin{cases} g_i(x) \\eqslantless 0 \\ \\ \\ i=1,2,...,m \\\\\rh_j(x) = 0 \\ \\ \\ j=1,2,...,n\r\\end{cases}\r$$\r若目标函数发f(x)是凸函数，约束集合是凸集，则称为凸优化问题，特别地，g是凸函数，h是线性函数时，约束集合和为凸集，该优化问题为凸优化问题。显然，支持向量机的目标函数$\\frac{1}{2}||\\omega||^2$是关于w的凸函数，不等式约束$1-y_i(\\omega^Tx_i+b)$是关于w的凸函数，因此支持向量机是一个凸优化问题。 拉格朗日对偶不一定用来解决凸优化问题，主要是是对一般地约束优化问题 设上述优化问题的定义域D是f、g、h三个函数定义域关于x的交集，可行集D^~在属于交集D的基础上，gi(x) \u003c= 0，hj(x) = 0。显然D^~是D的自己，最优质为 $p^*=min{f(\\hat{x})}$。由拉格朗日函数的定义可知上述优化问题的拉格朗日函数为\r$$L(x,\\mu,\\lambda) = f(x) + \\sum^m_{i=1} \\mu_i g_i(x) + \\sum^n_{j=1} \\lambda_j h_j(x)$$\r其中$\\mu=(\\mu_1,\\mu_2,...,\\mu_n)^T, \\ \\lambda = (\\lambda_1, \\lambda_2,...,\\lambda_n)^T$为拉格朗日乘子向量。\r定义上述优化问题的拉格朗日对偶函数$F(\\mu,\\lambda)$（注意其自变量不包含x）为$L(x,\\mu,\\lambda)$关于x的下确界（inf(e^x)=0，其中0就是e^x函数的下确界，永远也去不到的最小值），也即：\r$$F(\\mu,\\lambda) = {inf}_{x\\in D} L(x,\\mu,\\lambda) = inf_{x\\in D} \\Bigg( f(x) + \\sum^m_{i=1} \\mu_i g_i(x) + \\sum^n_{j=1} \\lambda_j h_j(x) \\Bigg)$$\r对偶函数F有如下性质：\r* 无论是否是凸优化问题，其对偶函数F恒为凹函数\r* 当lambda \u003e= 0时，F构成了上述优化问题最优质p^*的下界，也即\r$$F(\\mu,\\lambda) \\eqslantless p^*$$\r为什么支持向量机通常都采用拉格朗日对偶求解？ 因为无论主问题是什么优化问题，对偶问题一定是凸优化问题，凸优化问题是优化问题中最好解的。原始问题的时间复杂度和特征维数成正比（因为未知量是omega），而对偶问题和数据量成正比（因为未知量是alpha），当特征维数远高于数据量的时候拉格朗日对偶更高效。 对偶函数能很自然引入核函数，进而推广到非线性分类问题（主要原因） ","date":"2021-10-20","objectID":"/pumpkin-book-notes05/:1:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 05","uri":"/pumpkin-book-notes05/#62-对偶问题"},{"categories":["machine learning"],"content":"\r6.2 对偶问题对最大化间隔可以用拉格朗日乘子法得对偶问题 dual problem 6.2.1 基础知识：凸优化问题对于一般地约束优化问题： $$min \\ \\ \\ f(x)$$\r$$s.t. \\ \\ \\ \\begin{cases} g_i(x) \\eqslantless 0 \\ \\ \\ i=1,2,...,m \\\\\rh_j(x) = 0 \\ \\ \\ j=1,2,...,n\r\\end{cases}\r$$\r若目标函数发f(x)是凸函数，约束集合是凸集，则称为凸优化问题，特别地，g是凸函数，h是线性函数时，约束集合和为凸集，该优化问题为凸优化问题。显然，支持向量机的目标函数$\\frac{1}{2}||\\omega||^2$是关于w的凸函数，不等式约束$1-y_i(\\omega^Tx_i+b)$是关于w的凸函数，因此支持向量机是一个凸优化问题。 拉格朗日对偶不一定用来解决凸优化问题，主要是是对一般地约束优化问题 设上述优化问题的定义域D是f、g、h三个函数定义域关于x的交集，可行集D^~在属于交集D的基础上，gi(x) \u003c= 0，hj(x) = 0。显然D^~是D的自己，最优质为 $p^*=min{f(\\hat{x})}$。由拉格朗日函数的定义可知上述优化问题的拉格朗日函数为\r$$L(x,\\mu,\\lambda) = f(x) + \\sum^m_{i=1} \\mu_i g_i(x) + \\sum^n_{j=1} \\lambda_j h_j(x)$$\r其中$\\mu=(\\mu_1,\\mu_2,...,\\mu_n)^T, \\ \\lambda = (\\lambda_1, \\lambda_2,...,\\lambda_n)^T$为拉格朗日乘子向量。\r定义上述优化问题的拉格朗日对偶函数$F(\\mu,\\lambda)$（注意其自变量不包含x）为$L(x,\\mu,\\lambda)$关于x的下确界（inf(e^x)=0，其中0就是e^x函数的下确界，永远也去不到的最小值），也即：\r$$F(\\mu,\\lambda) = {inf}_{x\\in D} L(x,\\mu,\\lambda) = inf_{x\\in D} \\Bigg( f(x) + \\sum^m_{i=1} \\mu_i g_i(x) + \\sum^n_{j=1} \\lambda_j h_j(x) \\Bigg)$$\r对偶函数F有如下性质：\r* 无论是否是凸优化问题，其对偶函数F恒为凹函数\r* 当lambda \u003e= 0时，F构成了上述优化问题最优质p^*的下界，也即\r$$F(\\mu,\\lambda) \\eqslantless p^*$$\r为什么支持向量机通常都采用拉格朗日对偶求解？ 因为无论主问题是什么优化问题，对偶问题一定是凸优化问题，凸优化问题是优化问题中最好解的。原始问题的时间复杂度和特征维数成正比（因为未知量是omega），而对偶问题和数据量成正比（因为未知量是alpha），当特征维数远高于数据量的时候拉格朗日对偶更高效。 对偶函数能很自然引入核函数，进而推广到非线性分类问题（主要原因） ","date":"2021-10-20","objectID":"/pumpkin-book-notes05/:1:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 05","uri":"/pumpkin-book-notes05/#621-基础知识凸优化问题"},{"categories":["machine learning"],"content":"\r6.3 软间隔上述都是线性可分的问题，而现实中，大部分是线性不可分的问题，因此需要允许支持向量机犯错（软间隔）。 从数学角度来说，软间隔就是允许部分样本（也就是异常数据，尽可能要少）不满足下式中的约束条件： $$min_{\\omega,b} \\ \\ \\ \\frac{1}{2}||\\omega||^2$$\r$$s.t. \\ \\ \\ y_i(\\omega^Tx_i+b) \\eqslantgtr 1, \\ \\ \\ i=1,2,...,m$$\r因此，可以将必须严格执行的约束条件转化为具有一定灵活性的损失，合格的损失函数要求如下：\r* 当满足约束条件时，损失为0\r* 当不满足约束条件时，损失不为0\r* （可选）当不满足约束条件时，损失与其违反约束条件的成都成正比\r只要满足上述要求，才能保证最小化损失的过程中，保证不满足约束条件的样本尽可能的少。\r$$min_{\\omega,b} \\frac{1}{2} ||\\omega||^2 + C\\sum^m_{i=1} l_{0/1} (y_i(\\omega^T x_i +b)-1)$$\r其中，l(0/1)是0/1损失函数\r$$ l_{0/1}(z) =\r\\begin{cases}\r1, \\ \\ \\ if z \u003c 0 \\\\\r0, \\ \\ \\ if z \\eqslantgtr 0\r\\end{cases}\r$$\rC\u003e0是一个常数，用来调节损失的权重，显然当$C \\rightarrow + \\infin$时，会迫使所有样本的损失为0，进而退化为严格执行的约束条件，退化为硬间隔，因此，本世子可以看作支持向量机的一般化形式。\r由于l0/1非凸、非连续，数学性质不好，使得上式不易求解，因此常用一些数学性质较好的替代损失函数来代替，软间隔支持向量机通常采用的是hinge（合页）损失来代替：(合页损失函数是一个连续的凸函数)\r$$hinge损失：l_{hinge}(z)=max(0,1-z)$$\r替换进上式可得：\r$$min_{\\omega,b} \\frac{1}{2} ||\\omega||^2 + C \\sum^m_{i=1} max(0,1-y_i(\\omega^Tx_i+b))$$\r引入松弛变量 $\\xi$，上述优化问题遍和下述优化问题等价\r$$min_{\\omega, b, \\xi_i} \\ \\ \\ \\ \\frac{1}{2} ||\\omega||^2 + C \\sum^m_{i=1} \\xi_i$$\r$$s.t. \\begin{cases}\ry_i(\\omega^Tx_i +b) \\eqslantgtr 1- \\xi_i \\\\\r\\xi_i \\eqslantgtr 0, i = 1,2,...,m\r\\end{cases}\r$$\r证明：令\r$$max(0,1-y_i(\\omega^T x_i +b)) = \\xi_i$$\r显然 $\\xi_i \\eqslantgtr 0$，当 $1-y_i(\\omega^Tx_i+b) \u003e 0 \\Rightarrow \\xi_i = 1 - y_i(\\omega^Tx_i+b)$，当 $1-y_i(\\omega^Tx_i+b) \\eqslantless 0 \\Rightarrow \\xi_i = 0$，所以\r$$1-y_i(\\omega^Tx_i+b) \\eqslantless \\xi_i \\Rightarrow y_i(\\omega^Tx_i+b) \\eqslantgtr 1-\\xi_i$$\r","date":"2021-10-20","objectID":"/pumpkin-book-notes05/:1:4","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 05","uri":"/pumpkin-book-notes05/#63-软间隔"},{"categories":["machine learning"],"content":"\r支持向量回归相比于线性回归用一条线来拟合训练样本，支持向量回归（SVR）而采用一个以 $f(x)=\\omega^Tx+b$为中心，宽度为$2\\epsilon$的间隔带，来拟合训练样本。 落在带子上的样本不计算损失（类比线性回归在线性上的点预测误差为0），不在带子上的则偏离带子的距离作为损失（类比线性回归的均方误差），然后以最小化损失的方式破事间隔带从样本最密集的地方（中心地带）穿过，进而达到拟合训练样本的目的。 因此SVR优化问题可以写成 $$min_{\\omega, b} \\frac{1}{2} ||\\omega||^2 + C \\sum^m_{i=1} l_\\epsilon (f(x_i) - y_i)$$\r其中$l_\\epsilon (z)$为$\\epsilon$不敏感损失函数（类比均方误差损失）\r$$ l_\\epsilon (z) = \\begin{cases}\r0, \\ \\ \\ if|z| \\eqslantless \\epsilon \\\\\r|z|-\\epsilon, \\ \\ \\ if|z| \u003e \\epsilon\r\\end{cases}\r$$\r$\\frac{1}{2}||\\omega||^2$为L2正则项，此处引入正则项除了起正则化本身的作用外，也是为了和（软间隔）支持向量机的优化目标保持形式上的一致（在这里不用均方误差也是此目的），这样就可以导出对偶函数问题引入核函数，C为调节损失权重的常数。\r同软间隔支持向量机，引入松弛$\\xi_i$，令\r$$l_{\\epsilon}(f(x_i)-y_i)=\\xi_i$$\r显然$\\xi_i \\eqslantgtr 0$，并且\r当 $|f(x_i)-y_i| \\eqslantless \\epsilon \\Rightarrow \\xi_i = 0$\r当 $|f(x_i)-y_i| \u003e \\epsilon \\Rightarrow \\xi_i = |f(x_i)-y_i|-\\epsilon$\r所以\r$$|f(x_i) - y_i| - \\epsilon \\eqslantless \\xi_i \\Rightarrow |f(x_i) - y_i| \\eqslantless \\epsilon + \\xi_i$$\r$$-\\epsilon-\\xi_i \\eqslantless f(x_i)-y_i \\eqslantless \\epsilon+\\xi_i$$\r那么SVR的优化问题可以改写为\r$$min_{\\omega,b,\\xi_i} \\frac{1}{2} ||\\omega||^2 + C \\sum^m_{i=m} \\xi_i$$\r$$s.t. \\ \\ \\ -\\epsilon-\\xi_i \\eqslantless f(x_i)-y_i \\eqslantless \\epsilon + \\xi_i \\\\\r\\xi_i \\eqslantgtr0,i=1,2,...,m$$\r如果考虑两边采用不同的松弛程度\r$$min_{\\omega,b,\\xi_i,\\hat{\\xi_i}} \\frac{1}{2} ||\\omega||^2 + C \\sum^m_{i=m} (\\xi_i+\\hat{\\xi_i})$$\r$$s.t. \\ \\ \\ -\\epsilon-\\hat{\\xi_i} \\eqslantless f(x_i)-y_i \\eqslantless \\epsilon + \\xi_i \\\\\r\\xi_i \\eqslantgtr0,\\hat{\\xi_i} \\eqslantgtr0,i=1,2,...,m$$\r","date":"2021-10-20","objectID":"/pumpkin-book-notes05/:1:5","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 05","uri":"/pumpkin-book-notes05/#支持向量回归"},{"categories":["machine learning"],"content":"\r5. 神经网络 neural networks","date":"2021-10-15","objectID":"/pumpkin-book-notes04/:1:0","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 04","uri":"/pumpkin-book-notes04/#5-神经网络-neural-networks"},{"categories":["machine learning"],"content":"\r5.1 神经元模型神经元neuron是神经网络最基本的成分。 M-P神经元模型模仿生物神经网络，通过激活函数activation function处理神经元输出。 激活函数是阶跃函数，其输入值被映射成0或1，其中，1对应神经元兴奋，0对应神经元抑制。 但是由于阶跃函数的不连续性，实际常使用 Sigmoid函数作为激活函数，它可以将值挤压到(0,1)之间，有时也被叫做挤压函数squashing function。 将许多的神经元按照一定层次结构连接起来，就得到神经网络。 ","date":"2021-10-15","objectID":"/pumpkin-book-notes04/:1:1","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 04","uri":"/pumpkin-book-notes04/#51-神经元模型"},{"categories":["machine learning"],"content":"\r5.2 感知机perceptron与多层网络perceptron由两层神经网络组成，输入层接受外界输入信号后传递给输出层，输出层是M-P神经元也叫做阈值逻辑单元 threshold logic unit。感知机能容易地实现逻辑与、或、非运算。 感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元，学习能力十分有限。因此，对于非线性问题就要考虑多层功能神经元，两层感知机就能解决异或问题。 常见的神经网络是每层的每个神经元都会与下一层神经元全互连，不存在同层连接和跨层连接，这被称为 多层前馈神经网络 multi-layer feedforward neural networks。 输入层接受外界输入信号，不进行函数处理 隐层与输出层对包含功能神经元，会对信号加工处理 输出层进行输出 可知，只要有隐层，就是对层网络。 ","date":"2021-10-15","objectID":"/pumpkin-book-notes04/:1:2","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 04","uri":"/pumpkin-book-notes04/#52-感知机perceptron与多层网络"},{"categories":["machine learning"],"content":"\r5.3 误差逆传播算法 Error BackPropagation误差逆传播是最成功地训练多层网络的算法。不仅可用于多层前馈神经网络，还可用于其他类型神经网络，比如递归神经网络。但通常来说BP网络就是多层前馈神经网络。 假设，多层神经网络是拥有d个输入神经元、l个输出神经元、q个隐层神经元的多层前馈网络结构。输入信号x和输入层与隐层之间的连接权相乘得到，即，隐层到输出的输入信号。隐层与输出层之间的连接权乘以该输入信号的到输出层的输入信号，最后将信号输出得到y。 BP神经网络由于强大的表示能力会经常遭遇股过拟合。一般两种策略来缓解： 早停early stopping 正则化 regularization ","date":"2021-10-15","objectID":"/pumpkin-book-notes04/:1:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 04","uri":"/pumpkin-book-notes04/#53-误差逆传播算法-error-backpropagation"},{"categories":["machine learning"],"content":"\r4. 决策树 decision tree主流应用在集成学习中多棵决策树构建森林。 ","date":"2021-10-11","objectID":"/pumpkin-book-notes03/:1:0","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 03","uri":"/pumpkin-book-notes03/#4-决策树-decision-tree"},{"categories":["machine learning"],"content":"\r4.1 基本流程又叫判定树，在分类问题中，它基于树结构来进行决策和判定，给出最终判定结果。 决策树的生成是一个递归过程，三种情形导致递归： 当前节点包含的样本全属于同一类别，无需划分。 当前属性集为空，或全样本所在的所有属性上取值相同，无法划分。 当前节点包含的样本集合为空，不能划分。 其中，2情形下，把当前节点标记为叶节点，并将其类别设定为该节点所含样本最多的类别。3情形下，同样把当前节点标记为叶节点，但是将其类别设定为其父节点所含样本最多的类别。 二者处理实质不同，2利用的是后验分布，3是先验分布。 算法原理：逻辑上通过 if else语句判断；几何上根据某种准则划分特征空间，最终目的是将样本越分越纯。 特征空间是将样本各个特征作为一个轴，构成的空间。 ","date":"2021-10-11","objectID":"/pumpkin-book-notes03/:1:1","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 03","uri":"/pumpkin-book-notes03/#41-基本流程"},{"categories":["machine learning"],"content":"\r4.2 划分选择决策树学习的关键是如何选择最优划分属性，也就是节点purity要高。 4.2.1 信息增益information entropy 信息熵 是度量样本集合纯度最常用的指标。 $$Ent(D)=-\\sum^{|y|}_{k=1} p_k {log}_2 p_k$$ 其中，约定p=0，plog2p=0；ent值越小，d纯度越高。（结合2为底的对数函数图像去看就很好理解） 条件熵（Y的信息熵关于概率分布X的期望，条件熵也就是Y的信息熵）：在已知X后Y的不确定性 $$H(Y|X) = \\sum_x p(x) H(Y|X=x)$$ 解释：从单属性（特征）a（色泽）的角度来看，假设其可能取值为{a1, a2, a3, …, a^V}，其中，a1 \u003c 30，a2 \u003e 30。D^v表示属性a取到这些值的集合，因此D^1表示色泽小于30的样本集合，D^2表示色泽大于30的样本集合。 |D^v| / D 表示当前属样本集合（即a1或a2）占总样本（a1+…+av）的占比，那么在已知属性a的取值后，样本集合D的条件熵为 $$\\sum^V_{v=1} \\frac{|D^v|}{|D|} Ent(D^v)$$ 信息增益 $$Gain(D,a)=Ent(D)-\\sum^V_{v=1}\\frac{|D|^v}{|D|}Ent(D^v)$$ 信息增益越大，意味着属性a来进行划分所获得的纯度提升越大。 在已知属性（特征）a的取值后y的不确定性减少的量（差值），即纯度的提升。 通常前者值 $Ent(D)$（信息熵）大，后者值 $\\sum^V_{v=1}\\frac{|D|^v}{|D|}Ent(D^v)$（条件熵）小。 4.2.2 增益率然后，信息增益准则对取值数目较多的属性有所偏好，为减少这种偏好带来的不利影响，引入了增益率 gain ratio来选择最优划分属性 $$Gain \\ ratio(D,a)=\\frac{Gain(D,a)}{IV(a)}$$ 其中 $$IV(a)=\\sum^V_{v=1} \\frac{|D^v|}{D} {log}_2 \\frac{|D^v|}{|D|}$$ IV(a)称为属性a的固有值 intrinsic value。属性a可能取值数目越多（V越大），则IV(a)的值通常越大。 要注意的是和信息增益准则相反，增益率准则对可取值数目较少的属性有偏好。所以它不能直接用来进行属性的划分，而是使用了一个启发式，先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的。 4.2.3 基尼指数 Gini$$Gini(D,a) = \\sum^{|y|}{k=1} \\sum{k^{’} \\neq{k}} p_k p_{k^{’}}$$ $$=1-\\sum^{|y|}_{k=1} p^2_k$$ 直观来说，Gini(D) 反映了数据集D中随机抽取两个两本，其类别标记不一致的概率。因此，Gini越小，碰到异类的概率就小，数据集D的纯度越高。 属性a的基尼指数定义为 $$Gini_index(D,a) = \\sum^V_{v=1} \\frac{|D^v|}{|D|} Gini(D^v)$$ 因此，在候选属性集合A中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即 $a_* = arg_{a \\in A} min Gini_index(D,a)$ ","date":"2021-10-11","objectID":"/pumpkin-book-notes03/:1:2","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 03","uri":"/pumpkin-book-notes03/#42-划分选择"},{"categories":["machine learning"],"content":"\r4.2 划分选择决策树学习的关键是如何选择最优划分属性，也就是节点purity要高。 4.2.1 信息增益information entropy 信息熵 是度量样本集合纯度最常用的指标。 $$Ent(D)=-\\sum^{|y|}_{k=1} p_k {log}_2 p_k$$ 其中，约定p=0，plog2p=0；ent值越小，d纯度越高。（结合2为底的对数函数图像去看就很好理解） 条件熵（Y的信息熵关于概率分布X的期望，条件熵也就是Y的信息熵）：在已知X后Y的不确定性 $$H(Y|X) = \\sum_x p(x) H(Y|X=x)$$ 解释：从单属性（特征）a（色泽）的角度来看，假设其可能取值为{a1, a2, a3, …, a^V}，其中，a1 \u003c 30，a2 \u003e 30。D^v表示属性a取到这些值的集合，因此D^1表示色泽小于30的样本集合，D^2表示色泽大于30的样本集合。 |D^v| / D 表示当前属样本集合（即a1或a2）占总样本（a1+…+av）的占比，那么在已知属性a的取值后，样本集合D的条件熵为 $$\\sum^V_{v=1} \\frac{|D^v|}{|D|} Ent(D^v)$$ 信息增益 $$Gain(D,a)=Ent(D)-\\sum^V_{v=1}\\frac{|D|^v}{|D|}Ent(D^v)$$ 信息增益越大，意味着属性a来进行划分所获得的纯度提升越大。 在已知属性（特征）a的取值后y的不确定性减少的量（差值），即纯度的提升。 通常前者值 $Ent(D)$（信息熵）大，后者值 $\\sum^V_{v=1}\\frac{|D|^v}{|D|}Ent(D^v)$（条件熵）小。 4.2.2 增益率然后，信息增益准则对取值数目较多的属性有所偏好，为减少这种偏好带来的不利影响，引入了增益率 gain ratio来选择最优划分属性 $$Gain \\ ratio(D,a)=\\frac{Gain(D,a)}{IV(a)}$$ 其中 $$IV(a)=\\sum^V_{v=1} \\frac{|D^v|}{D} {log}_2 \\frac{|D^v|}{|D|}$$ IV(a)称为属性a的固有值 intrinsic value。属性a可能取值数目越多（V越大），则IV(a)的值通常越大。 要注意的是和信息增益准则相反，增益率准则对可取值数目较少的属性有偏好。所以它不能直接用来进行属性的划分，而是使用了一个启发式，先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的。 4.2.3 基尼指数 Gini$$Gini(D,a) = \\sum^{|y|}{k=1} \\sum{k^{’} \\neq{k}} p_k p_{k^{’}}$$ $$=1-\\sum^{|y|}_{k=1} p^2_k$$ 直观来说，Gini(D) 反映了数据集D中随机抽取两个两本，其类别标记不一致的概率。因此，Gini越小，碰到异类的概率就小，数据集D的纯度越高。 属性a的基尼指数定义为 $$Gini_index(D,a) = \\sum^V_{v=1} \\frac{|D^v|}{|D|} Gini(D^v)$$ 因此，在候选属性集合A中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即 $a_* = arg_{a \\in A} min Gini_index(D,a)$ ","date":"2021-10-11","objectID":"/pumpkin-book-notes03/:1:2","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 03","uri":"/pumpkin-book-notes03/#421-信息增益"},{"categories":["machine learning"],"content":"\r4.2 划分选择决策树学习的关键是如何选择最优划分属性，也就是节点purity要高。 4.2.1 信息增益information entropy 信息熵 是度量样本集合纯度最常用的指标。 $$Ent(D)=-\\sum^{|y|}_{k=1} p_k {log}_2 p_k$$ 其中，约定p=0，plog2p=0；ent值越小，d纯度越高。（结合2为底的对数函数图像去看就很好理解） 条件熵（Y的信息熵关于概率分布X的期望，条件熵也就是Y的信息熵）：在已知X后Y的不确定性 $$H(Y|X) = \\sum_x p(x) H(Y|X=x)$$ 解释：从单属性（特征）a（色泽）的角度来看，假设其可能取值为{a1, a2, a3, …, a^V}，其中，a1 \u003c 30，a2 \u003e 30。D^v表示属性a取到这些值的集合，因此D^1表示色泽小于30的样本集合，D^2表示色泽大于30的样本集合。 |D^v| / D 表示当前属样本集合（即a1或a2）占总样本（a1+…+av）的占比，那么在已知属性a的取值后，样本集合D的条件熵为 $$\\sum^V_{v=1} \\frac{|D^v|}{|D|} Ent(D^v)$$ 信息增益 $$Gain(D,a)=Ent(D)-\\sum^V_{v=1}\\frac{|D|^v}{|D|}Ent(D^v)$$ 信息增益越大，意味着属性a来进行划分所获得的纯度提升越大。 在已知属性（特征）a的取值后y的不确定性减少的量（差值），即纯度的提升。 通常前者值 $Ent(D)$（信息熵）大，后者值 $\\sum^V_{v=1}\\frac{|D|^v}{|D|}Ent(D^v)$（条件熵）小。 4.2.2 增益率然后，信息增益准则对取值数目较多的属性有所偏好，为减少这种偏好带来的不利影响，引入了增益率 gain ratio来选择最优划分属性 $$Gain \\ ratio(D,a)=\\frac{Gain(D,a)}{IV(a)}$$ 其中 $$IV(a)=\\sum^V_{v=1} \\frac{|D^v|}{D} {log}_2 \\frac{|D^v|}{|D|}$$ IV(a)称为属性a的固有值 intrinsic value。属性a可能取值数目越多（V越大），则IV(a)的值通常越大。 要注意的是和信息增益准则相反，增益率准则对可取值数目较少的属性有偏好。所以它不能直接用来进行属性的划分，而是使用了一个启发式，先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的。 4.2.3 基尼指数 Gini$$Gini(D,a) = \\sum^{|y|}{k=1} \\sum{k^{’} \\neq{k}} p_k p_{k^{’}}$$ $$=1-\\sum^{|y|}_{k=1} p^2_k$$ 直观来说，Gini(D) 反映了数据集D中随机抽取两个两本，其类别标记不一致的概率。因此，Gini越小，碰到异类的概率就小，数据集D的纯度越高。 属性a的基尼指数定义为 $$Gini_index(D,a) = \\sum^V_{v=1} \\frac{|D^v|}{|D|} Gini(D^v)$$ 因此，在候选属性集合A中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即 $a_* = arg_{a \\in A} min Gini_index(D,a)$ ","date":"2021-10-11","objectID":"/pumpkin-book-notes03/:1:2","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 03","uri":"/pumpkin-book-notes03/#422-增益率"},{"categories":["machine learning"],"content":"\r4.2 划分选择决策树学习的关键是如何选择最优划分属性，也就是节点purity要高。 4.2.1 信息增益information entropy 信息熵 是度量样本集合纯度最常用的指标。 $$Ent(D)=-\\sum^{|y|}_{k=1} p_k {log}_2 p_k$$ 其中，约定p=0，plog2p=0；ent值越小，d纯度越高。（结合2为底的对数函数图像去看就很好理解） 条件熵（Y的信息熵关于概率分布X的期望，条件熵也就是Y的信息熵）：在已知X后Y的不确定性 $$H(Y|X) = \\sum_x p(x) H(Y|X=x)$$ 解释：从单属性（特征）a（色泽）的角度来看，假设其可能取值为{a1, a2, a3, …, a^V}，其中，a1 \u003c 30，a2 \u003e 30。D^v表示属性a取到这些值的集合，因此D^1表示色泽小于30的样本集合，D^2表示色泽大于30的样本集合。 |D^v| / D 表示当前属样本集合（即a1或a2）占总样本（a1+…+av）的占比，那么在已知属性a的取值后，样本集合D的条件熵为 $$\\sum^V_{v=1} \\frac{|D^v|}{|D|} Ent(D^v)$$ 信息增益 $$Gain(D,a)=Ent(D)-\\sum^V_{v=1}\\frac{|D|^v}{|D|}Ent(D^v)$$ 信息增益越大，意味着属性a来进行划分所获得的纯度提升越大。 在已知属性（特征）a的取值后y的不确定性减少的量（差值），即纯度的提升。 通常前者值 $Ent(D)$（信息熵）大，后者值 $\\sum^V_{v=1}\\frac{|D|^v}{|D|}Ent(D^v)$（条件熵）小。 4.2.2 增益率然后，信息增益准则对取值数目较多的属性有所偏好，为减少这种偏好带来的不利影响，引入了增益率 gain ratio来选择最优划分属性 $$Gain \\ ratio(D,a)=\\frac{Gain(D,a)}{IV(a)}$$ 其中 $$IV(a)=\\sum^V_{v=1} \\frac{|D^v|}{D} {log}_2 \\frac{|D^v|}{|D|}$$ IV(a)称为属性a的固有值 intrinsic value。属性a可能取值数目越多（V越大），则IV(a)的值通常越大。 要注意的是和信息增益准则相反，增益率准则对可取值数目较少的属性有偏好。所以它不能直接用来进行属性的划分，而是使用了一个启发式，先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的。 4.2.3 基尼指数 Gini$$Gini(D,a) = \\sum^{|y|}{k=1} \\sum{k^{’} \\neq{k}} p_k p_{k^{’}}$$ $$=1-\\sum^{|y|}_{k=1} p^2_k$$ 直观来说，Gini(D) 反映了数据集D中随机抽取两个两本，其类别标记不一致的概率。因此，Gini越小，碰到异类的概率就小，数据集D的纯度越高。 属性a的基尼指数定义为 $$Gini_index(D,a) = \\sum^V_{v=1} \\frac{|D^v|}{|D|} Gini(D^v)$$ 因此，在候选属性集合A中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即 $a_* = arg_{a \\in A} min Gini_index(D,a)$ ","date":"2021-10-11","objectID":"/pumpkin-book-notes03/:1:2","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 03","uri":"/pumpkin-book-notes03/#423-基尼指数-gini"},{"categories":["machine learning"],"content":"\r4.3 剪枝处理 pruning剪枝是决策树算法解决过拟合的手段，节点划分过程有时造成分支过多，以至于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合。因此，可以主动去掉一些分支来降低过拟合的风险。 基本策略： 预剪枝（prepruning），每个节点在划分前先进性评估，若当前节点的划分不能提升决策树泛化能力，则不会把当前节点标记为叶节点。 后剪枝（post-pruning），先生成完整的决策树，然后自底向上地对非叶节点进行考察，若该节点对应子树能够提升泛化性，则将该子树替换为叶节点。 如何判断泛化性提升？先预留部分数据作验证集。 4.3.1 预剪枝基于信息增益准则，选取某属性对训练数据集进行划分，产生该属性种类n个分支。若不进行划分，直接将该节点标为叶节点，那么验证集对该节点进行评估，得出一个验证集的精度 a%。通过预剪枝划分后，同样用验证集进行评估，得到一个精度b%。比较a、b，如果b精度大于a，那么就证明这个预剪枝是有效的，反之就是无效。这样做可以阻止展开。 好处是降低过拟合风险，减少训练时间和测试时间。缺点是，某些节点可能当前不具备泛化性的提升，但是醉着后续的划分，可能会导致性能的提升。预剪枝基于的是贪心的思想，禁止分支展开。 4.3.2 后剪枝生成完整的决策树，得到验证集精度为a%。选择某一节点分支剪除，然后进行验证，如果得到的精度b% 高于a%，那就会进行剪枝，如果精度没有明显的提升，就会保留。 ","date":"2021-10-11","objectID":"/pumpkin-book-notes03/:1:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 03","uri":"/pumpkin-book-notes03/#43-剪枝处理-pruning"},{"categories":["machine learning"],"content":"\r4.3 剪枝处理 pruning剪枝是决策树算法解决过拟合的手段，节点划分过程有时造成分支过多，以至于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合。因此，可以主动去掉一些分支来降低过拟合的风险。 基本策略： 预剪枝（prepruning），每个节点在划分前先进性评估，若当前节点的划分不能提升决策树泛化能力，则不会把当前节点标记为叶节点。 后剪枝（post-pruning），先生成完整的决策树，然后自底向上地对非叶节点进行考察，若该节点对应子树能够提升泛化性，则将该子树替换为叶节点。 如何判断泛化性提升？先预留部分数据作验证集。 4.3.1 预剪枝基于信息增益准则，选取某属性对训练数据集进行划分，产生该属性种类n个分支。若不进行划分，直接将该节点标为叶节点，那么验证集对该节点进行评估，得出一个验证集的精度 a%。通过预剪枝划分后，同样用验证集进行评估，得到一个精度b%。比较a、b，如果b精度大于a，那么就证明这个预剪枝是有效的，反之就是无效。这样做可以阻止展开。 好处是降低过拟合风险，减少训练时间和测试时间。缺点是，某些节点可能当前不具备泛化性的提升，但是醉着后续的划分，可能会导致性能的提升。预剪枝基于的是贪心的思想，禁止分支展开。 4.3.2 后剪枝生成完整的决策树，得到验证集精度为a%。选择某一节点分支剪除，然后进行验证，如果得到的精度b% 高于a%，那就会进行剪枝，如果精度没有明显的提升，就会保留。 ","date":"2021-10-11","objectID":"/pumpkin-book-notes03/:1:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 03","uri":"/pumpkin-book-notes03/#431-预剪枝"},{"categories":["machine learning"],"content":"\r4.3 剪枝处理 pruning剪枝是决策树算法解决过拟合的手段，节点划分过程有时造成分支过多，以至于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合。因此，可以主动去掉一些分支来降低过拟合的风险。 基本策略： 预剪枝（prepruning），每个节点在划分前先进性评估，若当前节点的划分不能提升决策树泛化能力，则不会把当前节点标记为叶节点。 后剪枝（post-pruning），先生成完整的决策树，然后自底向上地对非叶节点进行考察，若该节点对应子树能够提升泛化性，则将该子树替换为叶节点。 如何判断泛化性提升？先预留部分数据作验证集。 4.3.1 预剪枝基于信息增益准则，选取某属性对训练数据集进行划分，产生该属性种类n个分支。若不进行划分，直接将该节点标为叶节点，那么验证集对该节点进行评估，得出一个验证集的精度 a%。通过预剪枝划分后，同样用验证集进行评估，得到一个精度b%。比较a、b，如果b精度大于a，那么就证明这个预剪枝是有效的，反之就是无效。这样做可以阻止展开。 好处是降低过拟合风险，减少训练时间和测试时间。缺点是，某些节点可能当前不具备泛化性的提升，但是醉着后续的划分，可能会导致性能的提升。预剪枝基于的是贪心的思想，禁止分支展开。 4.3.2 后剪枝生成完整的决策树，得到验证集精度为a%。选择某一节点分支剪除，然后进行验证，如果得到的精度b% 高于a%，那就会进行剪枝，如果精度没有明显的提升，就会保留。 ","date":"2021-10-11","objectID":"/pumpkin-book-notes03/:1:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 03","uri":"/pumpkin-book-notes03/#432-后剪枝"},{"categories":["machine learning"],"content":"\r4.4 连续和缺失值\r4.4.1 连续值处理之前讨论的都是离散值。由于连续属性的可取值数不再有限，因此，不能直接根据连续属性的可取值来对节点进行划分。连续属性离散化可能使用，最简单的策略，二分法（bi-partition） 有点难，没看懂 4.4.2 缺失值处理（不完整样本）\r4.4.3 多变量决策树","date":"2021-10-11","objectID":"/pumpkin-book-notes03/:1:4","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 03","uri":"/pumpkin-book-notes03/#44-连续和缺失值"},{"categories":["machine learning"],"content":"\r4.4 连续和缺失值\r4.4.1 连续值处理之前讨论的都是离散值。由于连续属性的可取值数不再有限，因此，不能直接根据连续属性的可取值来对节点进行划分。连续属性离散化可能使用，最简单的策略，二分法（bi-partition） 有点难，没看懂 4.4.2 缺失值处理（不完整样本）\r4.4.3 多变量决策树","date":"2021-10-11","objectID":"/pumpkin-book-notes03/:1:4","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 03","uri":"/pumpkin-book-notes03/#441-连续值处理"},{"categories":["machine learning"],"content":"\r4.4 连续和缺失值\r4.4.1 连续值处理之前讨论的都是离散值。由于连续属性的可取值数不再有限，因此，不能直接根据连续属性的可取值来对节点进行划分。连续属性离散化可能使用，最简单的策略，二分法（bi-partition） 有点难，没看懂 4.4.2 缺失值处理（不完整样本）\r4.4.3 多变量决策树","date":"2021-10-11","objectID":"/pumpkin-book-notes03/:1:4","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 03","uri":"/pumpkin-book-notes03/#442-缺失值处理不完整样本"},{"categories":["machine learning"],"content":"\r4.4 连续和缺失值\r4.4.1 连续值处理之前讨论的都是离散值。由于连续属性的可取值数不再有限，因此，不能直接根据连续属性的可取值来对节点进行划分。连续属性离散化可能使用，最简单的策略，二分法（bi-partition） 有点难，没看懂 4.4.2 缺失值处理（不完整样本）\r4.4.3 多变量决策树","date":"2021-10-11","objectID":"/pumpkin-book-notes03/:1:4","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 03","uri":"/pumpkin-book-notes03/#443-多变量决策树"},{"categories":["machine learning"],"content":"\r3 线性模型","date":"2021-10-06","objectID":"/pumpkin-book-notes02/:1:0","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 02","uri":"/pumpkin-book-notes02/#3-线性模型"},{"categories":["machine learning"],"content":"\r3.1 基本形式线性模型形式简单易于建模，一些非线性模型也可在此基础上引入层级结构或高维映射得到。 $$f(x)= \\omega^Tx+b$$\r","date":"2021-10-06","objectID":"/pumpkin-book-notes02/:1:1","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 02","uri":"/pumpkin-book-notes02/#31-基本形式"},{"categories":["machine learning"],"content":"\r3.2 Linear Regression线性回归通过给定数据晒图学得一个线性模型达到 $f(x)= \\omega^Tx+b$ 无限趋近于 y。 其中$\\omega$ 和 b 取值的关键在于衡量f(x)和y之间的差别，因此可以将军方误差最小化来达到目的，即 $$(\\omega^*, b^*) = \\underset{(\\omega, b)}{\\arg\\min} \\sum_{i=1}^{m} (f(x_i) - y_i)^2$$\r$$=arg \\, min_(\\omega,b) \\sum^m_{i=1} (y_i-\\omega x_i-b)^2$$\r均方误差的几何意义是欧氏距离，其求解方式是最小二乘法，找到一条直线使所有样本到该直线上的欧氏距离之和最小。 线性回归模型的最小二乘“参数估计”是求解$\\omega$和b是均方误差 $E_{(\\omega,b)}=\\sum^m_{i=1}(y_i-\\omega x_i-b)^2$ 最小化的过程。E是关于$\\omega$和b的凸函数，当导数为0时，得到最优解。 注：凸函数，对区间[a, b]上定义的函数f，若它对区间中任意两点x1，x2均有 $f(\\frac{x_1+x_2}{2} \u003c= \\frac{f(x_1)+f(x_2)}{2})$，则称f为区间[a,b]上的凸函数。 U醒曲线的函数如f(x)=x^2，通常为凸函数。 实数集上的函数，可通过求二阶导来判别：二阶导在区间上为非负就是凸函数；若其恒大于0，则称为严格凸函数。 3.2.1 一元线性回归 3.5 对 $\\omega$求导: $$\\frac{\\partial E_{(\\omega, b)}}{\\partial \\omega}=2 \\Bigg(\\omega \\sum^m_{i=1}x^2_i - \\sum^m_{i=1}(y_i - b) x_ix \\Bigg)$$\r推导： 已知 $E_{(\\omega,b)}=\\sum^m_{i=1}(y_i-\\omega x_i-b)^2$，所以 $$\\frac{\\partial E_{(\\omega, b)}}{\\partial \\omega}= \\frac{\\partial}{\\partial \\omega} \\Bigg[ \\sum^m_{i=1} (y_i - \\omega x_i - b)^2 \\Bigg]$$\r$$= \\sum^m_{i=1} \\frac{\\partial}{\\partial \\omega} \\Bigg[ (y_i - \\omega x_i - b)^2 \\Bigg] $$\r$$= \\sum^m_{i=1} [2 (y_i - \\omega x_i - b) (-x_i)]$$\r$$= \\sum^m_{i=1} [2 (\\omega x_i^2 - y_ix_i +bx_i)]$$\r$$= 2 \\Bigg( \\omega \\sum^m_{i=1} x_i^2 - \\sum^m_{i=1} y_ix_i + b \\sum^m_{i=1} x_i \\Bigg)$$\r$$= 2 \\Bigg( \\omega \\sum^m_{i=1} x_i^2 - \\sum^m_{i=1} (y_i - b)x_i \\Bigg)$$\r3.6 对b求导 $$\\frac{\\partial E_{(\\omega, b)}}{\\partial b} = 2 \\Bigg( mb - \\sum^m_{i=1} (y_i - \\omega x_i) \\Bigg)$$\r推导： 已知 $E_{(\\omega,b)}=\\sum^m_{i=1}(y_i-\\omega x_i-b)^2$，所以 $$\\frac{\\partial E_{(\\omega, bn)}}{\\partial b} = \\frac{\\partial}{\\partial b} \\Bigg[ \\sum^m_{i=1} (y_i - \\omega x_i - b)^2 \\Bigg]$$\r$$= \\sum^m_{i=1} \\frac{\\partial}{\\partial b} \\Bigg[ (y_i - \\omega x_i - b)^2 \\Bigg]$$\r$$= \\sum^m_{i=1} [2 (y_i - \\omega x_i - b)(-1)] $$\r$$= \\sum^m_{i=1} [2(b - y_i + \\omega x_i)]$$\r$$= 2 \\Bigg[ \\sum^m_{i=1} b - \\sum^m_{i=1} y_i + \\sum^m_{i=1} \\omega x_i \\Bigg]$$\r$$= 2 \\Bigg( mb - \\sum^m_{i=1}(y_i - \\omega x_i) \\Bigg)$$\r当3.5、3.6为0得到 $\\omega$ 和 b 最优解的闭式解（closed-form） 3.7 $$\\omega = \\frac{\\sum^m_{i=1} y_i (x_i - \\bar{x})}{\\sum^m_{i=1} x^2_i - \\frac{1}{m} (\\sum^m_{i=1} x_i)^2}$$\r3.8 $$b = \\frac{1}{m} \\sum^m_{i=1} (y_i - \\omega x_i)$$\r其中 $\\bar{x} = \\frac{1}{m} \\sum^m_{i=1} x_i$ 为 x 的均值。 3.7推导，令3.5等于0 $$0 = \\omega \\sum^m_{i=1} x^2_i - \\sum^m_{i=1} (y_i - b) x_i$$\r$$\\omega \\sum^m_{i=1} x^2_i = \\sum^m_{i=1} y_i x_i - \\sum^m_{i=1} b x_i$$\r由于令3.6等于0可得 $b = \\frac{1}{m} \\sum^m_{i=1} (y_i - \\omega x_i)$，又因为 $\\frac{1}{m} \\sum^m_{i=1} y_i = \\bar{y}$ ，$\\frac{1}{m} \\sum^m_{i=1} x_i = \\bar{x}$, 则 $b = \\bar{y} - \\omega \\bar{x}$，代入上式可得： $$\\omega \\sum^m_{i=1} x^2_i = \\sum^m_{i=1} y_i x_i - \\sum^m_{i=1} (\\bar{y} - \\omega \\bar{x}) x_i$$\r$$\\omega \\sum^m_{i=1} x^2_i = \\sum^m_{i=1} y_i x_i - \\bar{y} \\sum^m_{i=1} x_i + \\omega \\bar{x} \\sum^m_{i=1} x_i$$\r$$\\omega (\\sum^m_{i=1} x^2_i - \\bar{x} \\sum^m_{i=1} x_i) = \\sum^m_{i=1} y_i x_i - \\bar{y} \\sum^m_{i=1} x_i$$\r$$\\omega = \\frac{\\sum^m_{i=1} y_i x_i - \\bar{y} \\sum^m_{i=1} x_i}{\\sum^m_{i=1} x^2_i - \\bar{x} \\sum^m_{i=1} x_i}$$\r由于 $$\\bar{y} \\sum^m_{i=1} x_i = \\frac{1}{m} \\sum^m_{i=1}y_i \\sum^m_{i=1} x_i = \\bar{x} \\sum^m_{i=1} y_i$$\r$$\\bar{x} \\sum^m_{i=1} x_i = \\frac{1}{m} x_i \\sum^m_{i=1} x_1 = \\frac{1}{m}(\\sum^m_{i=1}x_i)^2$$\r代入上式即可得到公式3.7 $$\\omega = \\frac{\\sum^m_{i=1} y_i x_i - \\bar{y} \\sum^m_{i=1} x_i}{\\sum^m_{i=1} x^2_i - \\bar{x} \\sum^m_{i=1} x_i}$$\r$$= \\frac{\\sum^m_{i=1} y_i x_i - \\bar{x} \\sum^m_{i=1} y_i}{\\sum^m_{i=1} x^2_i - \\frac{1}{m}(\\sum^m_{i=1}x_i)^2}$$\r3.7 $$= \\frac{\\sum^m_{i=1} y_i (x_i-\\bar{x})}{\\sum^m_{i=1} x^2_i - \\frac{1}{m}(\\sum^m_{i=1}x_i)^2}$$\r上述的求和运算只能通过python的循环来实现，如果将上式向量化，转换成矩阵运算，那就可以通过 Numpy来实现。向量化： 将 $\\frac{1}{m}(\\sum^m_{i=1} x_i)^2 = \\bar{x} \\sum^m_{i=1} x_i$ 代入分母可得： $$\\omega = \\frac{\\sum^m_{i=1} y_i (x_i - \\bar{x})}{\\sum^m_{i=1} x^2_i - \\bar{x} \\sum^m_{i=1} x_i}$$\r$$=\\frac{\\sum^m_{i=1} (y_ix_i - y_i\\bar{x})}{\\sum^m_{i=1} (x^2_i - \\bar{x} x_i)}$$\r又因为 $$\\bar{y} \\sum^m_{i=1} x_i = \\bar{x} \\sum^m_{i=1}y_i = \\sum^m_{i=1} \\bar{y}x_i = \\sum^m_{i=1} \\bar{x","date":"2021-10-06","objectID":"/pumpkin-book-notes02/:1:2","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 02","uri":"/pumpkin-book-notes02/#32-linear-regression"},{"categories":["machine learning"],"content":"\r3.2 Linear Regression线性回归通过给定数据晒图学得一个线性模型达到 $f(x)= \\omega^Tx+b$ 无限趋近于 y。 其中$\\omega$ 和 b 取值的关键在于衡量f(x)和y之间的差别，因此可以将军方误差最小化来达到目的，即 $$(\\omega^*, b^*) = \\underset{(\\omega, b)}{\\arg\\min} \\sum_{i=1}^{m} (f(x_i) - y_i)^2$$\r$$=arg \\, min_(\\omega,b) \\sum^m_{i=1} (y_i-\\omega x_i-b)^2$$\r均方误差的几何意义是欧氏距离，其求解方式是最小二乘法，找到一条直线使所有样本到该直线上的欧氏距离之和最小。 线性回归模型的最小二乘“参数估计”是求解$\\omega$和b是均方误差 $E_{(\\omega,b)}=\\sum^m_{i=1}(y_i-\\omega x_i-b)^2$ 最小化的过程。E是关于$\\omega$和b的凸函数，当导数为0时，得到最优解。 注：凸函数，对区间[a, b]上定义的函数f，若它对区间中任意两点x1，x2均有 $f(\\frac{x_1+x_2}{2} \u003c= \\frac{f(x_1)+f(x_2)}{2})$，则称f为区间[a,b]上的凸函数。 U醒曲线的函数如f(x)=x^2，通常为凸函数。 实数集上的函数，可通过求二阶导来判别：二阶导在区间上为非负就是凸函数；若其恒大于0，则称为严格凸函数。 3.2.1 一元线性回归 3.5 对 $\\omega$求导: $$\\frac{\\partial E_{(\\omega, b)}}{\\partial \\omega}=2 \\Bigg(\\omega \\sum^m_{i=1}x^2_i - \\sum^m_{i=1}(y_i - b) x_ix \\Bigg)$$\r推导： 已知 $E_{(\\omega,b)}=\\sum^m_{i=1}(y_i-\\omega x_i-b)^2$，所以 $$\\frac{\\partial E_{(\\omega, b)}}{\\partial \\omega}= \\frac{\\partial}{\\partial \\omega} \\Bigg[ \\sum^m_{i=1} (y_i - \\omega x_i - b)^2 \\Bigg]$$\r$$= \\sum^m_{i=1} \\frac{\\partial}{\\partial \\omega} \\Bigg[ (y_i - \\omega x_i - b)^2 \\Bigg] $$\r$$= \\sum^m_{i=1} [2 (y_i - \\omega x_i - b) (-x_i)]$$\r$$= \\sum^m_{i=1} [2 (\\omega x_i^2 - y_ix_i +bx_i)]$$\r$$= 2 \\Bigg( \\omega \\sum^m_{i=1} x_i^2 - \\sum^m_{i=1} y_ix_i + b \\sum^m_{i=1} x_i \\Bigg)$$\r$$= 2 \\Bigg( \\omega \\sum^m_{i=1} x_i^2 - \\sum^m_{i=1} (y_i - b)x_i \\Bigg)$$\r3.6 对b求导 $$\\frac{\\partial E_{(\\omega, b)}}{\\partial b} = 2 \\Bigg( mb - \\sum^m_{i=1} (y_i - \\omega x_i) \\Bigg)$$\r推导： 已知 $E_{(\\omega,b)}=\\sum^m_{i=1}(y_i-\\omega x_i-b)^2$，所以 $$\\frac{\\partial E_{(\\omega, bn)}}{\\partial b} = \\frac{\\partial}{\\partial b} \\Bigg[ \\sum^m_{i=1} (y_i - \\omega x_i - b)^2 \\Bigg]$$\r$$= \\sum^m_{i=1} \\frac{\\partial}{\\partial b} \\Bigg[ (y_i - \\omega x_i - b)^2 \\Bigg]$$\r$$= \\sum^m_{i=1} [2 (y_i - \\omega x_i - b)(-1)] $$\r$$= \\sum^m_{i=1} [2(b - y_i + \\omega x_i)]$$\r$$= 2 \\Bigg[ \\sum^m_{i=1} b - \\sum^m_{i=1} y_i + \\sum^m_{i=1} \\omega x_i \\Bigg]$$\r$$= 2 \\Bigg( mb - \\sum^m_{i=1}(y_i - \\omega x_i) \\Bigg)$$\r当3.5、3.6为0得到 $\\omega$ 和 b 最优解的闭式解（closed-form） 3.7 $$\\omega = \\frac{\\sum^m_{i=1} y_i (x_i - \\bar{x})}{\\sum^m_{i=1} x^2_i - \\frac{1}{m} (\\sum^m_{i=1} x_i)^2}$$\r3.8 $$b = \\frac{1}{m} \\sum^m_{i=1} (y_i - \\omega x_i)$$\r其中 $\\bar{x} = \\frac{1}{m} \\sum^m_{i=1} x_i$ 为 x 的均值。 3.7推导，令3.5等于0 $$0 = \\omega \\sum^m_{i=1} x^2_i - \\sum^m_{i=1} (y_i - b) x_i$$\r$$\\omega \\sum^m_{i=1} x^2_i = \\sum^m_{i=1} y_i x_i - \\sum^m_{i=1} b x_i$$\r由于令3.6等于0可得 $b = \\frac{1}{m} \\sum^m_{i=1} (y_i - \\omega x_i)$，又因为 $\\frac{1}{m} \\sum^m_{i=1} y_i = \\bar{y}$ ，$\\frac{1}{m} \\sum^m_{i=1} x_i = \\bar{x}$, 则 $b = \\bar{y} - \\omega \\bar{x}$，代入上式可得： $$\\omega \\sum^m_{i=1} x^2_i = \\sum^m_{i=1} y_i x_i - \\sum^m_{i=1} (\\bar{y} - \\omega \\bar{x}) x_i$$\r$$\\omega \\sum^m_{i=1} x^2_i = \\sum^m_{i=1} y_i x_i - \\bar{y} \\sum^m_{i=1} x_i + \\omega \\bar{x} \\sum^m_{i=1} x_i$$\r$$\\omega (\\sum^m_{i=1} x^2_i - \\bar{x} \\sum^m_{i=1} x_i) = \\sum^m_{i=1} y_i x_i - \\bar{y} \\sum^m_{i=1} x_i$$\r$$\\omega = \\frac{\\sum^m_{i=1} y_i x_i - \\bar{y} \\sum^m_{i=1} x_i}{\\sum^m_{i=1} x^2_i - \\bar{x} \\sum^m_{i=1} x_i}$$\r由于 $$\\bar{y} \\sum^m_{i=1} x_i = \\frac{1}{m} \\sum^m_{i=1}y_i \\sum^m_{i=1} x_i = \\bar{x} \\sum^m_{i=1} y_i$$\r$$\\bar{x} \\sum^m_{i=1} x_i = \\frac{1}{m} x_i \\sum^m_{i=1} x_1 = \\frac{1}{m}(\\sum^m_{i=1}x_i)^2$$\r代入上式即可得到公式3.7 $$\\omega = \\frac{\\sum^m_{i=1} y_i x_i - \\bar{y} \\sum^m_{i=1} x_i}{\\sum^m_{i=1} x^2_i - \\bar{x} \\sum^m_{i=1} x_i}$$\r$$= \\frac{\\sum^m_{i=1} y_i x_i - \\bar{x} \\sum^m_{i=1} y_i}{\\sum^m_{i=1} x^2_i - \\frac{1}{m}(\\sum^m_{i=1}x_i)^2}$$\r3.7 $$= \\frac{\\sum^m_{i=1} y_i (x_i-\\bar{x})}{\\sum^m_{i=1} x^2_i - \\frac{1}{m}(\\sum^m_{i=1}x_i)^2}$$\r上述的求和运算只能通过python的循环来实现，如果将上式向量化，转换成矩阵运算，那就可以通过 Numpy来实现。向量化： 将 $\\frac{1}{m}(\\sum^m_{i=1} x_i)^2 = \\bar{x} \\sum^m_{i=1} x_i$ 代入分母可得： $$\\omega = \\frac{\\sum^m_{i=1} y_i (x_i - \\bar{x})}{\\sum^m_{i=1} x^2_i - \\bar{x} \\sum^m_{i=1} x_i}$$\r$$=\\frac{\\sum^m_{i=1} (y_ix_i - y_i\\bar{x})}{\\sum^m_{i=1} (x^2_i - \\bar{x} x_i)}$$\r又因为 $$\\bar{y} \\sum^m_{i=1} x_i = \\bar{x} \\sum^m_{i=1}y_i = \\sum^m_{i=1} \\bar{y}x_i = \\sum^m_{i=1} \\bar{x","date":"2021-10-06","objectID":"/pumpkin-book-notes02/:1:2","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 02","uri":"/pumpkin-book-notes02/#321--一元线性回归"},{"categories":["machine learning"],"content":"\r3.2 Linear Regression线性回归通过给定数据晒图学得一个线性模型达到 $f(x)= \\omega^Tx+b$ 无限趋近于 y。 其中$\\omega$ 和 b 取值的关键在于衡量f(x)和y之间的差别，因此可以将军方误差最小化来达到目的，即 $$(\\omega^*, b^*) = \\underset{(\\omega, b)}{\\arg\\min} \\sum_{i=1}^{m} (f(x_i) - y_i)^2$$\r$$=arg \\, min_(\\omega,b) \\sum^m_{i=1} (y_i-\\omega x_i-b)^2$$\r均方误差的几何意义是欧氏距离，其求解方式是最小二乘法，找到一条直线使所有样本到该直线上的欧氏距离之和最小。 线性回归模型的最小二乘“参数估计”是求解$\\omega$和b是均方误差 $E_{(\\omega,b)}=\\sum^m_{i=1}(y_i-\\omega x_i-b)^2$ 最小化的过程。E是关于$\\omega$和b的凸函数，当导数为0时，得到最优解。 注：凸函数，对区间[a, b]上定义的函数f，若它对区间中任意两点x1，x2均有 $f(\\frac{x_1+x_2}{2} \u003c= \\frac{f(x_1)+f(x_2)}{2})$，则称f为区间[a,b]上的凸函数。 U醒曲线的函数如f(x)=x^2，通常为凸函数。 实数集上的函数，可通过求二阶导来判别：二阶导在区间上为非负就是凸函数；若其恒大于0，则称为严格凸函数。 3.2.1 一元线性回归 3.5 对 $\\omega$求导: $$\\frac{\\partial E_{(\\omega, b)}}{\\partial \\omega}=2 \\Bigg(\\omega \\sum^m_{i=1}x^2_i - \\sum^m_{i=1}(y_i - b) x_ix \\Bigg)$$\r推导： 已知 $E_{(\\omega,b)}=\\sum^m_{i=1}(y_i-\\omega x_i-b)^2$，所以 $$\\frac{\\partial E_{(\\omega, b)}}{\\partial \\omega}= \\frac{\\partial}{\\partial \\omega} \\Bigg[ \\sum^m_{i=1} (y_i - \\omega x_i - b)^2 \\Bigg]$$\r$$= \\sum^m_{i=1} \\frac{\\partial}{\\partial \\omega} \\Bigg[ (y_i - \\omega x_i - b)^2 \\Bigg] $$\r$$= \\sum^m_{i=1} [2 (y_i - \\omega x_i - b) (-x_i)]$$\r$$= \\sum^m_{i=1} [2 (\\omega x_i^2 - y_ix_i +bx_i)]$$\r$$= 2 \\Bigg( \\omega \\sum^m_{i=1} x_i^2 - \\sum^m_{i=1} y_ix_i + b \\sum^m_{i=1} x_i \\Bigg)$$\r$$= 2 \\Bigg( \\omega \\sum^m_{i=1} x_i^2 - \\sum^m_{i=1} (y_i - b)x_i \\Bigg)$$\r3.6 对b求导 $$\\frac{\\partial E_{(\\omega, b)}}{\\partial b} = 2 \\Bigg( mb - \\sum^m_{i=1} (y_i - \\omega x_i) \\Bigg)$$\r推导： 已知 $E_{(\\omega,b)}=\\sum^m_{i=1}(y_i-\\omega x_i-b)^2$，所以 $$\\frac{\\partial E_{(\\omega, bn)}}{\\partial b} = \\frac{\\partial}{\\partial b} \\Bigg[ \\sum^m_{i=1} (y_i - \\omega x_i - b)^2 \\Bigg]$$\r$$= \\sum^m_{i=1} \\frac{\\partial}{\\partial b} \\Bigg[ (y_i - \\omega x_i - b)^2 \\Bigg]$$\r$$= \\sum^m_{i=1} [2 (y_i - \\omega x_i - b)(-1)] $$\r$$= \\sum^m_{i=1} [2(b - y_i + \\omega x_i)]$$\r$$= 2 \\Bigg[ \\sum^m_{i=1} b - \\sum^m_{i=1} y_i + \\sum^m_{i=1} \\omega x_i \\Bigg]$$\r$$= 2 \\Bigg( mb - \\sum^m_{i=1}(y_i - \\omega x_i) \\Bigg)$$\r当3.5、3.6为0得到 $\\omega$ 和 b 最优解的闭式解（closed-form） 3.7 $$\\omega = \\frac{\\sum^m_{i=1} y_i (x_i - \\bar{x})}{\\sum^m_{i=1} x^2_i - \\frac{1}{m} (\\sum^m_{i=1} x_i)^2}$$\r3.8 $$b = \\frac{1}{m} \\sum^m_{i=1} (y_i - \\omega x_i)$$\r其中 $\\bar{x} = \\frac{1}{m} \\sum^m_{i=1} x_i$ 为 x 的均值。 3.7推导，令3.5等于0 $$0 = \\omega \\sum^m_{i=1} x^2_i - \\sum^m_{i=1} (y_i - b) x_i$$\r$$\\omega \\sum^m_{i=1} x^2_i = \\sum^m_{i=1} y_i x_i - \\sum^m_{i=1} b x_i$$\r由于令3.6等于0可得 $b = \\frac{1}{m} \\sum^m_{i=1} (y_i - \\omega x_i)$，又因为 $\\frac{1}{m} \\sum^m_{i=1} y_i = \\bar{y}$ ，$\\frac{1}{m} \\sum^m_{i=1} x_i = \\bar{x}$, 则 $b = \\bar{y} - \\omega \\bar{x}$，代入上式可得： $$\\omega \\sum^m_{i=1} x^2_i = \\sum^m_{i=1} y_i x_i - \\sum^m_{i=1} (\\bar{y} - \\omega \\bar{x}) x_i$$\r$$\\omega \\sum^m_{i=1} x^2_i = \\sum^m_{i=1} y_i x_i - \\bar{y} \\sum^m_{i=1} x_i + \\omega \\bar{x} \\sum^m_{i=1} x_i$$\r$$\\omega (\\sum^m_{i=1} x^2_i - \\bar{x} \\sum^m_{i=1} x_i) = \\sum^m_{i=1} y_i x_i - \\bar{y} \\sum^m_{i=1} x_i$$\r$$\\omega = \\frac{\\sum^m_{i=1} y_i x_i - \\bar{y} \\sum^m_{i=1} x_i}{\\sum^m_{i=1} x^2_i - \\bar{x} \\sum^m_{i=1} x_i}$$\r由于 $$\\bar{y} \\sum^m_{i=1} x_i = \\frac{1}{m} \\sum^m_{i=1}y_i \\sum^m_{i=1} x_i = \\bar{x} \\sum^m_{i=1} y_i$$\r$$\\bar{x} \\sum^m_{i=1} x_i = \\frac{1}{m} x_i \\sum^m_{i=1} x_1 = \\frac{1}{m}(\\sum^m_{i=1}x_i)^2$$\r代入上式即可得到公式3.7 $$\\omega = \\frac{\\sum^m_{i=1} y_i x_i - \\bar{y} \\sum^m_{i=1} x_i}{\\sum^m_{i=1} x^2_i - \\bar{x} \\sum^m_{i=1} x_i}$$\r$$= \\frac{\\sum^m_{i=1} y_i x_i - \\bar{x} \\sum^m_{i=1} y_i}{\\sum^m_{i=1} x^2_i - \\frac{1}{m}(\\sum^m_{i=1}x_i)^2}$$\r3.7 $$= \\frac{\\sum^m_{i=1} y_i (x_i-\\bar{x})}{\\sum^m_{i=1} x^2_i - \\frac{1}{m}(\\sum^m_{i=1}x_i)^2}$$\r上述的求和运算只能通过python的循环来实现，如果将上式向量化，转换成矩阵运算，那就可以通过 Numpy来实现。向量化： 将 $\\frac{1}{m}(\\sum^m_{i=1} x_i)^2 = \\bar{x} \\sum^m_{i=1} x_i$ 代入分母可得： $$\\omega = \\frac{\\sum^m_{i=1} y_i (x_i - \\bar{x})}{\\sum^m_{i=1} x^2_i - \\bar{x} \\sum^m_{i=1} x_i}$$\r$$=\\frac{\\sum^m_{i=1} (y_ix_i - y_i\\bar{x})}{\\sum^m_{i=1} (x^2_i - \\bar{x} x_i)}$$\r又因为 $$\\bar{y} \\sum^m_{i=1} x_i = \\bar{x} \\sum^m_{i=1}y_i = \\sum^m_{i=1} \\bar{y}x_i = \\sum^m_{i=1} \\bar{x","date":"2021-10-06","objectID":"/pumpkin-book-notes02/:1:2","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 02","uri":"/pumpkin-book-notes02/#322-多元线性回归-multivariate-linear-regression"},{"categories":["machine learning"],"content":"\r3.3 对数几率回归扩展阅读 对数几率回归就是逻辑回归，本质是分类算法。回归算法主要预测一些具体的数值。 分类算法，比如二分类，预测一个结果的概率。因此它的值是在0-1中，而线性回归值域是在实数域，就不能直接拿来用。因此在线性模型的基础上套用个映射函数来实现分类的目的。 对数几率回归就是套用了一个 $\\frac{1}{1+e^{-z}}$这样的函数。它是一个在0-1之间的s型的曲线，又叫做 Sigmoid函数。其中，z就是线性回归的f(x)。 能够将线性函数映射到0-1区间的函数有很多，为什么只用Sigmod函数？原因很多，最有说服力的是从最大熵的角度来解释。 极大似然估计 第一步：确定概率质量函数（概率密度函数） 已知离散型随机变量 $y \\in \\lbrace 0,1 \\rbrace$ 取值为1和0的概率分别建模为 $$p(y=1|x) = \\frac{1}{1+e^{\\omega^Tx+b}} = \\frac{e^{\\omega^T x +b}}{1+e^{\\omega^Tx+b}}$$\r$$p(y=0|x) = 1 - p(y=1|x) = \\frac{1}{1+e^{\\omega^Tx+b}}$$\r为了便于讨论，令 $\\beta = (\\omega; b) , , , \\hat{x} = (x;1)$，则上式可简写为： $$p(y=1|\\hat{x}; \\beta) = \\frac{e^{\\beta^T \\hat{x}}}{1+e^{\\beta^T \\hat{x}}} = p_1(\\hat{x}; \\beta)$$\r$$p(y=0|\\hat{x}; \\beta) = \\frac{1}{1+e^{\\beta^T \\hat{x}}} = p_0(\\hat{x}; \\beta)$$\r由以上概率取值可推得随机变量 $y \\in \\lbrace 0,1 \\rbrace$ 的概率质量函数为 3.26 $$p(y|\\hat{x};\\beta) = y p_1(\\hat{x}; \\beta) + (1-y) p_0(\\hat{x}; \\beta)$$ 或者为 $$p(y|\\hat{x};\\beta) = [p_1(\\hat{x}; \\beta)]^y [p_0(\\hat{x};\\beta)]^{1-y}$$ 理解，当y=0，第一个式子前项为0，直接求后项。第二个式子前项为1，也是直接求后项即可，相反也是。 第二步：写出似然函数 $$L(\\beta) = \\prod^m_{i=1} p (y_i|\\hat{x}_i;\\beta)$$ 对数似然函数为 $$l(\\beta) = ln L(\\beta) = \\sum^m_{i=1} ln p(y_i | \\hat{x}_i ; \\beta)$$ $$l(\\beta) = \\sum^m_{i=1}ln(y_ip_1(\\hat{x}_i;\\beta) + (1 - y_i) p_0 (\\hat{x}_i) ; \\beta)$$ 将 $p_1(\\hat{x}_i ; \\beta) = \\frac{e^{\\beta^T \\hat{x}_i}}{1+e^{\\beta^T \\hat{x}_i}}$，$p_0(\\hat{x}_i;\\beta) = \\frac{1}{1 + e^{\\beta^T \\hat{x}_i}}$ 代入上式可得 $$l(\\beta) = \\sum^m_{i=1} ln \\Bigg( \\frac{y_1 e^{\\beta^T \\hat{x}_1}}{1+e^{\\beta^T \\hat{x}_i}} + \\frac{1-y_i}{1+e^{\\beta^T \\hat{x}_i}} \\Bigg)$$ $$= \\sum^m_{i=1} ln \\Bigg( \\frac{y_i e^{\\beta^T \\hat{x}_i} + 1 - y_i}{1+e^{\\beta^T \\hat{x}_i}} \\Bigg)$$ 根据ln函数规则，$ln \\frac{a}{b} = lna - lnb$ $$= \\sum^m_{i=1} (ln(y_i e^{\\beta^T \\hat{x}_1} + 1 - y_i) - ln (1 + e^{\\beta^T \\hat{x}_i}))$$ 由于$y_i \\in \\lbrace 0,1 \\rbrace$，则 $$ l(\\beta) = \\begin{cases} \\sum^m_{i=1}(-ln(1+e^{\\beta^T \\hat{x}i})) , \\ \\ \\ \\ \\ y_i = 0 \\ \\sum^m{i=1}(\\beta^T \\hat{x}_i - ln(1 + e^{\\beta^T \\hat{x}_i})) , \\ \\ \\ \\ \\ y_i = 1 \\end{cases} $$ 两式综合可得 $$l(\\beta) = \\sum^m_{i=1} \\bigg( y_i \\beta^T \\hat{x}_i - ln(1+e^{\\beta^T \\hat{x}_i}) \\bigg)$$ 为什么可以写成这样？是因为当y=0时，$y_i \\beta^T \\hat{x}_i = ln(y_i e^{\\beta^T \\hat{x}_1} + 1 - y_i) = 0$。y=1时，$y_i \\beta^T \\hat{x}_i = ln(y_i e^{\\beta^T \\hat{x}_1} + 1 - y_i) = \\beta^T \\hat{x}_i$。 损失函数通常是以最小化为优化目标，因此可以将最大化 $l(\\beta)$ 等价转换为最小化 $l(\\beta)$的相反数 $-l(\\beta)$，即得到公式3.27。 3.27 $$l(\\beta) = \\sum^m_{i=1} \\bigg( - y_i \\beta^T \\hat{x}_i - ln(1+e^{\\beta^T \\hat{x}_i}) \\bigg)$$ 拓展：信息论以概率论、随机过程为基本研究工具，研究广义通信系统的整个过程。常见应用：无损数据压缩（zip）、有损数据压缩（MP3、JPEG） 自信息：随机变量x，它有一个概率质量函数p(x)，它的自信息就是负的log底数为b的函数。 $$I(X) = -log_b p(x)$$ 当b=2时单位为bit，当b=e时单位为nat。（为e的时候就是ln） 信息熵（自信息的期望）：度量随机变量X的不确定性，信息熵越大越不确定。 $H(X) = E[I(X)] = - \\sum_x p(x) log_b p(x)$ （以离散型为例） 计算信息熵时约定：若p(x)=0，则p(x)log_b p(x)=0 （具体在决策树讲解） 相对熵 （KL散度）：度量两个分布的差异，其典型使用场景是用来度量理想分布 p(x) 和模拟分布 q(x) 之间的差异。 $$D_{KL}(p||q) = \\sum_x p(x) log_b (\\frac{p(x)}{q(x)})$$ $$= \\sum_x p(x) (log_b p(x) - log_bq (x))$$ $$= \\sum_x p(x) log_b p(x) - \\sum_x p(x) log_b q(x)$$ 其中，$\\sum_x p(x) log_b p(x)$ 没加负号的理想分布的信息熵，$- \\sum_x p(x) log_b q(x)$ 称为交叉熵，$\\sum_x$ 就是遍历x所有可能的取值，。 如何理交叉熵和信息熵？ 从机器学习三要素中\"策略\"的角度来说，与理想分布最接近的模拟分布即为最优分布，因此可以通过最小化相对熵这个策略来求出最优分布，原因是当相对熵达到最小时，q(x)最接近p(x)。由于理想分布p(x)是未知但固定分布（频率学派角度），所以 $\\sum_x p(x) log_b p(x)$为常量，那么最小化相对熵就等价于最小化交叉熵 $- \\sum_x p(x) log_b q(x)$。 以对数几率回归为例，对单个样本yi来说，理想分布 $$ p(y_i) = \\begin{cases} p(1)=1, p(0) = 0, \\ \\ \\ \\ y_i=1 \\ p(1)=0, p(0)=1, \\ \\ \\ \\ y_i=0 \\end{cases} $$ 它现在的模拟分布是 $$ q(y_1) = \\begin{cases} \\frac{e^{\\beta^T \\hat{x}}}{1+e^{\\beta^T \\hat{x}}} = p_1(\\hat{x}; \\beta), \\ \\ \\ \\ y_i=1 \\ \\frac{1}{1+e^{\\beta^T \\hat{x}}} = p_0(\\hat{x}; \\beta), \\ \\ \\ \\ y_i =0 \\end{cases} $$ 那么单个样本yi的交叉熵为 $$-\\sum_{y_i} p(y_i) log_b q(y_i)$$ $$-p(1) log_b p_1 (\\hat{x}; \\beta) - p(0) log_b p_0 (\\hat{x}; \\beta)$$ $$-y_i log_bp_1(\\hat{x}; \\beta) - (1 - y_i) log_b p_0 (\\hat{x}; \\beta)$$ 令 b=e $$-y_i ln p_1 (\\hat{x}; \\beta) - (1- y_i) ln p_0 (\\hat{x}; \\beta)$$ 全体训练样本的交叉熵 $$\\sum^m_{i=1} [-y_i ln p_1 (\\hat{x}; \\beta) - (1-y_i) ln p_0(\\hat{x}; \\bet","date":"2021-10-06","objectID":"/pumpkin-book-notes02/:1:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 02","uri":"/pumpkin-book-notes02/#33-对数几率回归"},{"categories":["machine learning"],"content":"\r3.3 对数几率回归扩展阅读 对数几率回归就是逻辑回归，本质是分类算法。回归算法主要预测一些具体的数值。 分类算法，比如二分类，预测一个结果的概率。因此它的值是在0-1中，而线性回归值域是在实数域，就不能直接拿来用。因此在线性模型的基础上套用个映射函数来实现分类的目的。 对数几率回归就是套用了一个 $\\frac{1}{1+e^{-z}}$这样的函数。它是一个在0-1之间的s型的曲线，又叫做 Sigmoid函数。其中，z就是线性回归的f(x)。 能够将线性函数映射到0-1区间的函数有很多，为什么只用Sigmod函数？原因很多，最有说服力的是从最大熵的角度来解释。 极大似然估计 第一步：确定概率质量函数（概率密度函数） 已知离散型随机变量 $y \\in \\lbrace 0,1 \\rbrace$ 取值为1和0的概率分别建模为 $$p(y=1|x) = \\frac{1}{1+e^{\\omega^Tx+b}} = \\frac{e^{\\omega^T x +b}}{1+e^{\\omega^Tx+b}}$$\r$$p(y=0|x) = 1 - p(y=1|x) = \\frac{1}{1+e^{\\omega^Tx+b}}$$\r为了便于讨论，令 $\\beta = (\\omega; b) , , , \\hat{x} = (x;1)$，则上式可简写为： $$p(y=1|\\hat{x}; \\beta) = \\frac{e^{\\beta^T \\hat{x}}}{1+e^{\\beta^T \\hat{x}}} = p_1(\\hat{x}; \\beta)$$\r$$p(y=0|\\hat{x}; \\beta) = \\frac{1}{1+e^{\\beta^T \\hat{x}}} = p_0(\\hat{x}; \\beta)$$\r由以上概率取值可推得随机变量 $y \\in \\lbrace 0,1 \\rbrace$ 的概率质量函数为 3.26 $$p(y|\\hat{x};\\beta) = y p_1(\\hat{x}; \\beta) + (1-y) p_0(\\hat{x}; \\beta)$$ 或者为 $$p(y|\\hat{x};\\beta) = [p_1(\\hat{x}; \\beta)]^y [p_0(\\hat{x};\\beta)]^{1-y}$$ 理解，当y=0，第一个式子前项为0，直接求后项。第二个式子前项为1，也是直接求后项即可，相反也是。 第二步：写出似然函数 $$L(\\beta) = \\prod^m_{i=1} p (y_i|\\hat{x}_i;\\beta)$$ 对数似然函数为 $$l(\\beta) = ln L(\\beta) = \\sum^m_{i=1} ln p(y_i | \\hat{x}_i ; \\beta)$$ $$l(\\beta) = \\sum^m_{i=1}ln(y_ip_1(\\hat{x}_i;\\beta) + (1 - y_i) p_0 (\\hat{x}_i) ; \\beta)$$ 将 $p_1(\\hat{x}_i ; \\beta) = \\frac{e^{\\beta^T \\hat{x}_i}}{1+e^{\\beta^T \\hat{x}_i}}$，$p_0(\\hat{x}_i;\\beta) = \\frac{1}{1 + e^{\\beta^T \\hat{x}_i}}$ 代入上式可得 $$l(\\beta) = \\sum^m_{i=1} ln \\Bigg( \\frac{y_1 e^{\\beta^T \\hat{x}_1}}{1+e^{\\beta^T \\hat{x}_i}} + \\frac{1-y_i}{1+e^{\\beta^T \\hat{x}_i}} \\Bigg)$$ $$= \\sum^m_{i=1} ln \\Bigg( \\frac{y_i e^{\\beta^T \\hat{x}_i} + 1 - y_i}{1+e^{\\beta^T \\hat{x}_i}} \\Bigg)$$ 根据ln函数规则，$ln \\frac{a}{b} = lna - lnb$ $$= \\sum^m_{i=1} (ln(y_i e^{\\beta^T \\hat{x}_1} + 1 - y_i) - ln (1 + e^{\\beta^T \\hat{x}_i}))$$ 由于$y_i \\in \\lbrace 0,1 \\rbrace$，则 $$ l(\\beta) = \\begin{cases} \\sum^m_{i=1}(-ln(1+e^{\\beta^T \\hat{x}i})) , \\ \\ \\ \\ \\ y_i = 0 \\ \\sum^m{i=1}(\\beta^T \\hat{x}_i - ln(1 + e^{\\beta^T \\hat{x}_i})) , \\ \\ \\ \\ \\ y_i = 1 \\end{cases} $$ 两式综合可得 $$l(\\beta) = \\sum^m_{i=1} \\bigg( y_i \\beta^T \\hat{x}_i - ln(1+e^{\\beta^T \\hat{x}_i}) \\bigg)$$ 为什么可以写成这样？是因为当y=0时，$y_i \\beta^T \\hat{x}_i = ln(y_i e^{\\beta^T \\hat{x}_1} + 1 - y_i) = 0$。y=1时，$y_i \\beta^T \\hat{x}_i = ln(y_i e^{\\beta^T \\hat{x}_1} + 1 - y_i) = \\beta^T \\hat{x}_i$。 损失函数通常是以最小化为优化目标，因此可以将最大化 $l(\\beta)$ 等价转换为最小化 $l(\\beta)$的相反数 $-l(\\beta)$，即得到公式3.27。 3.27 $$l(\\beta) = \\sum^m_{i=1} \\bigg( - y_i \\beta^T \\hat{x}_i - ln(1+e^{\\beta^T \\hat{x}_i}) \\bigg)$$ 拓展：信息论以概率论、随机过程为基本研究工具，研究广义通信系统的整个过程。常见应用：无损数据压缩（zip）、有损数据压缩（MP3、JPEG） 自信息：随机变量x，它有一个概率质量函数p(x)，它的自信息就是负的log底数为b的函数。 $$I(X) = -log_b p(x)$$ 当b=2时单位为bit，当b=e时单位为nat。（为e的时候就是ln） 信息熵（自信息的期望）：度量随机变量X的不确定性，信息熵越大越不确定。 $H(X) = E[I(X)] = - \\sum_x p(x) log_b p(x)$ （以离散型为例） 计算信息熵时约定：若p(x)=0，则p(x)log_b p(x)=0 （具体在决策树讲解） 相对熵 （KL散度）：度量两个分布的差异，其典型使用场景是用来度量理想分布 p(x) 和模拟分布 q(x) 之间的差异。 $$D_{KL}(p||q) = \\sum_x p(x) log_b (\\frac{p(x)}{q(x)})$$ $$= \\sum_x p(x) (log_b p(x) - log_bq (x))$$ $$= \\sum_x p(x) log_b p(x) - \\sum_x p(x) log_b q(x)$$ 其中，$\\sum_x p(x) log_b p(x)$ 没加负号的理想分布的信息熵，$- \\sum_x p(x) log_b q(x)$ 称为交叉熵，$\\sum_x$ 就是遍历x所有可能的取值，。 如何理交叉熵和信息熵？ 从机器学习三要素中\"策略\"的角度来说，与理想分布最接近的模拟分布即为最优分布，因此可以通过最小化相对熵这个策略来求出最优分布，原因是当相对熵达到最小时，q(x)最接近p(x)。由于理想分布p(x)是未知但固定分布（频率学派角度），所以 $\\sum_x p(x) log_b p(x)$为常量，那么最小化相对熵就等价于最小化交叉熵 $- \\sum_x p(x) log_b q(x)$。 以对数几率回归为例，对单个样本yi来说，理想分布 $$ p(y_i) = \\begin{cases} p(1)=1, p(0) = 0, \\ \\ \\ \\ y_i=1 \\ p(1)=0, p(0)=1, \\ \\ \\ \\ y_i=0 \\end{cases} $$ 它现在的模拟分布是 $$ q(y_1) = \\begin{cases} \\frac{e^{\\beta^T \\hat{x}}}{1+e^{\\beta^T \\hat{x}}} = p_1(\\hat{x}; \\beta), \\ \\ \\ \\ y_i=1 \\ \\frac{1}{1+e^{\\beta^T \\hat{x}}} = p_0(\\hat{x}; \\beta), \\ \\ \\ \\ y_i =0 \\end{cases} $$ 那么单个样本yi的交叉熵为 $$-\\sum_{y_i} p(y_i) log_b q(y_i)$$ $$-p(1) log_b p_1 (\\hat{x}; \\beta) - p(0) log_b p_0 (\\hat{x}; \\beta)$$ $$-y_i log_bp_1(\\hat{x}; \\beta) - (1 - y_i) log_b p_0 (\\hat{x}; \\beta)$$ 令 b=e $$-y_i ln p_1 (\\hat{x}; \\beta) - (1- y_i) ln p_0 (\\hat{x}; \\beta)$$ 全体训练样本的交叉熵 $$\\sum^m_{i=1} [-y_i ln p_1 (\\hat{x}; \\beta) - (1-y_i) ln p_0(\\hat{x}; \\bet","date":"2021-10-06","objectID":"/pumpkin-book-notes02/:1:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 02","uri":"/pumpkin-book-notes02/#拓展信息论"},{"categories":["machine learning"],"content":"\r3.4 二分类线性判别分析异类样本尽可能远，同类样本方差尽可能小。 在西瓜书定义中，假定了一个数据集 D={(xi, yi)}，其中，i是从1取到m，yi属于0到1。要注意的是Xi，大X的i和数据集中的i不是一个东西。X中i是0，1，X1表示所有y=1的[(xi,yi)]的集合。X0就是y=0的集合。 $||a||^2_2$，这种形式叫做二范数，求的是向量的模长。比如，一个向量 $a=(a_1,a_2)^T$，那它的二范数就是 $||a||_2 = \\sqrt{{a_1}^2 + {a_2}^2}$，上面再加个2就是平方 $||a||_2^2 = {a_1}^2 + {a_2}^2$。 ","date":"2021-10-06","objectID":"/pumpkin-book-notes02/:1:4","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 02","uri":"/pumpkin-book-notes02/#34-二分类线性判别分析"},{"categories":["machine learning"],"content":"\r1. 绪论","date":"2021-09-30","objectID":"/pumpkin-book-notes01/:1:0","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 01","uri":"/pumpkin-book-notes01/#1-绪论"},{"categories":["machine learning"],"content":"\r1.1 基本术语 data set instance / sample attribute / feature attribute value attribute space / sample space feature vector dimensionality learning / training training data training sample training set hypothesis: 模型对应了关于数据的某种潜在的规律 ground-truth: 潜在规律的本身 learner: 模型，给定数据和参数空间上的实例化 prediction label example, ($x_i, y_i$) label space classification, 分类、离散 regression, 连续 binary classification positive / negative class multi-class classification testing testing sample clustering, 将西瓜做聚类，即将训练集中的西瓜分成若干组 cluster, 每组称为一个簇 supervised learning, 分类、回归问题 unsupervised learning, 聚类 generalization, 泛化，模型适用于新样本的能力 distribution, 通常假设样本空间中全体样本服从一个未知分布 independent and identically distributed, 每个样本都是独立地从这个分布上采样获得的 ","date":"2021-09-30","objectID":"/pumpkin-book-notes01/:1:1","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 01","uri":"/pumpkin-book-notes01/#11-基本术语"},{"categories":["machine learning"],"content":"\r1.2 假设空间科学推理手段： induction, 从具体事实归结出一般性规律。 deduction, 从技术原理推演具体情况。 假设空间：存在一个所有可能的由输入空间到输出空间的映射所构成的集合，则假设空间就是该机喝的一个子集。 ","date":"2021-09-30","objectID":"/pumpkin-book-notes01/:1:2","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 01","uri":"/pumpkin-book-notes01/#12-假设空间"},{"categories":["machine learning"],"content":"\r1.3 归纳偏好归纳偏好是算法在学习过程中对某种类型假设的偏好，防止算法被假设空间中看似等效的假设过迷惑，而无法产生确定的结果。 ","date":"2021-09-30","objectID":"/pumpkin-book-notes01/:1:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 01","uri":"/pumpkin-book-notes01/#13-归纳偏好"},{"categories":["machine learning"],"content":"\r2. 模型评估和选择","date":"2021-09-30","objectID":"/pumpkin-book-notes01/:2:0","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 01","uri":"/pumpkin-book-notes01/#2-模型评估和选择"},{"categories":["machine learning"],"content":"\r2.1 经验误差与过拟合经验/训练误差，训练集上实际预测输出与样本的真实输出之间的差异。 过拟合，在训练样本表现过好，但是泛化性能下降。一般由于学习能力过于强大导致。 ","date":"2021-09-30","objectID":"/pumpkin-book-notes01/:2:1","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 01","uri":"/pumpkin-book-notes01/#21-经验误差与过拟合"},{"categories":["machine learning"],"content":"\r2.2 评估方法\r2.2.1 留出法 hold-out将数据集分为两个互斥的集合，一个训练集、一个测试集。两个集合要尽量保持数据分布的一致性。复杂度高。 2.2.2 交叉验证法 cross validation将数据集划分为k个大小相似的互斥子集，保证一致性。k-1个子集作为训练集，余下的并集作为测试集。 2.2.3 自助法 bootstrapping适用于数据集小、难以有效划分训练、测试集。改变了初始数据集的分布。在数据量足够时，不如前两个常用。 ","date":"2021-09-30","objectID":"/pumpkin-book-notes01/:2:2","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 01","uri":"/pumpkin-book-notes01/#22-评估方法"},{"categories":["machine learning"],"content":"\r2.2 评估方法\r2.2.1 留出法 hold-out将数据集分为两个互斥的集合，一个训练集、一个测试集。两个集合要尽量保持数据分布的一致性。复杂度高。 2.2.2 交叉验证法 cross validation将数据集划分为k个大小相似的互斥子集，保证一致性。k-1个子集作为训练集，余下的并集作为测试集。 2.2.3 自助法 bootstrapping适用于数据集小、难以有效划分训练、测试集。改变了初始数据集的分布。在数据量足够时，不如前两个常用。 ","date":"2021-09-30","objectID":"/pumpkin-book-notes01/:2:2","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 01","uri":"/pumpkin-book-notes01/#221-留出法-hold-out"},{"categories":["machine learning"],"content":"\r2.2 评估方法\r2.2.1 留出法 hold-out将数据集分为两个互斥的集合，一个训练集、一个测试集。两个集合要尽量保持数据分布的一致性。复杂度高。 2.2.2 交叉验证法 cross validation将数据集划分为k个大小相似的互斥子集，保证一致性。k-1个子集作为训练集，余下的并集作为测试集。 2.2.3 自助法 bootstrapping适用于数据集小、难以有效划分训练、测试集。改变了初始数据集的分布。在数据量足够时，不如前两个常用。 ","date":"2021-09-30","objectID":"/pumpkin-book-notes01/:2:2","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 01","uri":"/pumpkin-book-notes01/#222-交叉验证法-cross-validation"},{"categories":["machine learning"],"content":"\r2.2 评估方法\r2.2.1 留出法 hold-out将数据集分为两个互斥的集合，一个训练集、一个测试集。两个集合要尽量保持数据分布的一致性。复杂度高。 2.2.2 交叉验证法 cross validation将数据集划分为k个大小相似的互斥子集，保证一致性。k-1个子集作为训练集，余下的并集作为测试集。 2.2.3 自助法 bootstrapping适用于数据集小、难以有效划分训练、测试集。改变了初始数据集的分布。在数据量足够时，不如前两个常用。 ","date":"2021-09-30","objectID":"/pumpkin-book-notes01/:2:2","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 01","uri":"/pumpkin-book-notes01/#223-自助法-bootstrapping"},{"categories":["machine learning"],"content":"\r2.3 性能度量 performance measure衡量模型泛化能力的评价标准，比如回归任务常用的“均方误差”。 2.3.1 错误率和精度错误率与精度的关系为 $1-E(f;D)=acc(f;D)$ 2.3.2 查准率 precision、查全率 recall 与F1在类似信息检索的应用中，错误率和精度是不够用的。 二分类问题中，根据真实类别和学习器预测类别划分出真正例（true positive）、假正例（false positive）、真反例（true negative）、假反例（false negative），查准率P和查全率R分别定义为： $$P=\\frac{TP}{TP+FP}$$ $$R=\\frac{TP}{TP+FN}$$ 二者是矛盾的度量，一个高另一个就低。平衡点（Break-Event Point）就是一个综合考虑双方性能度量，取得相对双高的比例。 2.3.3 ROC 与 AUCROC (Receiver Operating Characteristic) 受试者工作特征曲线，横坐标是False Positive，纵坐标时 True Positive。其横纵坐标是没有相关性的，所以不能把其当作函数曲线分析，二十四将其看作无数的点。 AUC (Area Under ROC Curve), ROC曲线下的面积。当一个学习器的ROC曲线被另一个学习器的曲线完全包住，可断言后者性能优越。当二者ROC曲线发生交叉时，就需要通过AUC来判据。AUC估算为： $$AUC=\\frac{1}{2}\\sum^{m-1}{i=1}(x{i+1}-x_i)(y_i+y_{i+1})$$ AUC考虑的是样本预测的排序质量，因此它与排序误差有关联。 2.3.4 代价敏感错误率和代价曲线根据不同类型造成的不同损失，可为错误赋予非均等代价 unequal cost。 在非均等代价下，ROC曲线不能直接反应学习器的期望总体代价，但是代价曲线可以。 ","date":"2021-09-30","objectID":"/pumpkin-book-notes01/:2:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 01","uri":"/pumpkin-book-notes01/#23-性能度量-performance-measure"},{"categories":["machine learning"],"content":"\r2.3 性能度量 performance measure衡量模型泛化能力的评价标准，比如回归任务常用的“均方误差”。 2.3.1 错误率和精度错误率与精度的关系为 $1-E(f;D)=acc(f;D)$ 2.3.2 查准率 precision、查全率 recall 与F1在类似信息检索的应用中，错误率和精度是不够用的。 二分类问题中，根据真实类别和学习器预测类别划分出真正例（true positive）、假正例（false positive）、真反例（true negative）、假反例（false negative），查准率P和查全率R分别定义为： $$P=\\frac{TP}{TP+FP}$$ $$R=\\frac{TP}{TP+FN}$$ 二者是矛盾的度量，一个高另一个就低。平衡点（Break-Event Point）就是一个综合考虑双方性能度量，取得相对双高的比例。 2.3.3 ROC 与 AUCROC (Receiver Operating Characteristic) 受试者工作特征曲线，横坐标是False Positive，纵坐标时 True Positive。其横纵坐标是没有相关性的，所以不能把其当作函数曲线分析，二十四将其看作无数的点。 AUC (Area Under ROC Curve), ROC曲线下的面积。当一个学习器的ROC曲线被另一个学习器的曲线完全包住，可断言后者性能优越。当二者ROC曲线发生交叉时，就需要通过AUC来判据。AUC估算为： $$AUC=\\frac{1}{2}\\sum^{m-1}{i=1}(x{i+1}-x_i)(y_i+y_{i+1})$$ AUC考虑的是样本预测的排序质量，因此它与排序误差有关联。 2.3.4 代价敏感错误率和代价曲线根据不同类型造成的不同损失，可为错误赋予非均等代价 unequal cost。 在非均等代价下，ROC曲线不能直接反应学习器的期望总体代价，但是代价曲线可以。 ","date":"2021-09-30","objectID":"/pumpkin-book-notes01/:2:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 01","uri":"/pumpkin-book-notes01/#231-错误率和精度"},{"categories":["machine learning"],"content":"\r2.3 性能度量 performance measure衡量模型泛化能力的评价标准，比如回归任务常用的“均方误差”。 2.3.1 错误率和精度错误率与精度的关系为 $1-E(f;D)=acc(f;D)$ 2.3.2 查准率 precision、查全率 recall 与F1在类似信息检索的应用中，错误率和精度是不够用的。 二分类问题中，根据真实类别和学习器预测类别划分出真正例（true positive）、假正例（false positive）、真反例（true negative）、假反例（false negative），查准率P和查全率R分别定义为： $$P=\\frac{TP}{TP+FP}$$ $$R=\\frac{TP}{TP+FN}$$ 二者是矛盾的度量，一个高另一个就低。平衡点（Break-Event Point）就是一个综合考虑双方性能度量，取得相对双高的比例。 2.3.3 ROC 与 AUCROC (Receiver Operating Characteristic) 受试者工作特征曲线，横坐标是False Positive，纵坐标时 True Positive。其横纵坐标是没有相关性的，所以不能把其当作函数曲线分析，二十四将其看作无数的点。 AUC (Area Under ROC Curve), ROC曲线下的面积。当一个学习器的ROC曲线被另一个学习器的曲线完全包住，可断言后者性能优越。当二者ROC曲线发生交叉时，就需要通过AUC来判据。AUC估算为： $$AUC=\\frac{1}{2}\\sum^{m-1}{i=1}(x{i+1}-x_i)(y_i+y_{i+1})$$ AUC考虑的是样本预测的排序质量，因此它与排序误差有关联。 2.3.4 代价敏感错误率和代价曲线根据不同类型造成的不同损失，可为错误赋予非均等代价 unequal cost。 在非均等代价下，ROC曲线不能直接反应学习器的期望总体代价，但是代价曲线可以。 ","date":"2021-09-30","objectID":"/pumpkin-book-notes01/:2:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 01","uri":"/pumpkin-book-notes01/#232-查准率-precision查全率-recall-与f1"},{"categories":["machine learning"],"content":"\r2.3 性能度量 performance measure衡量模型泛化能力的评价标准，比如回归任务常用的“均方误差”。 2.3.1 错误率和精度错误率与精度的关系为 $1-E(f;D)=acc(f;D)$ 2.3.2 查准率 precision、查全率 recall 与F1在类似信息检索的应用中，错误率和精度是不够用的。 二分类问题中，根据真实类别和学习器预测类别划分出真正例（true positive）、假正例（false positive）、真反例（true negative）、假反例（false negative），查准率P和查全率R分别定义为： $$P=\\frac{TP}{TP+FP}$$ $$R=\\frac{TP}{TP+FN}$$ 二者是矛盾的度量，一个高另一个就低。平衡点（Break-Event Point）就是一个综合考虑双方性能度量，取得相对双高的比例。 2.3.3 ROC 与 AUCROC (Receiver Operating Characteristic) 受试者工作特征曲线，横坐标是False Positive，纵坐标时 True Positive。其横纵坐标是没有相关性的，所以不能把其当作函数曲线分析，二十四将其看作无数的点。 AUC (Area Under ROC Curve), ROC曲线下的面积。当一个学习器的ROC曲线被另一个学习器的曲线完全包住，可断言后者性能优越。当二者ROC曲线发生交叉时，就需要通过AUC来判据。AUC估算为： $$AUC=\\frac{1}{2}\\sum^{m-1}{i=1}(x{i+1}-x_i)(y_i+y_{i+1})$$ AUC考虑的是样本预测的排序质量，因此它与排序误差有关联。 2.3.4 代价敏感错误率和代价曲线根据不同类型造成的不同损失，可为错误赋予非均等代价 unequal cost。 在非均等代价下，ROC曲线不能直接反应学习器的期望总体代价，但是代价曲线可以。 ","date":"2021-09-30","objectID":"/pumpkin-book-notes01/:2:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 01","uri":"/pumpkin-book-notes01/#233-roc-与-auc"},{"categories":["machine learning"],"content":"\r2.3 性能度量 performance measure衡量模型泛化能力的评价标准，比如回归任务常用的“均方误差”。 2.3.1 错误率和精度错误率与精度的关系为 $1-E(f;D)=acc(f;D)$ 2.3.2 查准率 precision、查全率 recall 与F1在类似信息检索的应用中，错误率和精度是不够用的。 二分类问题中，根据真实类别和学习器预测类别划分出真正例（true positive）、假正例（false positive）、真反例（true negative）、假反例（false negative），查准率P和查全率R分别定义为： $$P=\\frac{TP}{TP+FP}$$ $$R=\\frac{TP}{TP+FN}$$ 二者是矛盾的度量，一个高另一个就低。平衡点（Break-Event Point）就是一个综合考虑双方性能度量，取得相对双高的比例。 2.3.3 ROC 与 AUCROC (Receiver Operating Characteristic) 受试者工作特征曲线，横坐标是False Positive，纵坐标时 True Positive。其横纵坐标是没有相关性的，所以不能把其当作函数曲线分析，二十四将其看作无数的点。 AUC (Area Under ROC Curve), ROC曲线下的面积。当一个学习器的ROC曲线被另一个学习器的曲线完全包住，可断言后者性能优越。当二者ROC曲线发生交叉时，就需要通过AUC来判据。AUC估算为： $$AUC=\\frac{1}{2}\\sum^{m-1}{i=1}(x{i+1}-x_i)(y_i+y_{i+1})$$ AUC考虑的是样本预测的排序质量，因此它与排序误差有关联。 2.3.4 代价敏感错误率和代价曲线根据不同类型造成的不同损失，可为错误赋予非均等代价 unequal cost。 在非均等代价下，ROC曲线不能直接反应学习器的期望总体代价，但是代价曲线可以。 ","date":"2021-09-30","objectID":"/pumpkin-book-notes01/:2:3","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 01","uri":"/pumpkin-book-notes01/#234-代价敏感错误率和代价曲线"},{"categories":["machine learning"],"content":"\r2.4 偏差与方差偏差方差分解是解释学习算法泛化性能的重要工具，对算法的期望泛化错误率进行拆解。 ","date":"2021-09-30","objectID":"/pumpkin-book-notes01/:2:4","series":["pumpkinbook - notes"],"tags":["machine learning","西瓜书","南瓜书"],"title":"Machine Learning Notes 01","uri":"/pumpkin-book-notes01/#24-偏差与方差"}]