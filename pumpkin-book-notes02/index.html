

<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="noodp" />
    <title>Machine Learning Notes 02 - HongSpell</title><meta name="Description" content=""><meta property="og:title" content="Machine Learning Notes 02" />
<meta property="og:description" content="3 线性模型 3.1 基本形式线性模型形式简单易于建模，一些非线性模型也可在此基础上引入层级结构或高维映射得到。 $$f(x)= \omega^Tx&#43;b$$ 3.2 Linear Regression线性回归通" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://hongspell.site/pumpkin-book-notes02/" /><meta property="og:image" content="https://hongspell.site/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-10-06T16:19:14+08:00" />
<meta property="article:modified_time" content="2021-10-06T16:19:14+08:00" /><meta property="og:site_name" content="HongSpell" />
<meta property="og:see_also" content="https://hongspell.site/pumpkin-book-notes01/" /><meta property="og:see_also" content="https://hongspell.site/pumpkin-book-notes03/" /><meta property="og:see_also" content="https://hongspell.site/pumpkin-book-notes04/" /><meta property="og:see_also" content="https://hongspell.site/pumpkin-book-notes05/" /><meta property="og:see_also" content="https://hongspell.site/pumpkin-book-notes06/" />


<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://hongspell.site/" /><meta name="twitter:title" content="Machine Learning Notes 02"/>
<meta name="twitter:description" content="3 线性模型 3.1 基本形式线性模型形式简单易于建模，一些非线性模型也可在此基础上引入层级结构或高维映射得到。 $$f(x)= \omega^Tx&#43;b$$ 3.2 Linear Regression线性回归通"/>
<meta name="twitter:site" content="@ssdlaohu9527"/>
<meta name="application-name" content="HongSpell">
<meta name="apple-mobile-web-app-title" content="HongSpell">

<meta name="theme-color" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><meta name="twitter:creator" content="@ssdlaohu9527" /><link rel="icon" href="/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="canonical" href="https://hongspell.site/pumpkin-book-notes02/" /><link rel="prev" href="https://hongspell.site/pumpkin-book-notes01/" /><link rel="next" href="https://hongspell.site/pumpkin-book-notes03/" />
<link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/color.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" as="style" onload="this.onload=null;this.rel='stylesheet'" href="/lib/fontawesome-free/all.min.css">
        <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" as="style" onload="this.onload=null;this.rel='stylesheet'" href="/lib/animate/animate.min.css">
        <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Machine Learning Notes 02",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://hongspell.site/pumpkin-book-notes02/"
        },"genre": "posts","keywords": "machine learning, 西瓜书, 南瓜书","wordcount":  5029 ,
        "url": "https://hongspell.site/pumpkin-book-notes02/","datePublished": "2021-10-06T16:19:14+08:00","dateModified": "2021-10-06T16:19:14+08:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "Hong"
            },"description": ""
    }
    </script></head>

<body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">
        function setTheme(theme) {document.body.setAttribute('theme', theme); document.documentElement.style.setProperty('color-scheme', theme === 'light' ? 'light' : 'dark'); window.theme = theme;   window.isDark = window.theme !== 'light' }
        function saveTheme(theme) {window.localStorage && localStorage.setItem('theme', theme);}
        function getMeta(metaName) {const metas = document.getElementsByTagName('meta'); for (let i = 0; i < metas.length; i++) if (metas[i].getAttribute('name') === metaName) return metas[i]; return '';}
        if (window.localStorage && localStorage.getItem('theme')) {let theme = localStorage.getItem('theme');theme === 'light' || theme === 'dark' || theme === 'black' ? setTheme(theme) : (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? setTheme('dark') : setTheme('light')); } else { if ('auto' === 'light' || 'auto' === 'dark' || 'auto' === 'black') setTheme('auto'), saveTheme('auto'); else saveTheme('auto'), window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? setTheme('dark') : setTheme('light');}
        let metaColors = {'light': '#f8f8f8','dark': '#252627','black': '#000000'}
        getMeta('theme-color').content = metaColors[document.body.getAttribute('theme')];
        window.switchThemeEventSet = new Set()
    </script>
    <div id="back-to-top"></div>
    <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="HongSpell"><span id="desktop-header-typeit" class="typeit"></span></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 文章 </a><a class="menu-item" href="/projects/"> 项目 </a><a class="menu-item" href="/sketch/"> 手绘 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/Hongmiao0207" title="GitHub" rel="noopener noreferrer" target="_blank"><i class='fab fa-github fa-fw'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-select" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                    <select class="color-theme-select" id="theme-select-desktop" title="Switch Theme">
                        <option value="light">Light</option>
                        <option value="dark">Dark</option>
                        <option value="black">Black</option>
                        <option value="auto">Auto</option>
                    </select>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="HongSpell"><span id="mobile-header-typeit" class="typeit"></span></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">文章</a><a class="menu-item" href="/projects/" title="">项目</a><a class="menu-item" href="/sketch/" title="">手绘</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/Hongmiao0207" title="GitHub" rel="noopener noreferrer" target="_blank"><i class='fab fa-github fa-fw'></i></a><a href="javascript:void(0);" class="menu-item theme-select" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
                <select class="color-theme-select" id="theme-select-mobile" title="Switch Theme">
                    <option value="light">Light</option>
                    <option value="dark">Dark</option>
                    <option value="black">Black</option>
                    <option value="auto">Auto</option>
                </select>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
            <div class="container"><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC", "false")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Machine Learning Notes 02</h1><div class="post-meta">
            <div class="post-meta-line">
                <span class="post-author"><span class="author fas fa-user-circle fa-fw"></span><a href="/" title="Author" rel=" author" class="author">Hong</a>
                </span>&nbsp;<span class="post-category">included in </span>&nbsp;<span class="post-category">category <a href="/categories/machine-learning/"><i class="far fa-folder fa-fw"></i>machine learning</a></span>&nbsp;<span class="post-category">and</span>&nbsp;<span class="post-series">series <a href="/series/pumpkinbook-notes/"><i class="far fa-list-alt fa-fw"></i>pumpkinbook - notes</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2021-10-06">2021-10-06</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;5029 words&nbsp;<i class="far fa-clock fa-fw"></i>&nbsp;11 minutes&nbsp;</div>
        </div><div class="featured-image"><img
        
        loading="eager"
        src="/posts/pumpkin-book-notes/xigua.png"
        srcset="/posts/pumpkin-book-notes/xigua.png, /posts/pumpkin-book-notes/xigua.png 1.5x, /posts/pumpkin-book-notes/xigua.png 2x"
        sizes="auto"
        alt="/posts/pumpkin-book-notes/xigua.png"
        title="/posts/pumpkin-book-notes/xigua.png" height="auto"   width="auto" ></div><div class="details series-nav open">
                                <div class="details-summary series-title">
                                    <span>Series - pumpkinbook - notes</span>
                                    <span><i class="details-icon fas fa-angle-right"></i></span>
                                </div>
                                <div class="details-content series-content">
                                    <nav>
                                        <ul>
                                                    <li><a href="/pumpkin-book-notes01/">Machine Learning Notes 01</a></li><li><span class="active">Machine Learning Notes 02</span></li>
                                                    <li><a href="/pumpkin-book-notes03/">Machine Learning Notes 03</a></li>
                                                    <li><a href="/pumpkin-book-notes04/">Machine Learning Notes 04</a></li>
                                                    <li><a href="/pumpkin-book-notes05/">Machine Learning Notes 05</a></li>
                                                    <li><a href="/pumpkin-book-notes06/">Machine Learning Notes 06</a></li></ul>
                                    </nav>
                                </div>
                            </div><div class="details toc" id="toc-static"  kept="true">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#3-线性模型">3 线性模型</a>
      <ul>
        <li><a href="#31-基本形式">3.1 基本形式</a></li>
        <li><a href="#32-linear-regression">3.2 Linear Regression</a>
          <ul>
            <li><a href="#321--一元线性回归">3.2.1  一元线性回归</a></li>
            <li><a href="#322-多元线性回归-multivariate-linear-regression">3.2.2 多元线性回归 multivariate linear regression</a></li>
          </ul>
        </li>
        <li><a href="#33-对数几率回归">3.3 对数几率回归</a>
          <ul>
            <li><a href="#拓展信息论">拓展：信息论</a></li>
          </ul>
        </li>
        <li><a href="#34-二分类线性判别分析">3.4 二分类线性判别分析</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h2 id="3-线性模型" class="headerLink">
    <a href="#3-%e7%ba%bf%e6%80%a7%e6%a8%a1%e5%9e%8b" class="header-mark"></a>3 线性模型</h2><h3 id="31-基本形式" class="headerLink">
    <a href="#31-%e5%9f%ba%e6%9c%ac%e5%bd%a2%e5%bc%8f" class="header-mark"></a>3.1 基本形式</h3><p>线性模型形式简单易于建模，一些非线性模型也可在此基础上引入层级结构或高维映射得到。</p>

$$f(x)= \omega^Tx+b$$

<h3 id="32-linear-regression" class="headerLink">
    <a href="#32-linear-regression" class="header-mark"></a>3.2 Linear Regression</h3><p>线性回归通过给定数据晒图学得一个线性模型达到 $f(x)= \omega^Tx+b$ 无限趋近于 y。</p>
<p>其中$\omega$ 和 b 取值的关键在于衡量f(x)和y之间的差别，因此可以将军方误差最小化来达到目的，即</p>

$$(\omega^*, b^*) = \underset{(\omega, b)}{\arg\min} \sum_{i=1}^{m} (f(x_i) - y_i)^2$$

$$=arg \, min_(\omega,b) \sum^m_{i=1} (y_i-\omega x_i-b)^2$$

<p>均方误差的几何意义是欧氏距离，其求解方式是最小二乘法，找到一条直线使所有样本到该直线上的欧氏距离之和最小。</p>
<p>线性回归模型的最小二乘“参数估计”是求解$\omega$和b是均方误差 $E_{(\omega,b)}=\sum^m_{i=1}(y_i-\omega x_i-b)^2$ 最小化的过程。E是关于$\omega$和b的凸函数，当导数为0时，得到最优解。</p>
<p>注：凸函数，对区间[a, b]上定义的函数f，若它对区间中任意两点x1，x2均有 $f(\frac{x_1+x_2}{2} &lt;= \frac{f(x_1)+f(x_2)}{2})$，则称f为区间[a,b]上的凸函数。</p>
<ul>
<li>U醒曲线的函数如f(x)=x^2，通常为凸函数。</li>
<li>实数集上的函数，可通过求二阶导来判别：二阶导在区间上为非负就是凸函数；若其恒大于0，则称为严格凸函数。</li>
</ul>
<h4 id="321--一元线性回归" class="headerLink">
    <a href="#321--%e4%b8%80%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92" class="header-mark"></a>3.2.1  一元线性回归</h4><ul>
<li>3.5 对 $\omega$求导:</li>
</ul>

$$\frac{\partial E_{(\omega, b)}}{\partial \omega}=2 \Bigg(\omega \sum^m_{i=1}x^2_i - \sum^m_{i=1}(y_i - b) x_ix \Bigg)$$

<ul>
<li>推导：</li>
</ul>
<p>已知 $E_{(\omega,b)}=\sum^m_{i=1}(y_i-\omega x_i-b)^2$，所以</p>

$$\frac{\partial E_{(\omega, b)}}{\partial \omega}= \frac{\partial}{\partial \omega} \Bigg[ \sum^m_{i=1} (y_i - \omega x_i - b)^2 \Bigg]$$

$$= \sum^m_{i=1} \frac{\partial}{\partial \omega} \Bigg[ (y_i - \omega x_i - b)^2 \Bigg] $$

$$= \sum^m_{i=1} [2 (y_i - \omega x_i - b) (-x_i)]$$

$$= \sum^m_{i=1} [2 (\omega x_i^2 - y_ix_i +bx_i)]$$

$$= 2 \Bigg( \omega \sum^m_{i=1} x_i^2 - \sum^m_{i=1} y_ix_i + b \sum^m_{i=1} x_i \Bigg)$$

$$= 2 \Bigg( \omega \sum^m_{i=1} x_i^2 - \sum^m_{i=1} (y_i - b)x_i \Bigg)$$

<ul>
<li>
<p>3.6 对b求导

$$\frac{\partial E_{(\omega, b)}}{\partial b} = 2 \Bigg( mb - \sum^m_{i=1} (y_i - \omega x_i) \Bigg)$$
</p>
</li>
<li>
<p>推导：</p>
</li>
</ul>
<p>已知 $E_{(\omega,b)}=\sum^m_{i=1}(y_i-\omega x_i-b)^2$，所以</p>

$$\frac{\partial E_{(\omega, bn)}}{\partial b} = \frac{\partial}{\partial b} \Bigg[ \sum^m_{i=1} (y_i - \omega x_i - b)^2 \Bigg]$$

$$= \sum^m_{i=1} \frac{\partial}{\partial b} \Bigg[ (y_i - \omega x_i - b)^2 \Bigg]$$

$$= \sum^m_{i=1} [2 (y_i - \omega x_i - b)(-1)] $$

$$= \sum^m_{i=1} [2(b - y_i + \omega x_i)]$$

$$= 2 \Bigg[ \sum^m_{i=1} b - \sum^m_{i=1} y_i + \sum^m_{i=1} \omega x_i \Bigg]$$

$$= 2 \Bigg( mb - \sum^m_{i=1}(y_i - \omega x_i) \Bigg)$$

<ul>
<li>当3.5、3.6为0得到 $\omega$ 和 b 最优解的闭式解（closed-form）</li>
</ul>
<p>3.7</p>

$$\omega = \frac{\sum^m_{i=1} y_i (x_i - \bar{x})}{\sum^m_{i=1} x^2_i - \frac{1}{m} (\sum^m_{i=1} x_i)^2}$$

<p>3.8</p>

$$b = \frac{1}{m} \sum^m_{i=1} (y_i - \omega x_i)$$

<p>其中 $\bar{x} = \frac{1}{m} \sum^m_{i=1} x_i$ 为 x 的均值。</p>
<p>3.7推导，令3.5等于0</p>

$$0 = \omega \sum^m_{i=1} x^2_i - \sum^m_{i=1} (y_i - b) x_i$$

$$\omega \sum^m_{i=1} x^2_i = \sum^m_{i=1} y_i x_i - \sum^m_{i=1}  b x_i$$

<p>由于令3.6等于0可得 $b = \frac{1}{m} \sum^m_{i=1} (y_i - \omega x_i)$，又因为 $\frac{1}{m} \sum^m_{i=1} y_i = \bar{y}$ ，$\frac{1}{m} \sum^m_{i=1} x_i = \bar{x}$, 则 $b = \bar{y} - \omega \bar{x}$，代入上式可得：</p>

$$\omega \sum^m_{i=1} x^2_i = \sum^m_{i=1} y_i x_i - \sum^m_{i=1} (\bar{y} - \omega \bar{x}) x_i$$

$$\omega \sum^m_{i=1} x^2_i = \sum^m_{i=1} y_i x_i - \bar{y} \sum^m_{i=1} x_i + \omega \bar{x} \sum^m_{i=1} x_i$$

$$\omega (\sum^m_{i=1} x^2_i - \bar{x} \sum^m_{i=1} x_i) = \sum^m_{i=1} y_i x_i - \bar{y} \sum^m_{i=1} x_i$$

$$\omega = \frac{\sum^m_{i=1} y_i x_i - \bar{y} \sum^m_{i=1} x_i}{\sum^m_{i=1} x^2_i - \bar{x} \sum^m_{i=1} x_i}$$

<p>由于</p>

$$\bar{y} \sum^m_{i=1} x_i = \frac{1}{m} \sum^m_{i=1}y_i \sum^m_{i=1} x_i = \bar{x} \sum^m_{i=1} y_i$$

$$\bar{x} \sum^m_{i=1} x_i = \frac{1}{m} x_i \sum^m_{i=1} x_1 = \frac{1}{m}(\sum^m_{i=1}x_i)^2$$

<p>代入上式即可得到公式3.7</p>

$$\omega = \frac{\sum^m_{i=1} y_i x_i - \bar{y} \sum^m_{i=1} x_i}{\sum^m_{i=1} x^2_i - \bar{x} \sum^m_{i=1} x_i}$$

$$= \frac{\sum^m_{i=1} y_i x_i - \bar{x} \sum^m_{i=1} y_i}{\sum^m_{i=1} x^2_i - \frac{1}{m}(\sum^m_{i=1}x_i)^2}$$

<p>3.7</p>

$$= \frac{\sum^m_{i=1} y_i (x_i-\bar{x})}{\sum^m_{i=1} x^2_i - \frac{1}{m}(\sum^m_{i=1}x_i)^2}$$

<p>上述的求和运算只能通过python的循环来实现，如果将上式向量化，转换成矩阵运算，那就可以通过 Numpy来实现。向量化：</p>
<p>将 $\frac{1}{m}(\sum^m_{i=1} x_i)^2 = \bar{x} \sum^m_{i=1} x_i$ 代入分母可得：</p>

$$\omega = \frac{\sum^m_{i=1} y_i (x_i - \bar{x})}{\sum^m_{i=1} x^2_i - \bar{x} \sum^m_{i=1} x_i}$$

$$=\frac{\sum^m_{i=1} (y_ix_i - y_i\bar{x})}{\sum^m_{i=1} (x^2_i - \bar{x} x_i)}$$

<p>又因为</p>

$$\bar{y} \sum^m_{i=1} x_i = \bar{x} \sum^m_{i=1}y_i = \sum^m_{i=1} \bar{y}x_i = \sum^m_{i=1} \bar{x}y_i = m\bar{x} \bar{y} = \sum^m_{i=1} \bar{x}\bar{y}$$

$$\sum^m_{i=1} x_i \bar{x} = \bar{x} \sum^m_{i=1} x_i = \bar{x}m\frac{1}{m} \sum^m_{i=1} x_i = m \bar{x}^2 = \sum^m_{i=1} \bar{x}^2$$

<p>则上式可化为：</p>

$$\omega = \frac{\sum^m_{i=1} (y_ix_i - y_i \bar{x} - x_i \bar{y} + \bar{x} \bar{y})}{\sum^m_{i=1}(x^2_i - x_i\bar{x} - x_i\bar{x} + \bar{x}^2)}$$

$$= \frac{\sum^m_{i=1} (x_i - \bar{x}) (y_i - \bar{y})}{\sum^m_{i=1} (x_i - \bar{x})^2}$$

<p>若令 $x = (x_1,x_2,&hellip;,x_m)^T$，$x_d=(x_1-\bar{x}, x_2-\bar{x},&hellip;,x_m-\bar{x})^T$ 为去均值后的x。 $y = (y_1,y_2,&hellip;,y_m)^T$，$y_d=(y_1-\bar{y}, y_2-\bar{y},&hellip;,y_m-\bar{y})^T$ 为去均值后的y。其中 x、xd、y、yd均为m行1列的列向量，代入上式可得：</p>
<p>$$\omega = \frac{x^T_d y_d}{x^T_d x_d}$$</p>
<h4 id="322-多元线性回归-multivariate-linear-regression" class="headerLink">
    <a href="#322-%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92-multivariate-linear-regression" class="header-mark"></a>3.2.2 多元线性回归 multivariate linear regression</h4><p>样本由d个属性描述，而不是单个属性描述。多个特征，x和权重就变成了向量。</p>
<p>$$f(x_i) = \omega^Tx_i+b$$</p>
<p>1</p>

$$f(x_1)=(\omega_1 \, \omega_2 \, \dots \, \omega_d)
\left(
\begin{matrix}
x_{i1} \\
x_{i2} \\
\vdots \\
x_{id}
\end{matrix}
\right)
+b
$$

<p>2</p>

$$ f(x_i) = \omega_1 x_{i1} + \omega_2 x_{i2} + ... + \omega_d x_{id} + b$$

<p>3</p>

$$f(x_i) = \omega_1 x_{i1} + \omega_2 x_{i2} + ... + \omega_d x_{id} + \omega_{d+1}$$

<p>解析：将 b 拆成 $\omega_{d+1} * 1$，此时前面是omega，后面的1是x，因此，在1式中，omega_d之后加一个新元素omega_d+1，因此 x_id后面也会多出一个新元素，就是1。所以可知，w_d+1 * 1 = b，所以 2、3式子是等价的。</p>
<p>这样做的好处是可以将式子全部向量化。</p>

$$
f(x_1)=(\omega_1 \, \omega_2 \, \dots \, \omega_d \, \omega_{d+1})
\left(
\begin{matrix}
x_{i1} \\
x_{i2} \\
\vdots \\
x_{id} \\
1
\end{matrix}
\right)
$$

<p>$$f(\hat{x}_i) = \hat{\omega}^T \hat{x}_i$$</p>
<p>由最小二乘法可得：</p>

$$
E_{\hat{\omega}}
= \sum^m_{i=1} (y_i - f(\hat{x}_i))^2
= \sum^m_{i=1}(y_i - \hat{\omega}^T \hat{x}_i)^2
$$

<p>再将求和符号向量化（向量化的目的是便于用过numpy运算）</p>
<ol>
<li>将求和拆开</li>
</ol>

$$
E_{\hat{\omega}}
= \sum^m_{i=1}(y_i - \hat{\omega}^T \hat{x}_i)^2
= (y_1 - \hat{\omega}^T \hat{x}_1)^2
+ (y_2 - \hat{\omega}^T \hat{x}_2)^2
+ \dots
+ (y_m - \hat{\omega}^T \hat{x}_m)^2
$$

<p>解析，如何将求和拆成向量形式</p>

$$
a^2+b^2
= [a \ b]
\left[
\begin{matrix}
a \\
b
\end{matrix}
\right]
$$

$$
E_{\hat{\omega}}
= (y_1 - \hat{\omega}^T \hat{x}_1 \ \ \ y_2 - \hat{\omega}^T \hat{x}_2 \ \ \ ... \ \ \ y_m - \hat{\omega}^T \hat{x}_m)
\left(
\begin{matrix}
y_1 - \hat{\omega}^T \hat{x}_1 \\
y_2 - \hat{\omega}^T \hat{x}_2 \\
\vdots \\
y_m - \hat{\omega}^T \hat{x}_m
\end{matrix}
\right)
$$

<p>后面的式子化简：</p>
<ol>
<li>先将拆成两个列向量相减，后面的根据 $a^Tb=b^Ta$ 原则进行转换，因为它最终为一个数，所以相等，可以转换。</li>
</ol>

$$
\left(
\begin{matrix}
y_1 - \hat{\omega}^T \hat{x}_1 \\
y_2 - \hat{\omega}^T \hat{x}_2 \\
\vdots \\
y_m - \hat{\omega}^T \hat{x}_m
\end{matrix}
\right)
=
\left(
\begin{matrix}
y_1 \\
y_2 \\
\vdots \\
y_m
\end{matrix}
\right)
-
\left(
\begin{matrix}
\hat{\omega}^T \hat{x}_1 \\
\hat{\omega}^T \hat{x}_2 \\
\vdots \\
\hat{\omega}^T \hat{x}_m
\end{matrix}
\right)
=
\left(
\begin{matrix}
y_1 \\
y_2 \\
\vdots \\
y_m
\end{matrix}
\right)
-
\left(
\begin{matrix}
\hat{x}^T_1 \hat{\omega} \\
\hat{x}^T_2 \hat{\omega} \\
\vdots \\
\hat{x}^T_m \hat{\omega}
\end{matrix}
\right)
$$

$$y = \left(\begin{matrix} y_1 \\ y_2 \\ \vdots \\ y_m \end{matrix} \right)$$

<ol>
<li>每个元素中都有 $\hat{\omega}$，所以提出来。将 $\hat{x}^T_i$ 拆成 $x^T_i \ \ 1$ 的形式，最后一个蒜素恒置为1。西瓜书上定义了一个大X，就是这样的形式。

$$
\left(
\begin{matrix}
\hat{x}^T_1 \hat{\omega} \\
\hat{x}^T_2 \hat{\omega} \\
\vdots \\
\hat{x}^T_m \hat{\omega}
\end{matrix}
\right)
=
\left(
\begin{matrix}
\hat{x}^T_1 \\
\hat{x}^T_2 \\
\vdots \\
\hat{x}^T_m \\
\end{matrix}
\right)

\hat{\omega}
=
\left(
\begin{matrix}
x^T_1 \ \ 1 \\
x^T_2 \ \ 1 \\
\vdots \ \ \ \ \vdots \\
x^T_m \ \ 1
\end{matrix}
\right)

\hat{\omega}
=
X \hat{\omega}
$$
</li>
</ol>
<p>因此</p>

$$
\left(
\begin{matrix}
y_1 - \hat{\omega}^T \hat{x}_1 \\
y_2 - \hat{\omega}^T \hat{x}_2 \\
\vdots \\
y_m - \hat{\omega}^T \hat{x}_m
\end{matrix}
\right)
= y - X \hat{\omega}
$$

<p>下面的式子中，第二项就是上面的，第一项就是第二项的转置</p>

$$
E_{\hat{\omega}}
=
\left(
\begin{matrix}
y_1 - \hat{\omega}^T \hat{x}_1 \ \ \
y_2 - \hat{\omega}^T \hat{x}_2 \ \ \
\dots \ \ \
y_m - \hat{\omega}^T \hat{x}_m
\end{matrix}
\right)

\left(
\begin{matrix}
y_1 - \hat{\omega}^T \hat{x}_1 \\
y_2 - \hat{\omega}^T \hat{x}_2 \\
\vdots \\
y_m - \hat{\omega}^T \hat{x}_m  
\end{matrix}
\right)
$$

<p>完成 损失函数 向量化</p>

$$
E{\hat{\omega}} = (y-X \hat{\omega})^T (y-X \hat{\omega})
$$

<p>求解$\hat{\omega}$
$$\hat{\omega}^* = arg_{\hat{\omega}}min (y - X \hat{\omega})^T (y-X\hat{\omega})$$</p>
<p>求解$\omega$仍然是一个多元函数求最值问题，也是凸函数求最值问题。</p>
<p>思路：</p>
<ol>
<li>证明 $E_{\hat{\omega}} = (y-X \hat{\omega})^T (y-X \hat{\omega})$ 是关于$\hat{\omega}$ 的凸函数。（证明凸函数就是Ew是w的二级偏导数。因为w是向量（多元），也就是求他的海森矩阵。证明该矩阵是一个半正定矩阵即可）</li>
<li>用凸函数求最值的思路求 $\hat{\omega}$。（令它的梯度为0，也就是关于w的一阶导数为0， $\frac{\partial E_{\hat{\omega}}}{\partial \hat{\omega}} = 0$）</li>
</ol>
<p>求解：</p>
<ol>
<li>求 $E_{\hat{\omega}}$ 的Hessian矩阵 $\nabla^2 E_{\hat{\omega}}$（这个倒三角符号就是梯度、向量微分），然后判断其正定性：</li>
</ol>

$$
\frac{\partial E_{\hat{\omega}}}{\partial \hat{\omega}}
=
\frac{\partial}{\partial \hat{\omega}}
[
    (y-X \hat{\omega})^T
    (y-X \hat{\omega})
]
$$

$$
= \frac{\partial}{\partial \hat{\omega}}
[
    (y^T - \hat{\omega}^T X^T)
    (y-X \hat{\omega})
]
$$

$$
= \frac{\partial}{\partial \hat{\omega}}
[
    y^T y - y^T X \hat{\omega} - \hat{\omega}^T X^T y + \hat{\omega} X^T X \hat{\omega}
]
$$

其中，$y^Ty$ 不含有 $\omega$，可以扔掉。

$$
= \frac{\partial}{\partial \hat{\omega}}
[
    - y^T X \hat{\omega} - \hat{\omega}^T X^T y + \hat{\omega} X^T X \hat{\omega}
]
$$

$$
= -\frac{\partial y^T X \hat{\omega}}{\partial \hat{\omega}}
- \frac{\partial \hat{\omega}^T X^T y}{\partial \hat{\omega}}
+\frac{\partial \hat{\omega} X^T X \hat{\omega}}{\partial \hat{\omega}}
$$

<p>已知 $\hat{\omega}$ 是d+1元的标量函数，也就是说它的未知数是d+1维的向量。但是三个分子上最后函数式子的结果是个标量，因为y转置是个行向量、X是个矩阵，$\hat{\omega}$是个列向量，整体结果是个标量。所以它是<strong>标量关于向量求导</strong> （矩阵微分的内容）。求导方式就是梯度</p>
<p>如何求解 $\hat{\omega}$</p>
<ul>
<li>【标量-向量】的矩阵微分公式：设 $x \in R^{n*1}, f: R^n \rightarrow R$ 为关于x的实值标量函数，则</li>
</ul>

$$
\frac{\partial f(x)}{\partial x}
=
\left[
\begin{matrix}
\frac{\partial f(x)}{\partial x_1} \\
\frac{\partial f(x)}{\partial x_2} \\
\vdots \\
\frac{\partial f(x)}{\partial x_n}
\end{matrix}
\right]
$$

$$
\frac{\partial f(x)}{\partial x^T}
=
\left(
\begin{matrix}
\frac{\partial f(x)}{\partial x_1} \ 
\frac{\partial f(x)}{\partial x_2} \
\dots \
\frac{\partial f(x)}{\partial x_n}
\end{matrix}
\right)
$$

<ul>
<li>就是对x分量（x1&hellip;xn）的偏导数求出来组成一个列向量。组成列向量为分母布局（默认），组成行向量为分子布局，仅差一个转置。</li>
</ul>
<p>求解 $\hat{\omega}$</p>

$$
\frac{\partial E_{\hat{\omega}}}{\partial \hat{\omega}}
=
-\frac{\partial y^T X \hat{\omega}}{\partial \hat{\omega}}
- \frac{\partial \hat{\omega}^T X^T y}{\partial \hat{\omega}}
+\frac{\partial \hat{\omega} X^T X \hat{\omega}}{\partial \hat{\omega}}
$$

<p>根据矩阵微分公式 $\frac{\partial x^T a}{\partial x} = \frac{\partial a^T x}{\partial x} = a$，$\frac{\partial x^T Ax}{\partial x} = (A + A^T)x$ 可得：</p>

$$
\frac{\partial E_{\hat{\omega}}}{\partial \hat{\omega}}
= -X^Ty - X^Ty + (X^TX + X^TX) \hat{\omega}
$$

$$= 2X^T(X \hat{\omega - y})$$

<p>其中，$-\frac{\partial y^T X \hat{\omega}}{\partial \hat{\omega}}$ 这一项是根据 $\frac{\partial a^T x}{\partial x}$ 公式求解的，公式中的x就是式子中的 $\omega$，公式中的是 $a^T$ 就是 $y^T X$，那么 a 就是 $y^T X$的转置 = $X^Ty$</p>
<p>在一阶导数上再求一阶导：</p>

$$\nabla^2 E_{\hat{\omega}} = \frac{\partial}{\partial \hat{\omega}} \Bigg( \frac{\partial E_{\hat{\omega}}}{\partial \hat{\omega}} \Bigg)$$

$$ = \frac{\partial}{\partial \hat{\omega}} [2X^T (X \hat{\omega} - y)] $$

$$= \frac{\partial}{\partial \hat{\omega}} (2X^TX \hat{\omega} - 2X^Ty)$$

<p>根据矩阵微分公式 $\frac{\partial Ax}{x} = A^T$ 可得：</p>
<p>$$\nabla^2 E_{\hat{\omega}} = 2X^TX$$</p>
<p>西瓜书上假定 $X^TX$ 为<strong>正定矩阵</strong>（本身不一定为），因此Ew是关于w的<strong>凸函数</strong>得证。</p>
<p>令其等于0：</p>
<p>$$\frac{\partial E_{\hat{\omega}}}{\partial \hat{\omega}} = 2X^T(X \hat{\omega} - y) = 0$$</p>
<p>$$2X^TX \hat{\omega} - 2X^Ty = 0$$</p>
<p>$$2X^TX \hat{\omega} = 2X^Ty$$</p>
<p>3.11
$$\hat{\omega} = (X^TX)^{-1} X^Ty$$</p>
<h3 id="33-对数几率回归" class="headerLink">
    <a href="#33-%e5%af%b9%e6%95%b0%e5%87%a0%e7%8e%87%e5%9b%9e%e5%bd%92" class="header-mark"></a>3.3 对数几率回归</h3><p><a href="https://sm1les.com/2019/01/17/logistic-regression-and-maximum-entropy/" target="_blank" rel="noopener noreferrer">扩展阅读</a></p>
<p>对数几率回归就是逻辑回归，本质是分类算法。回归算法主要预测一些具体的数值。</p>
<p>分类算法，比如二分类，预测一个结果的概率。因此它的值是在0-1中，而线性回归值域是在实数域，就不能直接拿来用。因此在线性模型的基础上套用个映射函数来实现分类的目的。</p>
<p>对数几率回归就是套用了一个 $\frac{1}{1+e^{-z}}$这样的函数。它是一个在0-1之间的s型的曲线，又叫做 Sigmoid函数。其中，z就是线性回归的f(x)。</p>
<p>能够将线性函数映射到0-1区间的函数有很多，为什么只用Sigmod函数？原因很多，最有说服力的是从最大熵的角度来解释。</p>
<p>极大似然估计</p>
<p>第一步：确定概率质量函数（概率密度函数）</p>
<p>已知离散型随机变量 $y \in \lbrace 0,1 \rbrace$ 取值为1和0的概率分别建模为</p>

$$p(y=1|x) = \frac{1}{1+e^{\omega^Tx+b}} = \frac{e^{\omega^T x +b}}{1+e^{\omega^Tx+b}}$$

$$p(y=0|x) = 1 - p(y=1|x) = \frac{1}{1+e^{\omega^Tx+b}}$$

<p>为了便于讨论，令 $\beta = (\omega; b) , , , \hat{x} = (x;1)$，则上式可简写为：</p>

$$p(y=1|\hat{x}; \beta) = \frac{e^{\beta^T \hat{x}}}{1+e^{\beta^T \hat{x}}} = p_1(\hat{x}; \beta)$$

$$p(y=0|\hat{x}; \beta) = \frac{1}{1+e^{\beta^T \hat{x}}} = p_0(\hat{x}; \beta)$$

<p>由以上概率取值可推得随机变量 $y \in \lbrace 0,1 \rbrace$ 的概率质量函数为</p>
<p>3.26
$$p(y|\hat{x};\beta) = y p_1(\hat{x}; \beta) + (1-y) p_0(\hat{x}; \beta)$$</p>
<p>或者为</p>
<p>$$p(y|\hat{x};\beta) = [p_1(\hat{x}; \beta)]^y [p_0(\hat{x};\beta)]^{1-y}$$</p>
<p>理解，当y=0，第一个式子前项为0，直接求后项。第二个式子前项为1，也是直接求后项即可，相反也是。</p>
<p>第二步：写出似然函数</p>
<p>$$L(\beta) = \prod^m_{i=1} p (y_i|\hat{x}_i;\beta)$$</p>
<p>对数似然函数为</p>
<p>$$l(\beta) = ln L(\beta) = \sum^m_{i=1} ln p(y_i | \hat{x}_i ; \beta)$$</p>
<p>$$l(\beta) = \sum^m_{i=1}ln(y_ip_1(\hat{x}_i;\beta) + (1 - y_i) p_0 (\hat{x}_i) ; \beta)$$</p>
<p>将 $p_1(\hat{x}_i ; \beta) = \frac{e^{\beta^T \hat{x}_i}}{1+e^{\beta^T \hat{x}_i}}$，$p_0(\hat{x}_i;\beta) = \frac{1}{1 + e^{\beta^T \hat{x}_i}}$ 代入上式可得</p>
<p>$$l(\beta) = \sum^m_{i=1} ln \Bigg( \frac{y_1 e^{\beta^T \hat{x}_1}}{1+e^{\beta^T \hat{x}_i}} + \frac{1-y_i}{1+e^{\beta^T \hat{x}_i}} \Bigg)$$</p>
<p>$$= \sum^m_{i=1} ln \Bigg( \frac{y_i e^{\beta^T \hat{x}_i} + 1 - y_i}{1+e^{\beta^T \hat{x}_i}} \Bigg)$$</p>
<p>根据ln函数规则，$ln \frac{a}{b} = lna - lnb$</p>
<p>$$= \sum^m_{i=1} (ln(y_i e^{\beta^T \hat{x}_1} + 1 - y_i) - ln (1 + e^{\beta^T \hat{x}_i}))$$</p>
<p>由于$y_i \in \lbrace 0,1 \rbrace$，则</p>
<p>$$
l(\beta) =
\begin{cases}
\sum^m_{i=1}(-ln(1+e^{\beta^T \hat{x}<em>i}))
, \ \ \ \ \ y_i = 0
\
\sum^m</em>{i=1}(\beta^T \hat{x}_i - ln(1 + e^{\beta^T \hat{x}_i}))
, \ \ \ \ \ y_i = 1
\end{cases}
$$</p>
<p>两式综合可得</p>
<p>$$l(\beta) = \sum^m_{i=1} \bigg( y_i \beta^T \hat{x}_i - ln(1+e^{\beta^T \hat{x}_i}) \bigg)$$</p>
<p>为什么可以写成这样？是因为当y=0时，$y_i \beta^T \hat{x}_i = ln(y_i e^{\beta^T \hat{x}_1} + 1 - y_i) = 0$。y=1时，$y_i \beta^T \hat{x}_i = ln(y_i e^{\beta^T \hat{x}_1} + 1 - y_i) = \beta^T \hat{x}_i$。</p>
<p>损失函数通常是以最小化为优化目标，因此可以将最大化 $l(\beta)$ 等价转换为最小化 $l(\beta)$的相反数 $-l(\beta)$，即得到公式3.27。</p>
<p>3.27
$$l(\beta) = \sum^m_{i=1} \bigg( - y_i \beta^T \hat{x}_i - ln(1+e^{\beta^T \hat{x}_i}) \bigg)$$</p>
<h4 id="拓展信息论" class="headerLink">
    <a href="#%e6%8b%93%e5%b1%95%e4%bf%a1%e6%81%af%e8%ae%ba" class="header-mark"></a>拓展：信息论</h4><p>以概率论、随机过程为基本研究工具，研究广义通信系统的整个过程。常见应用：无损数据压缩（zip）、有损数据压缩（MP3、JPEG）</p>
<p><strong>自信息</strong>：随机变量x，它有一个概率质量函数p(x)，它的自信息就是负的log底数为b的函数。</p>
<p>$$I(X) = -log_b p(x)$$</p>
<p>当b=2时单位为bit，当b=e时单位为nat。（为e的时候就是ln）</p>
<p><strong>信息熵</strong>（自信息的期望）：度量随机变量X的不确定性，信息熵越大越不确定。</p>
<p>$H(X) = E[I(X)] = - \sum_x p(x) log_b p(x)$ （以离散型为例）</p>
<p>计算信息熵时约定：若p(x)=0，则p(x)log_b p(x)=0 （具体在决策树讲解）</p>
<p><strong>相对熵</strong> （KL散度）：度量两个分布的差异，其典型使用场景是用来度量理想分布 p(x) 和模拟分布 q(x) 之间的差异。</p>
<p>$$D_{KL}(p||q) = \sum_x p(x) log_b (\frac{p(x)}{q(x)})$$
$$= \sum_x p(x) (log_b p(x) - log_bq (x))$$
$$= \sum_x p(x) log_b p(x) - \sum_x p(x) log_b q(x)$$</p>
<p>其中，$\sum_x p(x) log_b p(x)$ 没加负号的理想分布的信息熵，$- \sum_x p(x) log_b q(x)$ 称为交叉熵，$\sum_x$ 就是遍历x所有可能的取值，。</p>
<p><strong>如何理交叉熵和信息熵？</strong></p>
<ul>
<li>从机器学习三要素中&quot;策略&quot;的角度来说，与理想分布<strong>最接近的模拟分布</strong>即为最优分布，因此可以通过<strong>最小化相对熵</strong>这个策略来求出最优分布，原因是当相对熵达到最小时，q(x)最接近p(x)。由于理想分布p(x)是未知但固定分布（频率学派角度），所以 $\sum_x p(x) log_b p(x)$为常量，那么最小化相对熵就等价于最小化交叉熵 $- \sum_x p(x) log_b q(x)$。</li>
</ul>
<p>以对数几率回归为例，对单个样本yi来说，理想分布</p>
<p>$$
p(y_i) =
\begin{cases}
p(1)=1, p(0) = 0, \ \ \ \ y_i=1 \
p(1)=0, p(0)=1, \ \ \ \ y_i=0
\end{cases}
$$</p>
<p>它现在的模拟分布是</p>
<p>$$
q(y_1) =
\begin{cases}
\frac{e^{\beta^T \hat{x}}}{1+e^{\beta^T \hat{x}}} = p_1(\hat{x}; \beta), \ \ \ \ y_i=1 \
\frac{1}{1+e^{\beta^T \hat{x}}} = p_0(\hat{x}; \beta), \ \ \ \ y_i =0
\end{cases}
$$</p>
<p>那么单个样本yi的交叉熵为</p>
<p>$$-\sum_{y_i} p(y_i) log_b q(y_i)$$</p>
<p>$$-p(1) log_b p_1 (\hat{x}; \beta) - p(0) log_b p_0 (\hat{x}; \beta)$$</p>
<p>$$-y_i log_bp_1(\hat{x}; \beta) - (1 - y_i) log_b p_0 (\hat{x}; \beta)$$</p>
<p>令 b=e</p>
<p>$$-y_i ln p_1 (\hat{x}; \beta) - (1- y_i) ln p_0 (\hat{x}; \beta)$$</p>
<p>全体训练样本的交叉熵</p>
<p>$$\sum^m_{i=1} [-y_i ln p_1 (\hat{x}; \beta) - (1-y_i) ln p_0(\hat{x}; \beta)]$$</p>
<p>$$\sum^m_{i=1} \lbrace -y_i [ln p_1 (\hat{x}_i; \beta) - ln p_0 (\hat{x}_i ; \beta)] - ln(p_0(\hat{x}_i; \beta)) \rbrace$$</p>
<p>$$\sum^m_{i=1} \Bigg[ -y_iln \bigg( \frac{p_1(\hat{x}_i;\beta)}{p_0(\hat{x}_i;\beta)}  - ln(p_0(\hat{x}_i;\beta))\bigg) \Bigg]$$</p>
<p>$$\sum^m_{i=1} \Bigg[ -y_iln \bigg( \frac{\frac{e^{\beta^T \hat{x}}}{1+e^{\beta^T} \hat{x}}}{\frac{1}{1+e^{\beta^T} \hat{x}}}  - ln(\frac{1}{1+e^{\beta^T} \hat{x}})\bigg) \Bigg]$$</p>
<p>$$\sum^m_{i=1} \Bigg[ -y_i ln(e^{\beta^T} \hat{x}_i) - ln(\frac{1}{1+e^{\beta^T} \hat{x}}) \Bigg]$$</p>
<p>又得到了公式3.27</p>
<p>$$\sum^m_{i=1} (-y_i \beta^T \hat{x}_i) + ln(1+e^{\beta^T} \hat{x}_i)$$</p>
<p>对数几率回归算法的机器学习三要素</p>
<ol>
<li>模型：线性模型，输出值在[0,1]，近似阶跃的单调可微函数</li>
<li>策略：极大似然，信息论</li>
<li>算法：梯度下降，牛顿法</li>
</ol>
<h3 id="34-二分类线性判别分析" class="headerLink">
    <a href="#34-%e4%ba%8c%e5%88%86%e7%b1%bb%e7%ba%bf%e6%80%a7%e5%88%a4%e5%88%ab%e5%88%86%e6%9e%90" class="header-mark"></a>3.4 二分类线性判别分析</h3><p>异类样本尽可能远，同类样本方差尽可能小。</p>
<p>在西瓜书定义中，假定了一个数据集 D={(xi, yi)}，其中，i是从1取到m，yi属于0到1。要注意的是Xi，大X的i和数据集中的i不是一个东西。X中i是0，1，X1表示所有y=1的[(xi,yi)]的集合。X0就是y=0的集合。</p>
<p>$||a||^2_2$，这种形式叫做二范数，求的是向量的模长。比如，一个向量 $a=(a_1,a_2)^T$，那它的二范数就是 $||a||_2 = \sqrt{{a_1}^2 + {a_2}^2}$，上面再加个2就是平方 $||a||_2^2 = {a_1}^2 + {a_2}^2$。</p>
</div>

        


<h2>Related Content</h2>
<div class="related-container">
    <div class="related-item-container">
            <div class="related-image">
                <a href="/pumpkin-book-notes06/"><img
        
        loading="lazy"
        src="/posts/pumpkin-book-notes/xigua.png"
        srcset="/posts/pumpkin-book-notes/xigua.png, /posts/pumpkin-book-notes/xigua.png 1.5x, /posts/pumpkin-book-notes/xigua.png 2x"
        sizes="auto"
        alt="/posts/pumpkin-book-notes/xigua.png"
        title="/posts/pumpkin-book-notes/xigua.png" height="200"   width="400" ></a>
            </div><h2 class="related-title">
                <a href="/pumpkin-book-notes06/">Machine Learning Notes 06</a>
            </h2>
        </div>
    <div class="related-item-container">
            <div class="related-image">
                <a href="/pumpkin-book-notes05/"><img
        
        loading="lazy"
        src="/posts/pumpkin-book-notes/xigua.png"
        srcset="/posts/pumpkin-book-notes/xigua.png, /posts/pumpkin-book-notes/xigua.png 1.5x, /posts/pumpkin-book-notes/xigua.png 2x"
        sizes="auto"
        alt="/posts/pumpkin-book-notes/xigua.png"
        title="/posts/pumpkin-book-notes/xigua.png" height="200"   width="400" ></a>
            </div><h2 class="related-title">
                <a href="/pumpkin-book-notes05/">Machine Learning Notes 05</a>
            </h2>
        </div>
    <div class="related-item-container">
            <div class="related-image">
                <a href="/pumpkin-book-notes04/"><img
        
        loading="lazy"
        src="/posts/pumpkin-book-notes/xigua.png"
        srcset="/posts/pumpkin-book-notes/xigua.png, /posts/pumpkin-book-notes/xigua.png 1.5x, /posts/pumpkin-book-notes/xigua.png 2x"
        sizes="auto"
        alt="/posts/pumpkin-book-notes/xigua.png"
        title="/posts/pumpkin-book-notes/xigua.png" height="200"   width="400" ></a>
            </div><h2 class="related-title">
                <a href="/pumpkin-book-notes04/">Machine Learning Notes 04</a>
            </h2>
        </div>
    <div class="related-item-container">
            <div class="related-image">
                <a href="/pumpkin-book-notes03/"><img
        
        loading="lazy"
        src="/posts/pumpkin-book-notes/xigua.png"
        srcset="/posts/pumpkin-book-notes/xigua.png, /posts/pumpkin-book-notes/xigua.png 1.5x, /posts/pumpkin-book-notes/xigua.png 2x"
        sizes="auto"
        alt="/posts/pumpkin-book-notes/xigua.png"
        title="/posts/pumpkin-book-notes/xigua.png" height="200"   width="400" ></a>
            </div><h2 class="related-title">
                <a href="/pumpkin-book-notes03/">Machine Learning Notes 03</a>
            </h2>
        </div>
    <div class="related-item-container">
            <div class="related-image">
                <a href="/pumpkin-book-notes01/"><img
        
        loading="lazy"
        src="/posts/pumpkin-book-notes/xigua.png"
        srcset="/posts/pumpkin-book-notes/xigua.png, /posts/pumpkin-book-notes/xigua.png 1.5x, /posts/pumpkin-book-notes/xigua.png 2x"
        sizes="auto"
        alt="/posts/pumpkin-book-notes/xigua.png"
        title="/posts/pumpkin-book-notes/xigua.png" height="200"   width="400" ></a>
            </div><h2 class="related-title">
                <a href="/pumpkin-book-notes01/">Machine Learning Notes 01</a>
            </h2>
        </div>
    

</div>

<div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2021-10-06</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"><span><a class="link-to-mardown" href=/pumpkin-book-notes02/index.md target="_blank" rel="noopener noreferrer">Read markdown</a>
                    </span></div>
            <div class="post-info-share"><button title="Share on Twitter" data-sharer="twitter" data-url="https://hongspell.site/pumpkin-book-notes02/" data-title="Machine Learning Notes 02" data-via="ssdlaohu9527" data-hashtags="machine learning,西瓜书,南瓜书"><span class="fab fa-twitter fa-fw"></span></button><button title="Share on Facebook" data-sharer="facebook" data-url="https://hongspell.site/pumpkin-book-notes02/" data-hashtag="machine learning"><span class="fab fa-facebook-square fa-fw"></span></button><button title="Share on WhatsApp" data-sharer="whatsapp" data-url="https://hongspell.site/pumpkin-book-notes02/" data-title="Machine Learning Notes 02" data-web><span class="fab fa-whatsapp fa-fw"></span></button><button title="Share on Line" data-sharer="line" data-url="https://hongspell.site/pumpkin-book-notes02/" data-title="Machine Learning Notes 02"><span data-svg-src="/lib/simple-icons/icons/line.min.svg"></span></button><button title="Share on 微博" data-sharer="weibo" data-url="https://hongspell.site/pumpkin-book-notes02/" data-title="Machine Learning Notes 02" data-image="/posts/pumpkin-book-notes/xigua.png"><span class="fab fa-weibo fa-fw"></span></button><button title="Share on Myspace" data-sharer="myspace" data-url="https://hongspell.site/pumpkin-book-notes02/" data-title="Machine Learning Notes 02" data-description=""><span data-svg-src="/lib/simple-icons/icons/myspace.min.svg"></span></button><button title="Share on Blogger" data-sharer="blogger" data-url="https://hongspell.site/pumpkin-book-notes02/" data-title="Machine Learning Notes 02" data-description=""><span class="fab fa-blogger fa-fw"></span></button><button title="Share on Evernote" data-sharer="evernote" data-url="https://hongspell.site/pumpkin-book-notes02/" data-title="Machine Learning Notes 02"><span class="fab fa-evernote fa-fw"></span></button></div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/machine-learning/">machine learning</a>,&nbsp;<a href="/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/">西瓜书</a>,&nbsp;<a href="/tags/%E5%8D%97%E7%93%9C%E4%B9%A6/">南瓜书</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/pumpkin-book-notes01/" class="prev" rel="prev" title="Machine Learning Notes 01"><i class="fas fa-angle-left fa-fw"></i>Machine Learning Notes 01</a>
            <a href="/pumpkin-book-notes03/" class="next" rel="next" title="Machine Learning Notes 03">Machine Learning Notes 03<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
</article></div>
        </main><footer class="footer">
        <div class="footer-container"><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2023 - 2024</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank" rel="noopener noreferrer">Hong</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span><span class="icp-splitter">&nbsp;|&nbsp;</span><br class="icp-br"/>
                    <span class="icp"><a href="http://beian.miit.gov.cn/" target="_blank">黑ICP备2023016241号</a></span></div>
            <div class="footer-line"></div>
            <div class="footer-line">
            </div>
        </div></footer></div>

    <div id="fixed-buttons"><a href="#back-to-top" id="back-to-top-button" class="fixed-button" title="Back to Top">
            <i class="fas fa-arrow-up fa-fw"></i>
        </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
            <i class="fas fa-comment fa-fw"></i>
        </a>
    </div><div id="cookieconsent-container"></div><div class="assets"><link rel="stylesheet" href="/lib/lightgallery/lightgallery.min.css"><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="preload" as="style" onload="this.onload=null;this.rel='stylesheet'" href="/lib/katex/copy-tex.min.css">
        <noscript><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"></noscript><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"desktop-header-typeit":"HongSpell","mobile-header-typeit":"HongSpell"},"lightGallery":{"actualSize":false,"exThumbImage":"data-thumbnail","hideBarsDelay":2000,"selector":".lightgallery","speed":400,"thumbContHeight":80,"thumbWidth":80,"thumbnail":true},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"distance":100,"findAllMatches":false,"fuseIndexURL":"/index.json","highlightTag":"em","ignoreFieldNorm":false,"ignoreLocation":false,"isCaseSensitive":false,"location":0,"maxResultLength":10,"minMatchCharLength":2,"noResultsFound":"No results found","snippetLength":50,"threshold":0.3,"type":"fuse","useExtendedSearch":false},"sharerjs":true,"table":{"sort":true},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"desktop-header-typeit":["desktop-header-typeit"],"mobile-header-typeit":["mobile-header-typeit"]},"duration":-1,"speed":100}};</script><script type="text/javascript" src="/lib/tablesort/tablesort.min.js"></script><script type="text/javascript" src="/lib/lightgallery/lightgallery.min.js"></script><script type="text/javascript" src="/lib/lightgallery/lg-thumbnail.min.js"></script><script type="text/javascript" src="/lib/lightgallery/lg-zoom.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/typeit/typeit.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js" defer></script><script type="text/javascript" src="/lib/katex/auto-render.min.js" defer></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js" defer></script><script type="text/javascript" src="/lib/katex/mhchem.min.js" defer></script><script type="text/javascript" src="/js/katex.min.js" defer></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script type="text/javascript" src="/js/cookieconsent.min.js" defer></script><script type="text/javascript" src="/js/theme.min.js" defer></script></div>
</body>

</html>
