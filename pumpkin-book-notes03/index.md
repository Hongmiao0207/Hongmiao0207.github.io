# Machine Learning Notes 03


## 4. 决策树 decision tree

主流应用在集成学习中多棵决策树构建森林。

### 4.1 基本流程

又叫判定树，在分类问题中，它基于树结构来进行决策和判定，给出最终判定结果。

决策树的生成是一个递归过程，三种情形导致递归：

1. 当前节点包含的样本全属于同一类别，无需划分。
2. 当前属性集为空，或全样本所在的所有属性上取值相同，无法划分。
3. 当前节点包含的样本集合为空，不能划分。

其中，2情形下，把当前节点标记为叶节点，并将其类别设定为该节点所含样本最多的类别。3情形下，同样把当前节点标记为叶节点，但是将其类别设定为其父节点所含样本最多的类别。

* 二者处理实质不同，2利用的是后验分布，3是先验分布。

算法原理：逻辑上通过 if else语句判断；几何上根据某种准则划分特征空间，最终目的是将样本越分越纯。

* 特征空间是将样本各个特征作为一个轴，构成的空间。

### 4.2 划分选择

决策树学习的关键是如何选择最优划分属性，也就是节点purity要高。

#### 4.2.1 信息增益

information entropy 信息熵 是度量样本集合纯度最常用的指标。

$$Ent(D)=-\sum^{|y|}_{k=1} p_k {log}_2 p_k$$

其中，约定p=0，plog2p=0；ent值越小，d纯度越高。（结合2为底的对数函数图像去看就很好理解）

**条件熵**（Y的信息熵关于概率分布X的期望，条件熵也就是Y的信息熵）：在已知X后Y的不确定性
$$H(Y|X) = \sum_x p(x) H(Y|X=x)$$

解释：从单属性（特征）a（色泽）的角度来看，假设其可能取值为{a1, a2, a3, ..., a^V}，其中，a1 < 30，a2 > 30。D^v表示属性a取到这些值的集合，因此D^1表示色泽小于30的样本集合，D^2表示色泽大于30的样本集合。 |D^v| / D 表示当前属样本集合（即a1或a2）占总样本（a1+...+av）的占比，那么在已知属性a的取值后，样本集合D的条件熵为

$$\sum^V_{v=1} \frac{|D^v|}{|D|} Ent(D^v)$$

信息增益

$$Gain(D,a)=Ent(D)-\sum^V_{v=1}\frac{|D|^v}{|D|}Ent(D^v)$$

信息增益越大，意味着属性a来进行划分所获得的纯度提升越大。

在已知属性（特征）a的取值后y的不确定性减少的量（差值），即纯度的提升。

通常前者值 $Ent(D)$（信息熵）大，后者值 $\sum^V_{v=1}\frac{|D|^v}{|D|}Ent(D^v)$（条件熵）小。

#### 4.2.2 增益率

然后，信息增益准则对取值数目较多的属性有所偏好，为减少这种偏好带来的不利影响，引入了增益率 gain ratio来选择最优划分属性

$$Gain \ ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$$

其中

$$IV(a)=\sum^V_{v=1} \frac{|D^v|}{D} {log}_2 \frac{|D^v|}{|D|}$$

IV(a)称为属性a的固有值 intrinsic value。属性a可能取值数目越多（V越大），则IV(a)的值通常越大。

要注意的是和信息增益准则相反，增益率准则对可取值数目较少的属性有偏好。所以它不能直接用来进行属性的划分，而是使用了一个启发式，先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的。

#### 4.2.3 基尼指数 Gini

$$Gini(D,a) = \sum^{|y|}_{k=1}  \sum_{k^{'} \neq{k}} p_k p_{k^{'}}$$

$$=1-\sum^{|y|}_{k=1} p^2_k$$

直观来说，Gini(D) 反映了数据集D中随机抽取两个两本，其类别标记不一致的概率。因此，**Gini越小，碰到异类的概率就小，数据集D的纯度越高**。

属性a的基尼指数定义为

$$Gini_index(D,a) = \sum^V_{v=1} \frac{|D^v|}{|D|} Gini(D^v)$$

因此，**在候选属性集合A中，选择那个使得划分后基尼指数最小的属性作为最优划分属性**，即 $a_* = arg_{a \in A} min Gini_index(D,a)$

### 4.3 剪枝处理 pruning

剪枝是决策树算法解决过拟合的手段，节点划分过程有时造成分支过多，以至于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合。因此，可以主动去掉一些分支来降低过拟合的风险。

基本策略：

* 预剪枝（prepruning），每个节点在划分前先进性评估，若当前节点的划分不能提升决策树泛化能力，则不会把当前节点标记为叶节点。
* 后剪枝（post-pruning），先生成完整的决策树，然后自底向上地对非叶节点进行考察，若该节点对应子树能够提升泛化性，则将该子树替换为叶节点。

如何判断泛化性提升？先预留部分数据作验证集。

#### 4.3.1 预剪枝

基于信息增益准则，选取某属性对训练数据集进行划分，产生该属性种类n个分支。若不进行划分，直接将该节点标为叶节点，那么验证集对该节点进行评估，得出一个验证集的精度 a%。通过预剪枝划分后，同样用验证集进行评估，得到一个精度b%。比较a、b，如果b精度大于a，那么就证明这个预剪枝是有效的，反之就是无效。这样做可以阻止展开。

好处是降低过拟合风险，减少训练时间和测试时间。缺点是，某些节点可能当前不具备泛化性的提升，但是醉着后续的划分，可能会导致性能的提升。预剪枝基于的是贪心的思想，禁止分支展开。

#### 4.3.2 后剪枝

生成完整的决策树，得到验证集精度为a%。选择某一节点分支剪除，然后进行验证，如果得到的精度b% 高于a%，那就会进行剪枝，如果精度没有明显的提升，就会保留。

### 4.4 连续和缺失值

#### 4.4.1 连续值处理

之前讨论的都是离散值。由于连续属性的可取值数不再有限，因此，不能直接根据连续属性的可取值来对节点进行划分。连续属性离散化可能使用，最简单的策略，二分法（bi-partition）

有点难，没看懂

#### 4.4.2 缺失值处理（不完整样本）

#### 4.4.3 多变量决策树

